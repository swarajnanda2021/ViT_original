{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "ffdcda2bc7624974a5f95fd7d0c89c45",
            "777f138cc3974772b62998557bb198c4",
            "40ee4c3b73e54d828c8e9f2b101aef1f",
            "3e195770ea71400092ca73069d4b22d5",
            "90e39af36dd142d7a21dd51a48a9f4a6",
            "7c044a8d88434f09ade08433c1e4bf26",
            "1d621a7347b6471f94dad09f175d9667",
            "e2c54bf4c21d4f40ac631701c183c3ff",
            "50d42aa123ac46268a9af780b1b0a240",
            "70ac9bbea99f4432b047d3dd02be99e4",
            "9d67a662bdcb4641ace444cda376d387"
          ]
        },
        "id": "U0NvJlxLj8he",
        "outputId": "b2ea6f15-9dc5-4fb5-9c11-54f5e4ffbd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffdcda2bc7624974a5f95fd7d0c89c45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/cifar-10-python.tar.gz to /content/data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# We are going to implement the ViT model from scratch.\n",
        "\n",
        "# download the cifar-10 dataset\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "cifar_trainset = datasets.CIFAR10(root='/content/data', train=True, download=True, transform=None)\n",
        "cifar_testset = datasets.CIFAR10(root='/content/data', train=False, download=True, transform=None)\n",
        "\n",
        "   # ['ship', 'bird', 'frog', 'truck', 'automobile', 'deer', 'cat', 'dog', 'horse', 'airplane']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vIUkBxl8j8hh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def load_batch(fpath, label_key='labels'):\n",
        "    with open(fpath, 'rb') as f:\n",
        "        if sys.version_info < (3,):\n",
        "            d = pickle.load(f)\n",
        "        else:\n",
        "            d = pickle.load(f, encoding='bytes')\n",
        "            # decode utf8\n",
        "            d_decoded = {}\n",
        "            for k, v in d.items():\n",
        "                d_decoded[k.decode('utf8')] = v\n",
        "            d = d_decoded\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "def load_cifar10():\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for i in range(1, 6):\n",
        "        fpath = '/content/data/cifar-10-batches-py/data_batch_' + str(i)\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train.append(data)\n",
        "        y_train.append(labels)\n",
        "\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    x_test, y_test = load_batch(\"/content/data/cifar-10-batches-py/test_batch\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:10].shape"
      ],
      "metadata": {
        "id": "ywzSOwcebEgO",
        "outputId": "efb120ae-f236-4516-f4da-9054e0b1efe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3, 32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do some data augmentation\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
        "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "# Apply the transformation to a set of images\n",
        "pil_images = [Image.fromarray(image.astype(np.uint8).transpose(1,2,0), mode=\"RGB\") for image in x_train[:10]]\n",
        "augmented_images = [transform(image) for image in pil_images]\n",
        "\n",
        "\n",
        "# Choose an image to plot\n",
        "image = augmented_images[5]\n",
        "\n",
        "# Convert the tensor to a numpy array\n",
        "image = np.transpose(image.numpy(), (1, 2, 0))\n",
        "\n",
        "# Un-normalize the image\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "image = std * image + mean\n",
        "\n",
        "# Clip the values to [0, 1]\n",
        "image = np.clip(image, 0, 1)\n",
        "\n",
        "# Plot the image\n",
        "plt.figure(figsize=(1,1))\n",
        "#plt.imshow(np.transpose(x_train[0],(1,2,0)))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title('Transformed Image')\n",
        "plt.show()\n",
        "\n",
        "print(image.shape)"
      ],
      "metadata": {
        "id": "JfJJ0WxZa_Q_",
        "outputId": "0458d46d-94dc-4179-b867-76950134b8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93F6zxm4j8hi",
        "outputId": "808dd128-05de-4764-9db4-f431c713f283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 32, 32])\n",
            "torch.Size([1, 3, 2, 2])\n",
            "torch.Size([1, 3, 4])\n",
            "torch.Size([1, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let us now extract the patches from the image using a simple convolution layer\n",
        "# We will use a kernel size of 16 and stride of 16\n",
        "\n",
        "patch_size = 16\n",
        "img = torch.tensor(x_train[0]).unsqueeze(0).float()\n",
        "#img = nn.functional.pad(img, (8, 8, 8, 8))\n",
        "print(img.shape)\n",
        "\n",
        "patch_extractor = nn.Conv2d(\n",
        "    in_channels=3, \n",
        "    out_channels=3, \n",
        "    kernel_size=patch_size, \n",
        "    stride=patch_size, \n",
        "    bias=False)\n",
        "patches = patch_extractor(img)\n",
        "print(patches.shape)\n",
        "\n",
        "# now flatten the patches\n",
        "patches = patches.flatten(2) # img is 3 X 32 X 32, so patches is 3 X 2 X 2 and flattened, it is a vector that is 12 long\n",
        "print(patches.shape)  \n",
        "\n",
        "patches = patches.transpose(1,2)\n",
        "print(patches.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpU34oBaj8hj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We will now write a script for getting image patches\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "\n",
        "    '''\n",
        "    Params:\n",
        "    -------\n",
        "    img_size (int)      : size of the image (assumed to be square) \n",
        "    patch_size (int)    : size of the patch (assumed to be square)\n",
        "    in_chans (int)      : number of channels in the image (assumed to be RGB typically)\n",
        "    embed_dim (int)     : embedding dimension (will be constant throughout the network)\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "    num_patches (int)   : number of patches in the image\n",
        "    proj (nn.Conv2d)    : convolutional layer to get the patches, will have same stride as patch_size\n",
        "    '''\n",
        "    def __init__(self, img_size, patch_size, in_chans=3,embed_dim=256):\n",
        "        super().__init__() # call the super class constructor which is used to inherit the properties of the parent class\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2 # assuming square image\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride = patch_size\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ''' Parameters: \n",
        "        x (torch.Tensor): input image of shape (n_samples or batches, number of channels, height, width)\n",
        "        Returns: \n",
        "        output = n_samplex X n_patches X embed_dim shape tensor\n",
        "        '''\n",
        "        x = self.proj(x) # n_samples X embed_dim X sqrt(n_patches) X sqrt(n_patches)\n",
        "        x = x.flatten(2) # n_sample X embed_dim X n_patches\n",
        "        x = x.transpose(1, 2) # n_samples X n_patches X embed_dim (dimensions are swapped)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Let us now write the attention module\n",
        "class Attention(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding dimension, \n",
        "    n_heads (int)       : number of attention heads\n",
        "    qkv_bias (bool)     : if True, we will include a bias in the query, key and value projections\n",
        "    attn_d (float)      : Probability of dropout added to q, k and v during the training\n",
        "    proj_d (float)      : Probability of dropout added to the projection layer\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    scale (float)               : Used for norrmalizing the dot product\n",
        "    qkv (nn.Linear)             : Linear projection, which are used for performing the attention\n",
        "    proj (nn.Linear)            : Takes in the concatenated output of all attention heads and maps it further\n",
        "    attn_d, proj_d (nn.Dropout) : Dropout layers\n",
        "\n",
        "    '''\n",
        "    def __init__(self,dim, n_heads=4, qkv_bias = False, attn_d = 0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5 # scaling added as per Vaswani paper for not feeding extremely large values to softmas\n",
        "        self.qkv = nn.Linear(dim,dim * 3, bias = qkv_bias) # can be written separately too\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_d = nn.Dropout(proj_d)\n",
        "        self.attn_d = nn.Dropout(attn_d)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        ''' \n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor) : has shape (n_samples/batch, n_patches+1, dim)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor (n_samples, n_patches+1, dim)\n",
        "\n",
        "        '''\n",
        "        n_samples, n_tokens, dim = x.shape # extract shapes, tokens and dimensions from the output of the embeddings\n",
        "        if dim != self.dim:\n",
        "            raise ValueError # raise an error if dim isn't equal to the dimension set in the attention layer\n",
        "        \n",
        "        qkv = self.qkv(x) # Perform the query, key, value projections. (n_samples/batches, n_patches+1, 3*dim), the middle dimension is maintained\n",
        "\n",
        "        # Let us now reshape the qkv tensor to separate the query, key and value\n",
        "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(2,0,3,1,4) # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # Now extract the query, key and value\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2] # (n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # perform the dot product and scale the dot product\n",
        "        dot_prod = (q @ k.transpose(-2,-1)) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        # apply a softmax\n",
        "        attention = dot_prod.softmax(dim = -1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        attention = self.attn_d(attention) # apply dropout for regularization during training\n",
        "        # weighted average\n",
        "        wei = (attention @ v).transpose(1,2) # (n_samples, n_patches+1, n_heads, head_dim)\n",
        "        # flatten\n",
        "        wei = wei.flatten(2) # (n_samples, n_patches+1, dim) as dim = n_heads * head_dim\n",
        "        # we now apply the projection\n",
        "        x = self.proj(wei) # (n_samples, n_patches+1, dim)\n",
        "        x = self.proj_d(x) # apply dropout for regularization during training\n",
        "        return x\n",
        "\n",
        "    # Let us now write the MLP module\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features (int)           : embedding dimension, \n",
        "    hidden_features(int)        : dimension of the hidden layer\n",
        "    out_features (int)          : dimension of the hidden layer\n",
        "    dropout (float)     : probability of dropout\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    fc1 (nn.Linear)     : Linear projection, which are used for performing the attention\n",
        "    fc2 (nn.Linear)     : Takes in the concatenated output of all attention heads and maps it further\n",
        "    dropout (nn.Dropout): Dropout layer\n",
        "\n",
        "    '''\n",
        "    def __init__(self, in_features,hidden_features,out_features, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features) # takes in the input and maps it to the hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features) # takes in the hidden layer and maps it to the output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.GELU() # we will the GELU activation function in the paper\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) # apply the first linear projection, (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.act(x) # apply the activation function (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.fc2(x) # apply the second linear projection (n_samples, n_patches+1, out_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, out_features)\n",
        "        return x\n",
        "\n",
        "# We have everything we need to write the ViT class\n",
        "\n",
        "class Block(nn.Module):\n",
        "    ''' Transformer with Vision Token\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding\n",
        "    n_heads (int)       : number of attention heads\n",
        "    mlp_ratio (float)   : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)     : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,          : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2        :  LayerNorm layers\n",
        "    attn                : Attention layer\n",
        "    mlp                 : MLP layer\n",
        "    '''\n",
        "    def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias=True, attn_d=0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6) # division by zero is prevented and we match the props of the pretrained model\n",
        "        self.attn = Attention(dim, n_heads, qkv_bias, attn_d, proj_d)\n",
        "        self.norm2 = nn.LayerNorm(dim, eps = 1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features = dim, hidden_features = hidden_features, out_features = dim, dropout = proj_d)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x)) # add to the residual highway after performing Layernorm and attention\n",
        "        x = x + self.mlp(self.norm2(x)) # add to the residual highway after performing Layernorm and MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "# now we can write the Vision Transformer class\n",
        "class ViT(nn.Module):\n",
        "    ''' Vision Transformer\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_size (int)            : size of the input image\n",
        "    patch_size (int)            : size of the patches to be extracted from the input image\n",
        "    in_channels (int)           : number of input channels\n",
        "    num_classes (int)           : number of classes\n",
        "    embed_dim (int              : embedding dimension\n",
        "    depth (int)                 : number of transformer blocks\n",
        "    n_heads (int)               : number of attention heads per block\n",
        "    mlp_ratio (float)           : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)             : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,             : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed (nn.Conv2d)     : Convolutional embedding layer\n",
        "    pos_embed (nn.Parameter)    : learnable positional embedding\n",
        "    cls_token (nn.Parameter)    : learnable class token\n",
        "    blocks (nn.ModuleList)      : list of transformer blocks\n",
        "    norm (nn.LayerNorm)         : final LayerNorm layer\n",
        "    head (nn.Linear)            : final linear projection layer\n",
        "    '''\n",
        "    # initialize\n",
        "    def __init__(self, \n",
        "                img_size = 384, \n",
        "                patch_size = 16, \n",
        "                in_chans=3, \n",
        "                n_classes = 1000, \n",
        "                embed_dim = 768, \n",
        "                depth = 12,\n",
        "                n_heads = 12,\n",
        "                mlp_ratio = 4.0,\n",
        "                qkv_bias = True,\n",
        "                attn_d = 0.,\n",
        "                proj_d = 0.):\n",
        "        super().__init__()\n",
        "        # we will use the same image size as the pretrained model\n",
        "        self.patch_embed = PatchEmbed(\n",
        "                                        img_size = img_size,\n",
        "                                        patch_size = patch_size,\n",
        "                                        in_chans = in_chans,\n",
        "                                        embed_dim = embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)) # learnable class token\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim)) # learnable positional embedding\n",
        "        self.pos_d     = nn.Dropout(p = proj_d) # dropout layer\n",
        "        self.blocks    = nn.ModuleList(\n",
        "                            [\n",
        "                                Block( \n",
        "                                    dim = embed_dim, \n",
        "                                    n_heads = n_heads, \n",
        "                                    mlp_ratio = mlp_ratio, \n",
        "                                    qkv_bias = qkv_bias, \n",
        "                                    attn_d = attn_d, \n",
        "                                    proj_d = proj_d) for _ in range(depth)] # iteratively create the transformer blocks with same parameters\n",
        "                                    )\n",
        "        self.norm       = nn.LayerNorm(embed_dim, eps = 1e-6) # final LayerNorm layer\n",
        "        self.head       = nn.Linear(embed_dim, n_classes) # final linear projection layer    \n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        ''' Forward pass\n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor)            : n_samples X in_chans X img_size X img_size\n",
        "        Returns\n",
        "        -------\n",
        "        logits (torch.Tensor)       : n_samples X n_classes\n",
        "        '''\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x) # extract patches from the input image and turn them into patch embeddings\n",
        "        cls_tokens = self.cls_token.expand(n_samples, -1, -1) # expand the class token to match the batch size\n",
        "        # pre-append the class token to the patch embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim = 1) # n_samples X (n_patches + 1) X embed_dim\n",
        "        x = x + self.pos_embed # add the positional embedding to the patch embeddings\n",
        "        x = self.pos_d(x) # apply dropout to the embeddings\n",
        "        for block in self.blocks: # apply transformer blocks\n",
        "            x = block(x)\n",
        "        x = self.norm(x) # apply LayerNorm to the final output\n",
        "        # the shape of x now is n_samples X (n_patches + 1) X embed_dim\n",
        "        # extract the class token from the output\n",
        "        cls_token = x[:, 0] # n_samples X embed_dim\n",
        "        x = self.head(cls_token) # n_samples X n_classes\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6yTLrgBj8ho",
        "outputId": "f7d807dc-d571-4e4c-a18e-ef5f3bbfe41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n",
            "Total params: 38231562\n"
          ]
        }
      ],
      "source": [
        "# Verify that the model works\n",
        "model = ViT(img_size = 32, \n",
        "                    patch_size = 16, \n",
        "                    in_chans=3, \n",
        "                    n_classes = 10, \n",
        "                    embed_dim = 512, \n",
        "                    depth = 12,\n",
        "                    n_heads = 8,\n",
        "                    mlp_ratio = 4.0,\n",
        "                    qkv_bias = True,\n",
        "                    attn_d = 0.2,\n",
        "                    proj_d = 0.2)\n",
        "x = torch.randn(1, 3, 32, 32)\n",
        "y = model(x)\n",
        "print(y.shape)\n",
        "print(f'Total params: {sum(p.numel() for p in model.parameters())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XqFqUMWj8hr",
        "outputId": "5c253043-9e90-4945-fc8b-f22854a1ca4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step 50010: train loss = 1.6563, val loss = 1.3479\n",
            "step 50020: train loss = 1.1719, val loss = 1.4349\n",
            "step 50030: train loss = 1.2854, val loss = 1.3197\n",
            "step 50040: train loss = 1.4564, val loss = 1.9760\n",
            "step 50050: train loss = 1.1122, val loss = 1.2547\n",
            "step 50060: train loss = 1.5002, val loss = 1.5484\n",
            "step 50070: train loss = 0.9658, val loss = 1.4476\n",
            "step 50080: train loss = 1.0843, val loss = 1.4019\n",
            "step 50090: train loss = 0.9094, val loss = 1.8245\n",
            "step 50100: train loss = 1.2294, val loss = 1.8154\n",
            "step 50110: train loss = 1.3719, val loss = 1.5732\n",
            "step 50120: train loss = 1.2622, val loss = 1.3481\n",
            "step 50130: train loss = 1.0895, val loss = 1.3311\n",
            "step 50140: train loss = 1.3189, val loss = 1.4661\n",
            "step 50150: train loss = 0.9015, val loss = 1.3332\n",
            "step 50160: train loss = 1.2656, val loss = 1.8340\n",
            "step 50170: train loss = 1.1076, val loss = 1.3752\n",
            "step 50180: train loss = 1.5507, val loss = 1.1787\n",
            "step 50190: train loss = 1.6440, val loss = 1.3654\n",
            "step 50200: train loss = 0.9842, val loss = 2.0653\n",
            "step 50210: train loss = 1.0792, val loss = 1.8271\n",
            "step 50220: train loss = 1.1997, val loss = 1.1634\n",
            "step 50230: train loss = 1.1958, val loss = 1.3672\n",
            "step 50240: train loss = 1.1350, val loss = 1.7129\n",
            "step 50250: train loss = 1.2430, val loss = 1.1777\n",
            "step 50260: train loss = 1.3813, val loss = 1.7271\n",
            "step 50270: train loss = 1.0998, val loss = 1.7319\n",
            "step 50280: train loss = 1.4570, val loss = 1.4819\n",
            "step 50290: train loss = 1.5952, val loss = 1.6025\n",
            "step 50300: train loss = 1.2773, val loss = 1.7034\n",
            "step 50310: train loss = 1.1114, val loss = 2.0742\n",
            "step 50320: train loss = 1.3500, val loss = 1.8275\n",
            "step 50330: train loss = 1.1785, val loss = 1.7461\n",
            "step 50340: train loss = 0.9311, val loss = 1.9067\n",
            "step 50350: train loss = 1.1505, val loss = 1.5403\n",
            "step 50360: train loss = 1.5711, val loss = 1.6343\n",
            "step 50370: train loss = 1.0075, val loss = 1.5300\n",
            "step 50380: train loss = 1.2514, val loss = 1.1748\n",
            "step 50390: train loss = 1.2008, val loss = 1.1444\n",
            "step 50400: train loss = 0.9889, val loss = 1.3345\n",
            "step 50410: train loss = 1.0245, val loss = 1.7240\n",
            "step 50420: train loss = 1.1302, val loss = 1.3924\n",
            "step 50430: train loss = 1.1347, val loss = 1.1304\n",
            "step 50440: train loss = 1.1218, val loss = 1.4973\n",
            "step 50450: train loss = 1.4054, val loss = 1.5364\n",
            "step 50460: train loss = 1.1139, val loss = 1.6204\n",
            "step 50470: train loss = 1.0212, val loss = 1.3897\n",
            "step 50480: train loss = 0.9257, val loss = 1.6888\n",
            "step 50490: train loss = 1.4910, val loss = 1.3338\n",
            "step 50500: train loss = 1.1427, val loss = 1.3100\n",
            "step 50510: train loss = 1.3269, val loss = 1.1391\n",
            "step 50520: train loss = 1.1301, val loss = 1.3897\n",
            "step 50530: train loss = 1.0643, val loss = 1.4621\n",
            "step 50540: train loss = 1.0698, val loss = 1.3078\n",
            "step 50550: train loss = 0.9879, val loss = 1.6243\n",
            "step 50560: train loss = 1.3480, val loss = 1.7485\n",
            "step 50570: train loss = 0.9466, val loss = 1.8216\n",
            "step 50580: train loss = 1.0319, val loss = 1.1950\n",
            "step 50590: train loss = 1.6037, val loss = 1.2837\n",
            "step 50600: train loss = 1.3806, val loss = 1.3182\n",
            "step 50610: train loss = 1.3839, val loss = 1.6047\n",
            "step 50620: train loss = 1.1217, val loss = 1.5253\n",
            "step 50630: train loss = 1.2304, val loss = 1.8900\n",
            "step 50640: train loss = 1.1670, val loss = 1.7031\n",
            "step 50650: train loss = 0.9900, val loss = 1.6909\n",
            "step 50660: train loss = 1.3772, val loss = 1.5527\n",
            "step 50670: train loss = 1.0225, val loss = 1.3655\n",
            "step 50680: train loss = 1.1786, val loss = 1.2465\n",
            "step 50690: train loss = 1.3198, val loss = 1.4748\n",
            "step 50700: train loss = 1.1480, val loss = 1.6746\n",
            "step 50710: train loss = 1.0560, val loss = 1.2947\n",
            "step 50720: train loss = 1.0195, val loss = 1.9639\n",
            "step 50730: train loss = 1.2521, val loss = 1.1633\n",
            "step 50740: train loss = 1.3061, val loss = 1.4931\n",
            "step 50750: train loss = 1.1140, val loss = 1.8486\n",
            "step 50760: train loss = 1.3174, val loss = 1.6160\n",
            "step 50770: train loss = 1.0426, val loss = 1.3699\n",
            "step 50780: train loss = 0.8718, val loss = 1.3305\n",
            "step 50790: train loss = 1.3445, val loss = 1.3035\n",
            "step 50800: train loss = 1.2806, val loss = 1.5282\n",
            "step 50810: train loss = 1.3876, val loss = 1.1623\n",
            "step 50820: train loss = 1.4315, val loss = 1.2171\n",
            "step 50830: train loss = 1.1724, val loss = 1.8788\n",
            "step 50840: train loss = 1.1691, val loss = 1.8670\n",
            "step 50850: train loss = 1.4454, val loss = 1.4034\n",
            "step 50860: train loss = 0.9069, val loss = 1.8679\n",
            "step 50870: train loss = 0.9695, val loss = 1.5197\n",
            "step 50880: train loss = 1.4221, val loss = 1.3960\n",
            "step 50890: train loss = 1.2601, val loss = 1.3135\n",
            "step 50900: train loss = 1.3238, val loss = 1.6326\n",
            "step 50910: train loss = 1.2637, val loss = 1.7525\n",
            "step 50920: train loss = 1.2788, val loss = 1.5188\n",
            "step 50930: train loss = 1.2126, val loss = 1.4649\n",
            "step 50940: train loss = 1.0148, val loss = 1.3502\n",
            "step 50950: train loss = 1.0250, val loss = 1.2470\n",
            "step 50960: train loss = 1.3674, val loss = 1.3449\n",
            "step 50970: train loss = 1.2698, val loss = 1.5130\n",
            "step 50980: train loss = 1.1962, val loss = 1.8416\n",
            "step 50990: train loss = 1.2063, val loss = 1.1372\n",
            "step 51000: train loss = 1.1113, val loss = 1.9735\n",
            "step 51010: train loss = 1.0910, val loss = 1.1183\n",
            "step 51020: train loss = 1.1695, val loss = 1.6377\n",
            "step 51030: train loss = 1.0722, val loss = 1.1304\n",
            "step 51040: train loss = 1.0338, val loss = 1.2958\n",
            "step 51050: train loss = 1.4051, val loss = 1.4039\n",
            "step 51060: train loss = 1.0959, val loss = 1.5457\n",
            "step 51070: train loss = 1.1204, val loss = 1.2980\n",
            "step 51080: train loss = 1.3884, val loss = 1.9420\n",
            "step 51090: train loss = 1.0625, val loss = 1.6457\n",
            "step 51100: train loss = 1.1703, val loss = 1.7242\n",
            "step 51110: train loss = 1.2488, val loss = 1.2518\n",
            "step 51120: train loss = 1.2784, val loss = 1.2352\n",
            "step 51130: train loss = 1.0355, val loss = 1.9830\n",
            "step 51140: train loss = 1.3355, val loss = 1.3202\n",
            "step 51150: train loss = 1.5877, val loss = 1.5003\n",
            "step 51160: train loss = 1.4400, val loss = 1.8420\n",
            "step 51170: train loss = 1.3525, val loss = 1.4646\n",
            "step 51180: train loss = 1.1601, val loss = 1.7730\n",
            "step 51190: train loss = 1.5766, val loss = 1.7953\n",
            "step 51200: train loss = 1.3611, val loss = 1.6159\n",
            "step 51210: train loss = 1.2477, val loss = 1.3015\n",
            "step 51220: train loss = 1.2527, val loss = 1.2732\n",
            "step 51230: train loss = 1.2304, val loss = 1.4677\n",
            "step 51240: train loss = 1.1137, val loss = 1.4747\n",
            "step 51250: train loss = 0.9957, val loss = 1.5676\n",
            "step 51260: train loss = 1.1010, val loss = 1.4757\n",
            "step 51270: train loss = 1.1547, val loss = 1.4023\n",
            "step 51280: train loss = 1.2055, val loss = 1.7699\n",
            "step 51290: train loss = 1.3154, val loss = 1.5585\n",
            "step 51300: train loss = 1.3400, val loss = 1.5692\n",
            "step 51310: train loss = 1.0642, val loss = 1.4216\n",
            "step 51320: train loss = 1.2559, val loss = 1.6997\n",
            "step 51330: train loss = 1.1123, val loss = 1.2304\n",
            "step 51340: train loss = 0.8050, val loss = 1.7323\n",
            "step 51350: train loss = 1.0560, val loss = 1.0279\n",
            "step 51360: train loss = 1.1061, val loss = 1.5190\n",
            "step 51370: train loss = 1.0153, val loss = 1.8306\n",
            "step 51380: train loss = 1.6877, val loss = 1.8825\n",
            "step 51390: train loss = 0.9187, val loss = 1.5706\n",
            "step 51400: train loss = 1.2245, val loss = 1.2400\n",
            "step 51410: train loss = 1.2730, val loss = 1.1820\n",
            "step 51420: train loss = 1.1410, val loss = 1.1976\n",
            "step 51430: train loss = 1.1886, val loss = 1.2151\n",
            "step 51440: train loss = 1.2087, val loss = 1.8249\n",
            "step 51450: train loss = 1.0555, val loss = 1.4137\n",
            "step 51460: train loss = 0.9912, val loss = 1.3405\n",
            "step 51470: train loss = 1.2750, val loss = 1.2697\n",
            "step 51480: train loss = 1.2817, val loss = 1.8824\n",
            "step 51490: train loss = 1.4080, val loss = 1.4677\n",
            "step 51500: train loss = 0.7808, val loss = 2.1282\n",
            "step 51510: train loss = 1.2727, val loss = 1.5648\n",
            "step 51520: train loss = 1.0863, val loss = 1.6626\n",
            "step 51530: train loss = 1.3852, val loss = 1.5895\n",
            "step 51540: train loss = 1.3948, val loss = 1.4210\n",
            "step 51550: train loss = 1.4770, val loss = 1.3629\n",
            "step 51560: train loss = 0.9822, val loss = 1.5121\n",
            "step 51570: train loss = 1.3520, val loss = 1.6749\n",
            "step 51580: train loss = 1.2339, val loss = 1.2184\n",
            "step 51590: train loss = 1.4306, val loss = 1.6117\n",
            "step 51600: train loss = 1.4322, val loss = 1.5625\n",
            "step 51610: train loss = 1.0101, val loss = 1.3029\n",
            "step 51620: train loss = 1.3899, val loss = 1.7012\n",
            "step 51630: train loss = 1.3701, val loss = 1.3729\n",
            "step 51640: train loss = 1.0359, val loss = 1.8661\n",
            "step 51650: train loss = 1.3661, val loss = 1.3717\n",
            "step 51660: train loss = 1.2575, val loss = 1.6611\n",
            "step 51670: train loss = 1.4007, val loss = 1.5657\n",
            "step 51680: train loss = 1.5251, val loss = 1.4670\n",
            "step 51690: train loss = 1.0863, val loss = 1.4677\n",
            "step 51700: train loss = 1.1770, val loss = 1.4162\n",
            "step 51710: train loss = 1.2299, val loss = 1.3455\n",
            "step 51720: train loss = 1.1966, val loss = 1.4793\n",
            "step 51730: train loss = 1.2240, val loss = 1.7050\n",
            "step 51740: train loss = 1.3332, val loss = 1.6491\n",
            "step 51750: train loss = 1.2080, val loss = 1.2921\n",
            "step 51760: train loss = 0.8589, val loss = 1.0809\n",
            "step 51770: train loss = 1.0363, val loss = 1.2677\n",
            "step 51780: train loss = 1.0150, val loss = 1.3293\n",
            "step 51790: train loss = 1.3126, val loss = 1.7153\n",
            "step 51800: train loss = 1.0041, val loss = 1.8014\n",
            "step 51810: train loss = 1.2770, val loss = 1.6670\n",
            "step 51820: train loss = 1.4486, val loss = 1.5985\n",
            "step 51830: train loss = 1.3888, val loss = 1.5911\n",
            "step 51840: train loss = 1.0580, val loss = 1.3724\n",
            "step 51850: train loss = 0.9135, val loss = 1.5983\n",
            "step 51860: train loss = 1.1209, val loss = 1.5327\n",
            "step 51870: train loss = 1.2412, val loss = 1.2045\n",
            "step 51880: train loss = 1.4514, val loss = 1.5988\n",
            "step 51890: train loss = 1.3048, val loss = 1.7036\n",
            "step 51900: train loss = 0.9748, val loss = 1.6555\n",
            "step 51910: train loss = 1.1340, val loss = 1.5921\n",
            "step 51920: train loss = 0.7934, val loss = 1.7460\n",
            "step 51930: train loss = 1.1194, val loss = 1.6436\n",
            "step 51940: train loss = 1.0830, val loss = 1.9633\n",
            "step 51950: train loss = 1.3041, val loss = 1.2790\n",
            "step 51960: train loss = 1.1967, val loss = 1.5213\n",
            "step 51970: train loss = 1.5630, val loss = 1.1041\n",
            "step 51980: train loss = 1.0109, val loss = 1.2889\n",
            "step 51990: train loss = 0.9981, val loss = 1.4276\n",
            "step 52000: train loss = 1.2724, val loss = 1.6469\n",
            "step 52010: train loss = 1.1894, val loss = 1.5369\n",
            "step 52020: train loss = 0.9614, val loss = 1.7140\n",
            "step 52030: train loss = 1.0871, val loss = 1.1409\n",
            "step 52040: train loss = 1.3634, val loss = 1.6188\n",
            "step 52050: train loss = 1.2386, val loss = 1.5083\n",
            "step 52060: train loss = 1.2822, val loss = 1.8949\n",
            "step 52070: train loss = 0.7661, val loss = 1.5540\n",
            "step 52080: train loss = 1.3162, val loss = 1.6483\n",
            "step 52090: train loss = 1.1368, val loss = 1.5012\n",
            "step 52100: train loss = 1.2038, val loss = 1.2916\n",
            "step 52110: train loss = 1.1696, val loss = 1.4159\n",
            "step 52120: train loss = 1.1062, val loss = 1.4158\n",
            "step 52130: train loss = 1.1698, val loss = 1.3029\n",
            "step 52140: train loss = 1.0711, val loss = 1.5130\n",
            "step 52150: train loss = 1.2847, val loss = 1.2578\n",
            "step 52160: train loss = 0.9794, val loss = 1.3564\n",
            "step 52170: train loss = 1.4563, val loss = 1.3052\n",
            "step 52180: train loss = 1.1939, val loss = 1.2076\n",
            "step 52190: train loss = 0.9691, val loss = 1.4793\n",
            "step 52200: train loss = 1.2761, val loss = 1.6475\n",
            "step 52210: train loss = 1.1991, val loss = 1.9738\n",
            "step 52220: train loss = 0.8933, val loss = 1.8025\n",
            "step 52230: train loss = 0.7302, val loss = 1.4254\n",
            "step 52240: train loss = 1.1567, val loss = 1.5252\n",
            "step 52250: train loss = 1.3189, val loss = 1.3911\n",
            "step 52260: train loss = 1.3342, val loss = 1.4226\n",
            "step 52270: train loss = 1.0620, val loss = 1.9602\n",
            "step 52280: train loss = 1.1620, val loss = 1.4945\n",
            "step 52290: train loss = 1.1928, val loss = 1.0802\n",
            "step 52300: train loss = 0.9825, val loss = 1.1729\n",
            "step 52310: train loss = 1.1809, val loss = 1.5546\n",
            "step 52320: train loss = 1.2106, val loss = 1.5186\n",
            "step 52330: train loss = 0.8543, val loss = 1.6908\n",
            "step 52340: train loss = 1.6279, val loss = 1.7096\n",
            "step 52350: train loss = 1.0893, val loss = 1.8209\n",
            "step 52360: train loss = 1.3274, val loss = 1.3631\n",
            "step 52370: train loss = 1.0758, val loss = 1.9328\n",
            "step 52380: train loss = 1.0616, val loss = 1.6614\n",
            "step 52390: train loss = 1.1583, val loss = 1.2384\n",
            "step 52400: train loss = 1.1634, val loss = 1.3522\n",
            "step 52410: train loss = 1.3466, val loss = 1.4023\n",
            "step 52420: train loss = 1.2799, val loss = 1.7057\n",
            "step 52430: train loss = 1.0002, val loss = 1.0137\n",
            "step 52440: train loss = 1.2948, val loss = 1.3361\n",
            "step 52450: train loss = 1.2943, val loss = 1.2986\n",
            "step 52460: train loss = 1.5701, val loss = 1.5735\n",
            "step 52470: train loss = 1.2861, val loss = 1.3326\n",
            "step 52480: train loss = 1.4726, val loss = 1.2799\n",
            "step 52490: train loss = 1.0269, val loss = 1.6183\n",
            "step 52500: train loss = 1.0264, val loss = 1.3830\n",
            "step 52510: train loss = 1.3640, val loss = 1.5564\n",
            "step 52520: train loss = 1.1886, val loss = 1.5028\n",
            "step 52530: train loss = 1.2010, val loss = 1.7689\n",
            "step 52540: train loss = 1.1343, val loss = 1.5709\n",
            "step 52550: train loss = 1.2242, val loss = 1.2432\n",
            "step 52560: train loss = 1.5387, val loss = 1.7535\n",
            "step 52570: train loss = 0.9304, val loss = 1.8790\n",
            "step 52580: train loss = 0.6071, val loss = 1.4360\n",
            "step 52590: train loss = 1.2674, val loss = 1.7550\n",
            "step 52600: train loss = 1.3501, val loss = 1.3039\n",
            "step 52610: train loss = 1.5674, val loss = 1.3430\n",
            "step 52620: train loss = 1.4301, val loss = 1.3541\n",
            "step 52630: train loss = 1.2816, val loss = 1.8917\n",
            "step 52640: train loss = 0.9032, val loss = 1.2480\n",
            "step 52650: train loss = 1.3529, val loss = 1.2321\n",
            "step 52660: train loss = 0.9081, val loss = 1.2208\n",
            "step 52670: train loss = 1.3827, val loss = 1.2753\n",
            "step 52680: train loss = 1.2850, val loss = 1.2672\n",
            "step 52690: train loss = 1.1493, val loss = 2.0457\n",
            "step 52700: train loss = 1.3305, val loss = 1.4540\n",
            "step 52710: train loss = 0.9847, val loss = 1.8770\n",
            "step 52720: train loss = 1.1560, val loss = 0.8219\n",
            "step 52730: train loss = 1.2801, val loss = 1.5868\n",
            "step 52740: train loss = 0.8916, val loss = 1.3682\n",
            "step 52750: train loss = 0.7755, val loss = 1.7184\n",
            "step 52760: train loss = 1.3571, val loss = 1.5027\n",
            "step 52770: train loss = 1.0433, val loss = 1.4205\n",
            "step 52780: train loss = 1.0240, val loss = 1.8044\n",
            "step 52790: train loss = 1.0687, val loss = 1.3628\n",
            "step 52800: train loss = 1.2623, val loss = 1.3678\n",
            "step 52810: train loss = 1.1698, val loss = 1.4457\n",
            "step 52820: train loss = 1.3867, val loss = 1.7266\n",
            "step 52830: train loss = 1.0814, val loss = 1.6757\n",
            "step 52840: train loss = 0.9339, val loss = 1.2675\n",
            "step 52850: train loss = 1.0180, val loss = 1.5757\n",
            "step 52860: train loss = 1.1280, val loss = 1.3723\n",
            "step 52870: train loss = 0.9585, val loss = 1.5814\n",
            "step 52880: train loss = 1.3741, val loss = 1.3517\n",
            "step 52890: train loss = 1.5923, val loss = 1.3621\n",
            "step 52900: train loss = 0.8899, val loss = 1.4170\n",
            "step 52910: train loss = 1.1278, val loss = 1.5425\n",
            "step 52920: train loss = 1.3484, val loss = 1.2778\n",
            "step 52930: train loss = 1.1686, val loss = 1.0601\n",
            "step 52940: train loss = 1.0466, val loss = 1.1091\n",
            "step 52950: train loss = 1.3874, val loss = 1.4830\n",
            "step 52960: train loss = 1.6030, val loss = 1.7834\n",
            "step 52970: train loss = 1.2154, val loss = 1.4437\n",
            "step 52980: train loss = 1.3170, val loss = 1.7895\n",
            "step 52990: train loss = 1.3534, val loss = 1.3065\n",
            "step 53000: train loss = 1.0274, val loss = 1.2257\n",
            "step 53010: train loss = 0.8504, val loss = 1.8821\n",
            "step 53020: train loss = 1.2874, val loss = 1.5111\n",
            "step 53030: train loss = 1.3145, val loss = 1.3527\n",
            "step 53040: train loss = 1.0125, val loss = 1.4188\n",
            "step 53050: train loss = 1.1657, val loss = 1.5354\n",
            "step 53060: train loss = 1.4303, val loss = 1.3337\n",
            "step 53070: train loss = 1.2017, val loss = 1.8374\n",
            "step 53080: train loss = 0.9555, val loss = 1.5348\n",
            "step 53090: train loss = 1.2017, val loss = 1.6132\n",
            "step 53100: train loss = 1.0096, val loss = 1.6548\n",
            "step 53110: train loss = 0.8514, val loss = 1.2974\n",
            "step 53120: train loss = 1.0588, val loss = 1.8480\n",
            "step 53130: train loss = 1.0551, val loss = 1.5271\n",
            "step 53140: train loss = 1.3912, val loss = 1.1666\n",
            "step 53150: train loss = 0.8512, val loss = 1.6170\n",
            "step 53160: train loss = 0.8768, val loss = 1.4453\n",
            "step 53170: train loss = 1.1936, val loss = 1.2454\n",
            "step 53180: train loss = 1.2366, val loss = 1.1243\n",
            "step 53190: train loss = 1.1361, val loss = 1.9021\n",
            "step 53200: train loss = 1.4455, val loss = 1.8028\n",
            "step 53210: train loss = 1.0737, val loss = 1.3993\n",
            "step 53220: train loss = 0.9132, val loss = 1.7083\n",
            "step 53230: train loss = 1.3012, val loss = 2.0780\n",
            "step 53240: train loss = 0.9755, val loss = 1.4966\n",
            "step 53250: train loss = 1.3805, val loss = 1.6355\n",
            "step 53260: train loss = 1.2555, val loss = 1.6135\n",
            "step 53270: train loss = 1.1642, val loss = 1.2859\n",
            "step 53280: train loss = 1.2583, val loss = 1.8994\n",
            "step 53290: train loss = 1.5375, val loss = 1.7766\n",
            "step 53300: train loss = 1.2097, val loss = 1.3975\n",
            "step 53310: train loss = 1.0659, val loss = 2.0656\n",
            "step 53320: train loss = 0.7419, val loss = 1.8354\n",
            "step 53330: train loss = 1.0344, val loss = 1.2459\n",
            "step 53340: train loss = 1.1655, val loss = 1.3755\n",
            "step 53350: train loss = 1.4147, val loss = 1.4326\n",
            "step 53360: train loss = 1.1219, val loss = 1.2272\n",
            "step 53370: train loss = 1.0132, val loss = 1.6890\n",
            "step 53380: train loss = 1.1213, val loss = 1.4419\n",
            "step 53390: train loss = 1.3218, val loss = 1.1641\n",
            "step 53400: train loss = 1.1140, val loss = 1.6090\n",
            "step 53410: train loss = 1.3133, val loss = 1.7471\n",
            "step 53420: train loss = 1.3177, val loss = 1.3704\n",
            "step 53430: train loss = 0.9783, val loss = 1.5910\n",
            "step 53440: train loss = 1.1170, val loss = 1.5965\n",
            "step 53450: train loss = 1.1659, val loss = 1.6340\n",
            "step 53460: train loss = 0.9577, val loss = 1.5110\n",
            "step 53470: train loss = 1.2908, val loss = 1.3508\n",
            "step 53480: train loss = 1.1673, val loss = 1.6959\n",
            "step 53490: train loss = 0.9223, val loss = 1.2033\n",
            "step 53500: train loss = 1.0706, val loss = 1.3892\n",
            "step 53510: train loss = 1.2797, val loss = 1.3587\n",
            "step 53520: train loss = 1.0821, val loss = 1.0449\n",
            "step 53530: train loss = 1.0265, val loss = 1.5821\n",
            "step 53540: train loss = 1.1482, val loss = 1.3313\n",
            "step 53550: train loss = 0.9573, val loss = 1.8022\n",
            "step 53560: train loss = 1.1857, val loss = 1.5727\n",
            "step 53570: train loss = 1.2569, val loss = 1.6018\n",
            "step 53580: train loss = 1.1769, val loss = 2.0700\n",
            "step 53590: train loss = 1.1212, val loss = 1.5829\n",
            "step 53600: train loss = 1.3783, val loss = 1.5663\n",
            "step 53610: train loss = 1.2643, val loss = 1.8062\n",
            "step 53620: train loss = 1.0995, val loss = 1.4804\n",
            "step 53630: train loss = 1.1237, val loss = 1.4766\n",
            "step 53640: train loss = 1.0590, val loss = 1.4440\n",
            "step 53650: train loss = 1.1145, val loss = 1.6233\n",
            "step 53660: train loss = 1.1731, val loss = 1.1085\n",
            "step 53670: train loss = 1.4646, val loss = 1.3447\n",
            "step 53680: train loss = 1.0063, val loss = 1.3432\n",
            "step 53690: train loss = 1.3178, val loss = 1.2464\n",
            "step 53700: train loss = 1.1177, val loss = 1.4262\n",
            "step 53710: train loss = 1.4547, val loss = 1.5533\n",
            "step 53720: train loss = 1.2819, val loss = 1.8022\n",
            "step 53730: train loss = 1.1492, val loss = 1.5998\n",
            "step 53740: train loss = 1.5190, val loss = 1.6575\n",
            "step 53750: train loss = 1.1576, val loss = 1.5671\n",
            "step 53760: train loss = 1.1413, val loss = 1.6357\n",
            "step 53770: train loss = 1.0748, val loss = 1.3486\n",
            "step 53780: train loss = 1.2440, val loss = 1.3559\n",
            "step 53790: train loss = 1.1297, val loss = 1.5924\n",
            "step 53800: train loss = 1.3026, val loss = 1.5270\n",
            "step 53810: train loss = 1.4480, val loss = 1.7829\n",
            "step 53820: train loss = 1.1897, val loss = 1.1033\n",
            "step 53830: train loss = 1.2978, val loss = 1.4629\n",
            "step 53840: train loss = 1.0174, val loss = 1.3039\n",
            "step 53850: train loss = 1.2218, val loss = 1.4691\n",
            "step 53860: train loss = 1.2979, val loss = 1.8327\n",
            "step 53870: train loss = 1.0313, val loss = 1.3590\n",
            "step 53880: train loss = 1.0162, val loss = 1.0736\n",
            "step 53890: train loss = 1.2050, val loss = 1.3940\n",
            "step 53900: train loss = 1.2246, val loss = 1.4417\n",
            "step 53910: train loss = 1.3210, val loss = 1.6953\n",
            "step 53920: train loss = 1.2331, val loss = 1.4500\n",
            "step 53930: train loss = 1.0309, val loss = 1.3180\n",
            "step 53940: train loss = 1.3789, val loss = 1.4183\n",
            "step 53950: train loss = 0.6085, val loss = 1.3458\n",
            "step 53960: train loss = 1.1316, val loss = 1.1308\n",
            "step 53970: train loss = 1.2025, val loss = 1.6317\n",
            "step 53980: train loss = 1.1545, val loss = 1.4539\n",
            "step 53990: train loss = 1.4044, val loss = 1.2956\n",
            "step 54000: train loss = 1.6486, val loss = 1.3565\n",
            "step 54010: train loss = 1.6824, val loss = 1.9176\n",
            "step 54020: train loss = 1.1280, val loss = 1.6818\n",
            "step 54030: train loss = 1.3695, val loss = 1.2011\n",
            "step 54040: train loss = 1.3960, val loss = 1.9534\n",
            "step 54050: train loss = 1.2195, val loss = 1.6183\n",
            "step 54060: train loss = 0.6916, val loss = 2.1539\n",
            "step 54070: train loss = 0.8096, val loss = 1.3149\n",
            "step 54080: train loss = 1.2180, val loss = 1.1259\n",
            "step 54090: train loss = 1.2059, val loss = 1.4369\n",
            "step 54100: train loss = 1.0283, val loss = 0.9791\n",
            "step 54110: train loss = 1.3623, val loss = 1.4020\n",
            "step 54120: train loss = 1.4265, val loss = 1.0687\n",
            "step 54130: train loss = 1.2281, val loss = 1.5882\n",
            "step 54140: train loss = 1.0953, val loss = 1.6759\n",
            "step 54150: train loss = 1.3969, val loss = 1.9163\n",
            "step 54160: train loss = 1.2212, val loss = 1.3215\n",
            "step 54170: train loss = 0.8870, val loss = 1.3206\n",
            "step 54180: train loss = 1.3526, val loss = 1.4222\n",
            "step 54190: train loss = 1.0618, val loss = 1.7287\n",
            "step 54200: train loss = 1.1760, val loss = 1.6278\n",
            "step 54210: train loss = 1.0380, val loss = 1.2485\n",
            "step 54220: train loss = 1.0570, val loss = 1.6072\n",
            "step 54230: train loss = 0.9510, val loss = 1.4911\n",
            "step 54240: train loss = 1.2284, val loss = 1.7414\n",
            "step 54250: train loss = 1.0514, val loss = 1.8396\n",
            "step 54260: train loss = 1.1773, val loss = 1.9544\n",
            "step 54270: train loss = 1.2718, val loss = 1.2841\n",
            "step 54280: train loss = 1.0980, val loss = 1.5976\n",
            "step 54290: train loss = 1.0752, val loss = 1.8617\n",
            "step 54300: train loss = 1.1754, val loss = 1.5173\n",
            "step 54310: train loss = 1.0964, val loss = 1.3552\n",
            "step 54320: train loss = 1.3797, val loss = 2.1118\n",
            "step 54330: train loss = 0.9939, val loss = 1.3265\n",
            "step 54340: train loss = 0.9819, val loss = 1.1796\n",
            "step 54350: train loss = 1.1222, val loss = 1.7775\n",
            "step 54360: train loss = 1.1717, val loss = 1.4228\n",
            "step 54370: train loss = 0.8041, val loss = 1.8840\n",
            "step 54380: train loss = 1.1991, val loss = 1.4689\n",
            "step 54390: train loss = 1.1279, val loss = 1.3397\n",
            "step 54400: train loss = 0.9332, val loss = 1.4676\n",
            "step 54410: train loss = 1.2167, val loss = 1.3367\n",
            "step 54420: train loss = 1.1441, val loss = 1.5159\n",
            "step 54430: train loss = 0.9886, val loss = 1.1685\n",
            "step 54440: train loss = 1.2423, val loss = 1.2151\n",
            "step 54450: train loss = 1.0534, val loss = 1.8650\n",
            "step 54460: train loss = 1.2306, val loss = 1.2751\n",
            "step 54470: train loss = 1.1734, val loss = 1.4191\n",
            "step 54480: train loss = 1.3634, val loss = 1.3854\n",
            "step 54490: train loss = 1.3700, val loss = 1.4379\n",
            "step 54500: train loss = 1.2337, val loss = 1.7865\n",
            "step 54510: train loss = 1.0642, val loss = 1.7430\n",
            "step 54520: train loss = 1.1178, val loss = 1.7798\n",
            "step 54530: train loss = 1.3484, val loss = 1.1395\n",
            "step 54540: train loss = 0.9642, val loss = 1.8180\n",
            "step 54550: train loss = 0.8522, val loss = 1.4517\n",
            "step 54560: train loss = 1.2719, val loss = 1.9245\n",
            "step 54570: train loss = 1.1229, val loss = 1.6016\n",
            "step 54580: train loss = 1.1088, val loss = 1.5847\n",
            "step 54590: train loss = 1.0838, val loss = 1.8946\n",
            "step 54600: train loss = 1.1190, val loss = 1.6593\n",
            "step 54610: train loss = 1.2191, val loss = 1.3762\n",
            "step 54620: train loss = 1.4416, val loss = 1.3519\n",
            "step 54630: train loss = 0.7960, val loss = 1.9483\n",
            "step 54640: train loss = 1.2030, val loss = 1.3109\n",
            "step 54650: train loss = 1.4445, val loss = 1.2217\n",
            "step 54660: train loss = 0.8829, val loss = 1.0274\n",
            "step 54670: train loss = 1.2386, val loss = 1.2485\n",
            "step 54680: train loss = 1.1974, val loss = 1.5656\n",
            "step 54690: train loss = 0.7942, val loss = 1.1547\n",
            "step 54700: train loss = 1.0365, val loss = 1.8237\n",
            "step 54710: train loss = 1.1387, val loss = 1.5231\n",
            "step 54720: train loss = 1.1135, val loss = 1.6878\n",
            "step 54730: train loss = 1.3128, val loss = 1.5544\n",
            "step 54740: train loss = 1.0560, val loss = 2.0390\n",
            "step 54750: train loss = 1.0071, val loss = 1.2366\n",
            "step 54760: train loss = 0.9938, val loss = 1.2618\n",
            "step 54770: train loss = 0.9168, val loss = 1.4706\n",
            "step 54780: train loss = 0.6835, val loss = 1.2937\n",
            "step 54790: train loss = 1.0527, val loss = 1.4022\n",
            "step 54800: train loss = 0.7054, val loss = 1.3000\n",
            "step 54810: train loss = 1.1553, val loss = 1.7727\n",
            "step 54820: train loss = 1.1378, val loss = 1.6559\n",
            "step 54830: train loss = 0.9568, val loss = 1.1411\n",
            "step 54840: train loss = 1.4633, val loss = 1.2470\n",
            "step 54850: train loss = 0.9712, val loss = 1.6951\n",
            "step 54860: train loss = 0.9444, val loss = 1.4001\n",
            "step 54870: train loss = 1.0933, val loss = 1.4798\n",
            "step 54880: train loss = 1.3536, val loss = 1.5631\n",
            "step 54890: train loss = 0.9463, val loss = 1.2132\n",
            "step 54900: train loss = 1.2884, val loss = 1.6024\n",
            "step 54910: train loss = 1.2043, val loss = 1.2871\n",
            "step 54920: train loss = 1.0598, val loss = 1.3528\n",
            "step 54930: train loss = 1.0267, val loss = 1.3340\n",
            "step 54940: train loss = 1.0446, val loss = 1.2260\n",
            "step 54950: train loss = 0.9205, val loss = 1.2358\n",
            "step 54960: train loss = 1.2680, val loss = 1.9234\n",
            "step 54970: train loss = 1.0332, val loss = 1.3940\n",
            "step 54980: train loss = 1.6903, val loss = 1.2688\n",
            "step 54990: train loss = 1.6704, val loss = 1.7749\n",
            "step 55000: train loss = 0.9147, val loss = 1.6918\n",
            "step 55010: train loss = 1.1351, val loss = 1.4620\n",
            "step 55020: train loss = 1.0327, val loss = 1.4342\n",
            "step 55030: train loss = 0.9238, val loss = 1.3054\n",
            "step 55040: train loss = 1.3215, val loss = 1.6467\n",
            "step 55050: train loss = 0.9393, val loss = 1.5393\n",
            "step 55060: train loss = 1.3270, val loss = 1.7003\n",
            "step 55070: train loss = 1.3770, val loss = 1.6393\n",
            "step 55080: train loss = 1.1411, val loss = 1.6954\n",
            "step 55090: train loss = 0.9887, val loss = 1.4602\n",
            "step 55100: train loss = 1.1711, val loss = 1.3409\n",
            "step 55110: train loss = 1.1398, val loss = 1.6404\n",
            "step 55120: train loss = 0.6558, val loss = 1.6885\n",
            "step 55130: train loss = 1.2952, val loss = 1.6861\n",
            "step 55140: train loss = 1.4033, val loss = 1.5646\n",
            "step 55150: train loss = 1.3800, val loss = 1.4941\n",
            "step 55160: train loss = 1.1054, val loss = 1.5482\n",
            "step 55170: train loss = 1.1283, val loss = 1.6312\n",
            "step 55180: train loss = 0.9199, val loss = 1.6584\n",
            "step 55190: train loss = 1.2520, val loss = 1.5241\n",
            "step 55200: train loss = 1.1939, val loss = 2.0183\n",
            "step 55210: train loss = 0.9180, val loss = 1.5762\n",
            "step 55220: train loss = 1.1539, val loss = 1.8487\n",
            "step 55230: train loss = 0.9583, val loss = 1.5816\n",
            "step 55240: train loss = 1.0254, val loss = 1.2327\n",
            "step 55250: train loss = 1.4533, val loss = 2.3958\n",
            "step 55260: train loss = 1.3083, val loss = 1.6314\n",
            "step 55270: train loss = 1.0497, val loss = 1.2420\n",
            "step 55280: train loss = 1.1441, val loss = 1.5856\n",
            "step 55290: train loss = 1.4536, val loss = 1.4188\n",
            "step 55300: train loss = 1.1584, val loss = 1.6850\n",
            "step 55310: train loss = 1.1912, val loss = 1.4798\n",
            "step 55320: train loss = 1.1899, val loss = 1.1287\n",
            "step 55330: train loss = 0.9638, val loss = 1.8952\n",
            "step 55340: train loss = 1.2251, val loss = 1.4581\n",
            "step 55350: train loss = 1.3113, val loss = 1.3800\n",
            "step 55360: train loss = 1.1519, val loss = 1.3176\n",
            "step 55370: train loss = 0.9800, val loss = 1.3784\n",
            "step 55380: train loss = 1.2599, val loss = 1.8443\n",
            "step 55390: train loss = 1.1168, val loss = 1.7205\n",
            "step 55400: train loss = 1.3537, val loss = 1.3647\n",
            "step 55410: train loss = 1.0701, val loss = 1.6027\n",
            "step 55420: train loss = 1.1645, val loss = 1.4490\n",
            "step 55430: train loss = 1.1556, val loss = 1.7988\n",
            "step 55440: train loss = 1.5735, val loss = 1.3639\n",
            "step 55450: train loss = 1.0524, val loss = 1.5604\n",
            "step 55460: train loss = 0.6840, val loss = 1.3460\n",
            "step 55470: train loss = 1.1989, val loss = 1.6338\n",
            "step 55480: train loss = 1.1496, val loss = 1.4430\n",
            "step 55490: train loss = 1.1763, val loss = 1.5023\n",
            "step 55500: train loss = 1.2003, val loss = 1.2843\n",
            "step 55510: train loss = 0.9310, val loss = 1.7763\n",
            "step 55520: train loss = 0.8699, val loss = 1.3079\n",
            "step 55530: train loss = 0.8948, val loss = 1.8048\n",
            "step 55540: train loss = 0.9199, val loss = 1.9743\n",
            "step 55550: train loss = 1.4791, val loss = 1.1688\n",
            "step 55560: train loss = 1.1282, val loss = 1.4315\n",
            "step 55570: train loss = 1.2261, val loss = 1.1915\n",
            "step 55580: train loss = 1.0005, val loss = 1.3536\n",
            "step 55590: train loss = 1.1606, val loss = 1.2994\n",
            "step 55600: train loss = 1.2500, val loss = 1.5925\n",
            "step 55610: train loss = 1.0112, val loss = 1.4258\n",
            "step 55620: train loss = 1.4973, val loss = 1.9685\n",
            "step 55630: train loss = 1.0530, val loss = 1.4505\n",
            "step 55640: train loss = 1.0159, val loss = 1.3071\n",
            "step 55650: train loss = 1.2407, val loss = 1.4119\n",
            "step 55660: train loss = 1.0551, val loss = 1.3166\n",
            "step 55670: train loss = 1.3350, val loss = 1.2881\n",
            "step 55680: train loss = 0.9606, val loss = 1.1357\n",
            "step 55690: train loss = 0.9178, val loss = 1.7246\n",
            "step 55700: train loss = 0.9636, val loss = 1.5724\n",
            "step 55710: train loss = 0.9809, val loss = 1.6910\n",
            "step 55720: train loss = 1.0548, val loss = 1.4673\n",
            "step 55730: train loss = 1.3159, val loss = 1.4849\n",
            "step 55740: train loss = 1.0713, val loss = 1.5304\n",
            "step 55750: train loss = 0.9130, val loss = 1.4678\n",
            "step 55760: train loss = 1.1737, val loss = 1.7674\n",
            "step 55770: train loss = 1.2500, val loss = 1.5893\n",
            "step 55780: train loss = 1.2045, val loss = 1.5542\n",
            "step 55790: train loss = 1.2813, val loss = 1.7597\n",
            "step 55800: train loss = 0.7741, val loss = 1.9352\n",
            "step 55810: train loss = 0.9318, val loss = 1.3175\n",
            "step 55820: train loss = 1.0811, val loss = 1.5667\n",
            "step 55830: train loss = 1.2759, val loss = 1.7639\n",
            "step 55840: train loss = 1.3856, val loss = 1.4914\n",
            "step 55850: train loss = 1.2573, val loss = 1.1061\n",
            "step 55860: train loss = 1.6097, val loss = 1.4961\n",
            "step 55870: train loss = 1.2262, val loss = 1.2995\n",
            "step 55880: train loss = 1.0589, val loss = 1.2425\n",
            "step 55890: train loss = 0.8198, val loss = 1.8203\n",
            "step 55900: train loss = 1.4965, val loss = 1.4659\n",
            "step 55910: train loss = 1.1458, val loss = 1.4672\n",
            "step 55920: train loss = 1.0977, val loss = 1.3873\n",
            "step 55930: train loss = 1.3501, val loss = 1.2450\n",
            "step 55940: train loss = 0.7746, val loss = 1.9950\n",
            "step 55950: train loss = 1.4242, val loss = 2.0971\n",
            "step 55960: train loss = 1.2215, val loss = 1.2963\n",
            "step 55970: train loss = 1.0199, val loss = 1.5409\n",
            "step 55980: train loss = 0.9269, val loss = 1.2794\n",
            "step 55990: train loss = 0.6601, val loss = 1.7234\n",
            "step 56000: train loss = 1.1371, val loss = 1.4363\n",
            "step 56010: train loss = 1.1752, val loss = 0.9432\n",
            "step 56020: train loss = 1.1229, val loss = 1.5985\n",
            "step 56030: train loss = 1.0269, val loss = 1.6414\n",
            "step 56040: train loss = 1.5439, val loss = 1.7333\n",
            "step 56050: train loss = 1.0066, val loss = 1.7713\n",
            "step 56060: train loss = 0.8645, val loss = 1.6179\n",
            "step 56070: train loss = 1.1193, val loss = 1.5040\n",
            "step 56080: train loss = 1.2685, val loss = 1.4498\n",
            "step 56090: train loss = 0.8469, val loss = 1.9297\n",
            "step 56100: train loss = 1.4671, val loss = 1.4245\n",
            "step 56110: train loss = 1.1053, val loss = 1.7708\n",
            "step 56120: train loss = 1.2616, val loss = 1.5707\n",
            "step 56130: train loss = 1.0417, val loss = 1.3985\n",
            "step 56140: train loss = 0.9064, val loss = 1.6966\n",
            "step 56150: train loss = 1.2054, val loss = 1.6435\n",
            "step 56160: train loss = 0.9053, val loss = 1.8020\n",
            "step 56170: train loss = 1.4492, val loss = 1.7653\n",
            "step 56180: train loss = 1.4490, val loss = 1.7716\n",
            "step 56190: train loss = 1.0171, val loss = 1.2324\n",
            "step 56200: train loss = 1.4760, val loss = 1.3174\n",
            "step 56210: train loss = 1.3089, val loss = 1.7282\n",
            "step 56220: train loss = 0.9822, val loss = 1.1079\n",
            "step 56230: train loss = 1.0805, val loss = 1.8150\n",
            "step 56240: train loss = 0.8350, val loss = 1.5126\n",
            "step 56250: train loss = 1.5267, val loss = 1.6839\n",
            "step 56260: train loss = 1.3617, val loss = 1.9090\n",
            "step 56270: train loss = 1.5005, val loss = 1.6761\n",
            "step 56280: train loss = 1.0368, val loss = 1.7701\n",
            "step 56290: train loss = 0.9992, val loss = 1.7677\n",
            "step 56300: train loss = 1.0569, val loss = 1.8050\n",
            "step 56310: train loss = 1.0730, val loss = 1.4531\n",
            "step 56320: train loss = 1.2858, val loss = 1.3492\n",
            "step 56330: train loss = 0.8323, val loss = 1.2692\n",
            "step 56340: train loss = 1.1631, val loss = 1.2817\n",
            "step 56350: train loss = 1.0023, val loss = 1.4725\n",
            "step 56360: train loss = 1.2109, val loss = 1.4878\n",
            "step 56370: train loss = 1.1366, val loss = 1.4774\n",
            "step 56380: train loss = 1.0580, val loss = 1.3221\n",
            "step 56390: train loss = 1.0943, val loss = 1.8427\n",
            "step 56400: train loss = 1.0473, val loss = 1.5505\n",
            "step 56410: train loss = 1.1467, val loss = 1.3232\n",
            "step 56420: train loss = 0.9972, val loss = 1.5150\n",
            "step 56430: train loss = 0.8862, val loss = 1.7365\n",
            "step 56440: train loss = 1.2097, val loss = 1.5723\n",
            "step 56450: train loss = 1.0876, val loss = 1.8912\n",
            "step 56460: train loss = 0.8458, val loss = 1.7020\n",
            "step 56470: train loss = 1.0768, val loss = 1.5144\n",
            "step 56480: train loss = 1.1060, val loss = 1.4053\n",
            "step 56490: train loss = 1.0393, val loss = 2.0364\n",
            "step 56500: train loss = 1.0037, val loss = 1.4682\n",
            "step 56510: train loss = 1.2783, val loss = 1.8363\n",
            "step 56520: train loss = 1.1907, val loss = 1.7748\n",
            "step 56530: train loss = 1.0780, val loss = 1.2867\n",
            "step 56540: train loss = 1.4511, val loss = 1.8924\n",
            "step 56550: train loss = 1.3173, val loss = 1.4614\n",
            "step 56560: train loss = 0.9697, val loss = 1.3906\n",
            "step 56570: train loss = 1.1104, val loss = 1.6572\n",
            "step 56580: train loss = 1.0281, val loss = 1.9749\n",
            "step 56590: train loss = 1.2217, val loss = 1.4103\n",
            "step 56600: train loss = 1.0541, val loss = 1.5357\n",
            "step 56610: train loss = 0.9318, val loss = 1.7202\n",
            "step 56620: train loss = 1.3572, val loss = 2.1243\n",
            "step 56630: train loss = 1.1778, val loss = 1.9761\n",
            "step 56640: train loss = 1.2707, val loss = 1.0220\n",
            "step 56650: train loss = 0.8820, val loss = 1.4323\n",
            "step 56660: train loss = 1.2072, val loss = 1.1370\n",
            "step 56670: train loss = 1.2759, val loss = 1.0824\n",
            "step 56680: train loss = 1.1959, val loss = 1.2534\n",
            "step 56690: train loss = 1.4894, val loss = 1.5105\n",
            "step 56700: train loss = 1.2909, val loss = 1.3366\n",
            "step 56710: train loss = 1.4184, val loss = 1.3406\n",
            "step 56720: train loss = 1.2528, val loss = 1.3243\n",
            "step 56730: train loss = 1.0161, val loss = 1.5676\n",
            "step 56740: train loss = 0.9677, val loss = 1.5545\n",
            "step 56750: train loss = 1.0930, val loss = 1.2977\n",
            "step 56760: train loss = 0.7782, val loss = 1.1432\n",
            "step 56770: train loss = 1.0211, val loss = 1.6234\n",
            "step 56780: train loss = 1.3422, val loss = 1.6848\n",
            "step 56790: train loss = 1.0540, val loss = 1.7867\n",
            "step 56800: train loss = 0.8964, val loss = 1.1872\n",
            "step 56810: train loss = 1.6968, val loss = 1.6526\n",
            "step 56820: train loss = 1.1825, val loss = 1.0717\n",
            "step 56830: train loss = 1.2584, val loss = 1.4475\n",
            "step 56840: train loss = 1.0260, val loss = 1.5127\n",
            "step 56850: train loss = 1.2235, val loss = 1.2619\n",
            "step 56860: train loss = 1.1248, val loss = 1.6694\n",
            "step 56870: train loss = 1.1505, val loss = 1.7385\n",
            "step 56880: train loss = 0.9167, val loss = 1.6115\n",
            "step 56890: train loss = 1.0819, val loss = 1.0850\n",
            "step 56900: train loss = 1.3959, val loss = 1.7375\n",
            "step 56910: train loss = 1.2839, val loss = 1.7796\n",
            "step 56920: train loss = 1.0019, val loss = 1.6135\n",
            "step 56930: train loss = 1.0923, val loss = 1.1894\n",
            "step 56940: train loss = 0.9630, val loss = 1.2888\n",
            "step 56950: train loss = 1.2301, val loss = 1.4608\n",
            "step 56960: train loss = 0.9279, val loss = 1.3714\n",
            "step 56970: train loss = 0.8416, val loss = 1.3953\n",
            "step 56980: train loss = 0.9870, val loss = 1.6019\n",
            "step 56990: train loss = 1.1846, val loss = 1.0857\n",
            "step 57000: train loss = 0.9677, val loss = 1.2942\n",
            "step 57010: train loss = 0.8233, val loss = 1.3241\n",
            "step 57020: train loss = 1.0208, val loss = 1.6504\n",
            "step 57030: train loss = 1.1585, val loss = 1.4780\n",
            "step 57040: train loss = 1.1889, val loss = 1.4873\n",
            "step 57050: train loss = 1.2307, val loss = 1.7620\n",
            "step 57060: train loss = 0.9916, val loss = 1.8709\n",
            "step 57070: train loss = 1.0340, val loss = 1.6452\n",
            "step 57080: train loss = 1.0332, val loss = 1.5685\n",
            "step 57090: train loss = 1.5039, val loss = 1.4383\n",
            "step 57100: train loss = 1.3528, val loss = 0.9682\n",
            "step 57110: train loss = 1.1033, val loss = 1.2584\n",
            "step 57120: train loss = 1.0992, val loss = 1.5319\n",
            "step 57130: train loss = 1.5001, val loss = 1.3962\n",
            "step 57140: train loss = 0.9882, val loss = 1.4118\n",
            "step 57150: train loss = 0.7764, val loss = 1.0754\n",
            "step 57160: train loss = 1.0384, val loss = 1.4958\n",
            "step 57170: train loss = 1.3042, val loss = 1.2094\n",
            "step 57180: train loss = 1.2020, val loss = 1.3350\n",
            "step 57190: train loss = 1.3040, val loss = 1.3132\n",
            "step 57200: train loss = 1.3217, val loss = 1.2484\n",
            "step 57210: train loss = 1.4395, val loss = 1.2122\n",
            "step 57220: train loss = 1.0617, val loss = 1.6335\n",
            "step 57230: train loss = 1.3115, val loss = 1.5409\n",
            "step 57240: train loss = 1.3000, val loss = 1.3013\n",
            "step 57250: train loss = 1.2875, val loss = 1.6863\n",
            "step 57260: train loss = 0.9620, val loss = 2.2281\n",
            "step 57270: train loss = 0.9468, val loss = 2.1402\n",
            "step 57280: train loss = 0.9514, val loss = 1.5464\n",
            "step 57290: train loss = 1.1719, val loss = 1.4324\n",
            "step 57300: train loss = 0.9704, val loss = 1.4618\n",
            "step 57310: train loss = 0.8047, val loss = 1.5001\n",
            "step 57320: train loss = 0.9200, val loss = 1.2288\n",
            "step 57330: train loss = 0.6658, val loss = 1.4788\n",
            "step 57340: train loss = 0.9513, val loss = 1.7113\n",
            "step 57350: train loss = 0.9703, val loss = 1.3651\n",
            "step 57360: train loss = 0.9256, val loss = 1.2516\n",
            "step 57370: train loss = 0.8016, val loss = 1.2298\n",
            "step 57380: train loss = 1.1098, val loss = 1.5452\n",
            "step 57390: train loss = 1.0811, val loss = 1.6209\n",
            "step 57400: train loss = 1.1781, val loss = 1.6525\n",
            "step 57410: train loss = 1.0565, val loss = 1.4933\n",
            "step 57420: train loss = 1.2400, val loss = 1.5900\n",
            "step 57430: train loss = 0.9233, val loss = 1.7249\n",
            "step 57440: train loss = 1.0358, val loss = 1.4964\n",
            "step 57450: train loss = 1.1962, val loss = 1.3026\n",
            "step 57460: train loss = 1.1029, val loss = 1.4754\n",
            "step 57470: train loss = 1.3781, val loss = 1.0605\n",
            "step 57480: train loss = 1.1426, val loss = 1.6548\n",
            "step 57490: train loss = 1.2817, val loss = 1.7686\n",
            "step 57500: train loss = 1.4107, val loss = 1.2365\n",
            "step 57510: train loss = 0.8240, val loss = 1.5885\n",
            "step 57520: train loss = 1.2107, val loss = 1.7120\n",
            "step 57530: train loss = 1.0944, val loss = 1.3596\n",
            "step 57540: train loss = 1.1052, val loss = 1.1397\n",
            "step 57550: train loss = 1.4075, val loss = 1.4098\n",
            "step 57560: train loss = 1.2481, val loss = 1.5264\n",
            "step 57570: train loss = 0.9827, val loss = 1.5929\n",
            "step 57580: train loss = 1.5083, val loss = 1.5729\n",
            "step 57590: train loss = 0.9619, val loss = 1.1747\n",
            "step 57600: train loss = 1.1597, val loss = 1.2852\n",
            "step 57610: train loss = 1.1694, val loss = 1.6006\n",
            "step 57620: train loss = 0.8239, val loss = 1.4018\n",
            "step 57630: train loss = 0.8683, val loss = 1.8545\n",
            "step 57640: train loss = 1.2397, val loss = 1.8744\n",
            "step 57650: train loss = 1.1507, val loss = 1.4493\n",
            "step 57660: train loss = 0.9331, val loss = 0.8680\n",
            "step 57670: train loss = 1.1187, val loss = 1.4662\n",
            "step 57680: train loss = 0.8245, val loss = 1.5277\n",
            "step 57690: train loss = 1.1732, val loss = 1.5182\n",
            "step 57700: train loss = 1.1042, val loss = 1.7071\n",
            "step 57710: train loss = 1.0087, val loss = 1.1883\n",
            "step 57720: train loss = 1.4302, val loss = 1.4235\n",
            "step 57730: train loss = 1.1177, val loss = 1.5063\n",
            "step 57740: train loss = 1.6325, val loss = 1.5315\n",
            "step 57750: train loss = 1.4642, val loss = 1.4568\n",
            "step 57760: train loss = 0.8700, val loss = 1.5884\n",
            "step 57770: train loss = 1.2013, val loss = 1.3292\n",
            "step 57780: train loss = 1.1936, val loss = 1.6275\n",
            "step 57790: train loss = 1.0305, val loss = 1.2358\n",
            "step 57800: train loss = 1.2344, val loss = 1.4533\n",
            "step 57810: train loss = 0.9828, val loss = 1.8520\n",
            "step 57820: train loss = 0.9456, val loss = 1.2705\n",
            "step 57830: train loss = 1.7049, val loss = 1.6227\n",
            "step 57840: train loss = 1.1834, val loss = 1.2534\n",
            "step 57850: train loss = 0.8468, val loss = 1.5215\n",
            "step 57860: train loss = 1.3697, val loss = 1.3776\n",
            "step 57870: train loss = 1.1494, val loss = 1.2858\n",
            "step 57880: train loss = 0.9674, val loss = 1.6253\n",
            "step 57890: train loss = 1.0826, val loss = 1.5732\n",
            "step 57900: train loss = 1.3905, val loss = 1.5743\n",
            "step 57910: train loss = 1.2398, val loss = 1.6777\n",
            "step 57920: train loss = 0.8560, val loss = 1.4094\n",
            "step 57930: train loss = 1.4083, val loss = 1.6853\n",
            "step 57940: train loss = 1.0524, val loss = 1.3583\n",
            "step 57950: train loss = 1.1324, val loss = 1.7614\n",
            "step 57960: train loss = 0.9527, val loss = 1.4119\n",
            "step 57970: train loss = 1.0333, val loss = 1.2914\n",
            "step 57980: train loss = 1.1733, val loss = 1.6994\n",
            "step 57990: train loss = 1.3180, val loss = 1.5364\n",
            "step 58000: train loss = 1.1601, val loss = 1.0324\n",
            "step 58010: train loss = 1.2467, val loss = 1.7183\n",
            "step 58020: train loss = 0.6643, val loss = 1.0925\n",
            "step 58030: train loss = 1.3887, val loss = 1.4652\n",
            "step 58040: train loss = 1.0587, val loss = 1.6277\n",
            "step 58050: train loss = 0.8445, val loss = 1.6375\n",
            "step 58060: train loss = 1.0388, val loss = 1.4568\n",
            "step 58070: train loss = 1.2600, val loss = 1.6381\n",
            "step 58080: train loss = 0.9337, val loss = 1.5398\n",
            "step 58090: train loss = 1.2762, val loss = 1.1666\n",
            "step 58100: train loss = 1.1666, val loss = 1.7108\n",
            "step 58110: train loss = 1.0893, val loss = 1.4429\n",
            "step 58120: train loss = 0.9828, val loss = 1.4407\n",
            "step 58130: train loss = 1.1828, val loss = 1.2186\n",
            "step 58140: train loss = 0.9458, val loss = 1.7595\n",
            "step 58150: train loss = 1.0980, val loss = 1.4660\n",
            "step 58160: train loss = 1.0149, val loss = 1.4265\n",
            "step 58170: train loss = 1.1449, val loss = 1.1732\n",
            "step 58180: train loss = 1.3237, val loss = 1.5898\n",
            "step 58190: train loss = 1.0639, val loss = 1.4969\n",
            "step 58200: train loss = 1.1361, val loss = 1.8235\n",
            "step 58210: train loss = 1.1527, val loss = 1.3827\n",
            "step 58220: train loss = 1.4514, val loss = 1.5306\n",
            "step 58230: train loss = 1.4548, val loss = 1.7089\n",
            "step 58240: train loss = 1.0545, val loss = 1.3894\n",
            "step 58250: train loss = 1.1577, val loss = 1.9825\n",
            "step 58260: train loss = 1.3065, val loss = 1.6620\n",
            "step 58270: train loss = 1.0606, val loss = 1.1253\n",
            "step 58280: train loss = 1.0195, val loss = 1.2678\n",
            "step 58290: train loss = 1.6840, val loss = 1.4231\n",
            "step 58300: train loss = 1.0299, val loss = 1.1669\n",
            "step 58310: train loss = 1.0443, val loss = 1.4016\n",
            "step 58320: train loss = 1.0061, val loss = 1.3916\n",
            "step 58330: train loss = 1.0539, val loss = 1.5911\n",
            "step 58340: train loss = 1.4081, val loss = 1.6711\n",
            "step 58350: train loss = 1.0488, val loss = 1.8074\n",
            "step 58360: train loss = 1.1134, val loss = 1.1757\n",
            "step 58370: train loss = 1.1878, val loss = 1.7642\n",
            "step 58380: train loss = 1.2174, val loss = 1.3103\n",
            "step 58390: train loss = 1.2026, val loss = 1.9644\n",
            "step 58400: train loss = 0.7653, val loss = 1.6020\n",
            "step 58410: train loss = 1.4021, val loss = 1.4881\n",
            "step 58420: train loss = 1.4905, val loss = 1.4586\n",
            "step 58430: train loss = 1.1192, val loss = 1.6090\n",
            "step 58440: train loss = 1.1805, val loss = 1.4260\n",
            "step 58450: train loss = 0.9835, val loss = 1.3402\n",
            "step 58460: train loss = 1.1953, val loss = 1.6095\n",
            "step 58470: train loss = 1.0126, val loss = 1.5235\n",
            "step 58480: train loss = 1.1547, val loss = 1.3640\n",
            "step 58490: train loss = 0.9808, val loss = 1.3328\n",
            "step 58500: train loss = 1.1430, val loss = 1.5230\n",
            "step 58510: train loss = 1.2152, val loss = 1.8072\n",
            "step 58520: train loss = 1.2572, val loss = 1.4912\n",
            "step 58530: train loss = 1.2006, val loss = 1.5768\n",
            "step 58540: train loss = 1.2902, val loss = 1.2469\n",
            "step 58550: train loss = 0.9476, val loss = 1.5798\n",
            "step 58560: train loss = 1.1206, val loss = 1.6129\n",
            "step 58570: train loss = 1.0252, val loss = 1.6573\n",
            "step 58580: train loss = 1.1183, val loss = 1.2377\n",
            "step 58590: train loss = 0.9369, val loss = 1.5953\n",
            "step 58600: train loss = 0.9277, val loss = 1.4275\n",
            "step 58610: train loss = 0.8972, val loss = 1.0689\n",
            "step 58620: train loss = 1.1391, val loss = 1.5066\n",
            "step 58630: train loss = 1.0162, val loss = 1.2313\n",
            "step 58640: train loss = 1.1391, val loss = 1.4723\n",
            "step 58650: train loss = 1.2859, val loss = 1.6566\n",
            "step 58660: train loss = 0.9244, val loss = 1.4272\n",
            "step 58670: train loss = 1.0984, val loss = 1.4500\n",
            "step 58680: train loss = 0.8794, val loss = 1.4582\n",
            "step 58690: train loss = 1.4438, val loss = 1.8198\n",
            "step 58700: train loss = 1.3714, val loss = 1.7779\n",
            "step 58710: train loss = 1.2634, val loss = 1.3301\n",
            "step 58720: train loss = 1.3462, val loss = 1.4103\n",
            "step 58730: train loss = 1.3495, val loss = 1.7379\n",
            "step 58740: train loss = 1.1847, val loss = 1.4927\n",
            "step 58750: train loss = 1.1443, val loss = 1.4244\n",
            "step 58760: train loss = 0.8915, val loss = 1.2584\n",
            "step 58770: train loss = 1.0624, val loss = 1.1339\n",
            "step 58780: train loss = 1.2417, val loss = 1.6538\n",
            "step 58790: train loss = 1.0854, val loss = 1.4783\n",
            "step 58800: train loss = 1.2873, val loss = 1.2808\n",
            "step 58810: train loss = 1.5064, val loss = 1.6973\n",
            "step 58820: train loss = 1.3326, val loss = 1.4769\n",
            "step 58830: train loss = 1.2176, val loss = 1.2859\n",
            "step 58840: train loss = 1.0042, val loss = 1.5153\n",
            "step 58850: train loss = 1.3951, val loss = 1.4042\n",
            "step 58860: train loss = 1.3407, val loss = 1.7518\n",
            "step 58870: train loss = 0.8500, val loss = 1.1470\n",
            "step 58880: train loss = 1.1311, val loss = 1.6188\n",
            "step 58890: train loss = 1.4899, val loss = 1.5303\n",
            "step 58900: train loss = 1.2301, val loss = 1.6197\n",
            "step 58910: train loss = 1.2798, val loss = 1.7019\n",
            "step 58920: train loss = 0.7778, val loss = 1.8804\n",
            "step 58930: train loss = 0.8498, val loss = 1.6452\n",
            "step 58940: train loss = 0.7994, val loss = 1.4367\n",
            "step 58950: train loss = 0.8086, val loss = 1.4104\n",
            "step 58960: train loss = 1.4755, val loss = 1.7952\n",
            "step 58970: train loss = 0.9723, val loss = 1.4615\n",
            "step 58980: train loss = 1.3714, val loss = 1.4450\n",
            "step 58990: train loss = 0.9112, val loss = 1.5397\n",
            "step 59000: train loss = 1.0732, val loss = 1.6521\n",
            "step 59010: train loss = 1.3724, val loss = 1.1489\n",
            "step 59020: train loss = 0.9681, val loss = 1.3998\n",
            "step 59030: train loss = 1.1339, val loss = 1.3223\n",
            "step 59040: train loss = 1.1633, val loss = 1.3536\n",
            "step 59050: train loss = 1.3776, val loss = 1.9519\n",
            "step 59060: train loss = 1.2160, val loss = 1.1800\n",
            "step 59070: train loss = 1.3231, val loss = 1.7336\n",
            "step 59080: train loss = 1.3751, val loss = 1.8357\n",
            "step 59090: train loss = 1.3096, val loss = 1.6489\n",
            "step 59100: train loss = 0.9366, val loss = 1.7427\n",
            "step 59110: train loss = 1.0177, val loss = 1.6447\n",
            "step 59120: train loss = 1.3987, val loss = 1.5482\n",
            "step 59130: train loss = 1.2284, val loss = 0.9204\n",
            "step 59140: train loss = 1.1074, val loss = 1.7858\n",
            "step 59150: train loss = 1.5092, val loss = 1.9057\n",
            "step 59160: train loss = 1.1924, val loss = 1.0889\n",
            "step 59170: train loss = 1.0386, val loss = 0.9194\n",
            "step 59180: train loss = 1.0983, val loss = 1.2314\n",
            "step 59190: train loss = 1.2599, val loss = 1.4246\n",
            "step 59200: train loss = 0.9242, val loss = 1.9370\n",
            "step 59210: train loss = 0.6415, val loss = 1.6887\n",
            "step 59220: train loss = 1.0161, val loss = 1.2907\n",
            "step 59230: train loss = 1.4403, val loss = 1.5011\n",
            "step 59240: train loss = 1.2909, val loss = 1.5472\n",
            "step 59250: train loss = 1.4551, val loss = 1.4974\n",
            "step 59260: train loss = 1.5831, val loss = 1.2564\n",
            "step 59270: train loss = 1.0691, val loss = 1.3113\n",
            "step 59280: train loss = 1.3626, val loss = 1.5717\n",
            "step 59290: train loss = 1.3205, val loss = 1.2896\n",
            "step 59300: train loss = 0.9917, val loss = 1.6477\n",
            "step 59310: train loss = 1.0298, val loss = 1.3340\n",
            "step 59320: train loss = 0.9276, val loss = 1.2856\n",
            "step 59330: train loss = 1.3623, val loss = 1.7134\n",
            "step 59340: train loss = 1.0035, val loss = 1.4711\n",
            "step 59350: train loss = 1.1753, val loss = 1.1599\n",
            "step 59360: train loss = 1.3049, val loss = 1.4838\n",
            "step 59370: train loss = 0.9005, val loss = 1.3200\n",
            "step 59380: train loss = 1.1866, val loss = 1.3968\n",
            "step 59390: train loss = 1.1246, val loss = 1.1024\n",
            "step 59400: train loss = 1.0629, val loss = 1.8305\n",
            "step 59410: train loss = 1.1106, val loss = 1.4237\n",
            "step 59420: train loss = 0.9239, val loss = 1.6477\n",
            "step 59430: train loss = 1.1946, val loss = 1.2730\n",
            "step 59440: train loss = 1.1975, val loss = 1.4048\n",
            "step 59450: train loss = 1.1565, val loss = 1.6754\n",
            "step 59460: train loss = 1.1264, val loss = 1.5303\n",
            "step 59470: train loss = 1.2366, val loss = 1.4984\n",
            "step 59480: train loss = 1.1851, val loss = 1.3926\n",
            "step 59490: train loss = 1.2627, val loss = 1.7517\n",
            "step 59500: train loss = 1.5323, val loss = 1.1549\n",
            "step 59510: train loss = 1.6482, val loss = 1.3441\n",
            "step 59520: train loss = 1.3394, val loss = 1.3828\n",
            "step 59530: train loss = 1.1976, val loss = 1.4210\n",
            "step 59540: train loss = 1.1556, val loss = 1.4160\n",
            "step 59550: train loss = 1.1132, val loss = 1.6541\n",
            "step 59560: train loss = 1.0520, val loss = 1.6383\n",
            "step 59570: train loss = 0.9343, val loss = 1.6664\n",
            "step 59580: train loss = 0.7711, val loss = 1.3008\n",
            "step 59590: train loss = 1.4282, val loss = 1.8212\n",
            "step 59600: train loss = 1.0822, val loss = 1.7293\n",
            "step 59610: train loss = 1.1457, val loss = 1.8628\n",
            "step 59620: train loss = 1.0461, val loss = 1.8309\n",
            "step 59630: train loss = 1.5382, val loss = 1.3759\n",
            "step 59640: train loss = 1.0229, val loss = 1.2475\n",
            "step 59650: train loss = 1.3076, val loss = 1.7548\n",
            "step 59660: train loss = 0.9466, val loss = 1.3446\n",
            "step 59670: train loss = 1.0355, val loss = 1.0147\n",
            "step 59680: train loss = 1.0342, val loss = 1.3257\n",
            "step 59690: train loss = 1.4856, val loss = 1.7331\n",
            "step 59700: train loss = 1.1774, val loss = 1.6663\n",
            "step 59710: train loss = 1.2467, val loss = 1.5635\n",
            "step 59720: train loss = 1.1004, val loss = 1.7960\n",
            "step 59730: train loss = 1.3558, val loss = 1.6077\n",
            "step 59740: train loss = 1.2229, val loss = 1.3602\n",
            "step 59750: train loss = 1.0009, val loss = 1.6668\n",
            "step 59760: train loss = 1.3037, val loss = 1.4240\n",
            "step 59770: train loss = 1.0327, val loss = 1.3708\n",
            "step 59780: train loss = 1.2036, val loss = 1.1347\n",
            "step 59790: train loss = 1.1821, val loss = 1.0974\n",
            "step 59800: train loss = 0.9967, val loss = 1.7541\n",
            "step 59810: train loss = 1.1207, val loss = 1.2761\n",
            "step 59820: train loss = 1.0196, val loss = 1.6217\n",
            "step 59830: train loss = 0.9102, val loss = 1.8642\n",
            "step 59840: train loss = 0.9878, val loss = 1.3767\n",
            "step 59850: train loss = 1.0676, val loss = 1.5149\n",
            "step 59860: train loss = 1.0125, val loss = 1.2759\n",
            "step 59870: train loss = 1.1023, val loss = 2.1404\n",
            "step 59880: train loss = 1.1380, val loss = 1.3674\n",
            "step 59890: train loss = 1.0979, val loss = 1.8388\n",
            "step 59900: train loss = 1.1304, val loss = 1.4187\n",
            "step 59910: train loss = 1.5291, val loss = 1.1783\n",
            "step 59920: train loss = 1.0307, val loss = 1.2439\n",
            "step 59930: train loss = 1.1331, val loss = 1.5759\n",
            "step 59940: train loss = 1.3683, val loss = 1.8263\n",
            "step 59950: train loss = 0.7287, val loss = 1.3745\n",
            "step 59960: train loss = 1.1277, val loss = 1.8438\n",
            "step 59970: train loss = 1.1816, val loss = 1.3608\n",
            "step 59980: train loss = 1.0965, val loss = 1.4410\n",
            "step 59990: train loss = 1.2698, val loss = 1.1694\n",
            "step 60000: train loss = 0.9000, val loss = 1.5079\n",
            "step 60010: train loss = 1.2500, val loss = 1.9161\n",
            "step 60020: train loss = 0.9008, val loss = 1.2435\n",
            "step 60030: train loss = 1.2258, val loss = 1.3055\n",
            "step 60040: train loss = 1.2629, val loss = 1.3736\n",
            "step 60050: train loss = 1.1148, val loss = 1.4403\n",
            "step 60060: train loss = 1.5945, val loss = 1.2758\n",
            "step 60070: train loss = 0.9764, val loss = 1.1519\n",
            "step 60080: train loss = 1.0863, val loss = 1.5365\n",
            "step 60090: train loss = 0.8976, val loss = 1.3528\n",
            "step 60100: train loss = 0.9832, val loss = 1.5306\n",
            "step 60110: train loss = 1.0422, val loss = 1.8581\n",
            "step 60120: train loss = 1.2181, val loss = 1.8792\n",
            "step 60130: train loss = 1.1827, val loss = 1.0863\n",
            "step 60140: train loss = 1.0258, val loss = 1.4822\n",
            "step 60150: train loss = 1.1791, val loss = 1.5907\n",
            "step 60160: train loss = 1.1630, val loss = 1.6135\n",
            "step 60170: train loss = 1.0401, val loss = 1.3754\n",
            "step 60180: train loss = 1.0986, val loss = 1.6477\n",
            "step 60190: train loss = 1.2695, val loss = 1.8983\n",
            "step 60200: train loss = 0.8192, val loss = 1.2910\n",
            "step 60210: train loss = 0.8258, val loss = 1.4473\n",
            "step 60220: train loss = 1.1991, val loss = 1.1142\n",
            "step 60230: train loss = 1.2086, val loss = 1.4431\n",
            "step 60240: train loss = 1.0872, val loss = 1.5734\n",
            "step 60250: train loss = 1.2378, val loss = 1.2811\n",
            "step 60260: train loss = 1.5433, val loss = 1.5938\n",
            "step 60270: train loss = 1.1295, val loss = 1.4214\n",
            "step 60280: train loss = 1.2857, val loss = 1.3606\n",
            "step 60290: train loss = 1.1837, val loss = 1.6777\n",
            "step 60300: train loss = 1.2955, val loss = 1.5896\n",
            "step 60310: train loss = 1.1336, val loss = 1.6839\n",
            "step 60320: train loss = 1.3756, val loss = 1.7937\n",
            "step 60330: train loss = 1.1341, val loss = 1.2975\n",
            "step 60340: train loss = 1.1698, val loss = 1.2053\n",
            "step 60350: train loss = 0.9808, val loss = 1.3881\n",
            "step 60360: train loss = 1.1075, val loss = 1.2005\n",
            "step 60370: train loss = 1.1431, val loss = 1.5790\n",
            "step 60380: train loss = 1.2801, val loss = 1.8429\n",
            "step 60390: train loss = 0.9237, val loss = 1.0462\n",
            "step 60400: train loss = 1.3261, val loss = 1.3913\n",
            "step 60410: train loss = 1.1129, val loss = 0.9577\n",
            "step 60420: train loss = 1.2282, val loss = 1.5527\n",
            "step 60430: train loss = 1.0568, val loss = 1.5907\n",
            "step 60440: train loss = 1.2318, val loss = 1.2667\n",
            "step 60450: train loss = 1.2672, val loss = 1.9301\n",
            "step 60460: train loss = 1.1823, val loss = 1.5136\n",
            "step 60470: train loss = 1.1521, val loss = 1.5959\n",
            "step 60480: train loss = 1.1570, val loss = 1.8004\n",
            "step 60490: train loss = 1.3864, val loss = 1.5526\n",
            "step 60500: train loss = 0.8426, val loss = 1.5348\n",
            "step 60510: train loss = 1.0288, val loss = 2.0329\n",
            "step 60520: train loss = 0.7838, val loss = 1.4404\n",
            "step 60530: train loss = 1.1182, val loss = 1.0816\n",
            "step 60540: train loss = 1.0435, val loss = 1.3888\n",
            "step 60550: train loss = 1.1115, val loss = 1.3241\n",
            "step 60560: train loss = 1.1175, val loss = 1.4946\n",
            "step 60570: train loss = 1.1589, val loss = 1.6677\n",
            "step 60580: train loss = 1.3017, val loss = 1.3868\n",
            "step 60590: train loss = 0.8070, val loss = 1.3947\n",
            "step 60600: train loss = 1.1605, val loss = 1.5210\n",
            "step 60610: train loss = 1.1531, val loss = 1.4821\n",
            "step 60620: train loss = 1.0786, val loss = 1.6217\n",
            "step 60630: train loss = 1.4524, val loss = 1.6534\n",
            "step 60640: train loss = 1.1663, val loss = 1.5053\n",
            "step 60650: train loss = 1.2052, val loss = 1.0398\n",
            "step 60660: train loss = 1.4955, val loss = 1.7945\n",
            "step 60670: train loss = 1.0680, val loss = 1.5180\n",
            "step 60680: train loss = 1.2064, val loss = 1.6600\n",
            "step 60690: train loss = 0.7083, val loss = 1.3280\n",
            "step 60700: train loss = 0.9162, val loss = 1.4128\n",
            "step 60710: train loss = 1.0798, val loss = 1.5143\n",
            "step 60720: train loss = 1.4998, val loss = 1.3957\n",
            "step 60730: train loss = 1.2895, val loss = 1.2080\n",
            "step 60740: train loss = 0.9384, val loss = 1.2679\n",
            "step 60750: train loss = 1.4926, val loss = 1.7484\n",
            "step 60760: train loss = 1.2273, val loss = 1.1858\n",
            "step 60770: train loss = 0.9393, val loss = 1.4680\n",
            "step 60780: train loss = 1.1229, val loss = 1.6074\n",
            "step 60790: train loss = 1.1617, val loss = 1.6675\n",
            "step 60800: train loss = 1.1064, val loss = 1.4385\n",
            "step 60810: train loss = 0.8228, val loss = 1.6561\n",
            "step 60820: train loss = 1.0882, val loss = 1.2864\n",
            "step 60830: train loss = 1.1888, val loss = 1.9599\n",
            "step 60840: train loss = 0.7187, val loss = 1.2372\n",
            "step 60850: train loss = 1.0040, val loss = 1.4246\n",
            "step 60860: train loss = 1.1866, val loss = 1.3175\n",
            "step 60870: train loss = 0.9282, val loss = 1.5755\n",
            "step 60880: train loss = 1.1823, val loss = 1.4717\n",
            "step 60890: train loss = 1.2671, val loss = 0.9717\n",
            "step 60900: train loss = 1.0377, val loss = 1.1783\n",
            "step 60910: train loss = 1.0496, val loss = 2.0003\n",
            "step 60920: train loss = 0.8189, val loss = 1.1250\n",
            "step 60930: train loss = 0.8158, val loss = 1.9443\n",
            "step 60940: train loss = 0.9606, val loss = 1.6715\n",
            "step 60950: train loss = 1.4545, val loss = 1.3874\n",
            "step 60960: train loss = 1.2784, val loss = 1.2278\n",
            "step 60970: train loss = 1.1231, val loss = 1.2518\n",
            "step 60980: train loss = 0.7743, val loss = 1.8692\n",
            "step 60990: train loss = 1.0557, val loss = 2.0382\n",
            "step 61000: train loss = 0.8746, val loss = 1.4162\n",
            "step 61010: train loss = 1.1338, val loss = 1.4791\n",
            "step 61020: train loss = 1.2083, val loss = 1.8908\n",
            "step 61030: train loss = 1.1914, val loss = 1.7573\n",
            "step 61040: train loss = 1.5098, val loss = 1.3285\n",
            "step 61050: train loss = 1.7028, val loss = 1.5964\n",
            "step 61060: train loss = 1.1131, val loss = 1.6481\n",
            "step 61070: train loss = 1.1311, val loss = 1.5696\n",
            "step 61080: train loss = 1.4675, val loss = 1.4582\n",
            "step 61090: train loss = 1.0126, val loss = 1.7851\n",
            "step 61100: train loss = 1.2967, val loss = 1.8655\n",
            "step 61110: train loss = 0.9638, val loss = 1.7214\n",
            "step 61120: train loss = 1.0673, val loss = 1.4651\n",
            "step 61130: train loss = 0.9843, val loss = 1.2136\n",
            "step 61140: train loss = 1.0287, val loss = 1.2391\n",
            "step 61150: train loss = 0.7415, val loss = 1.5014\n",
            "step 61160: train loss = 1.6040, val loss = 1.6051\n",
            "step 61170: train loss = 1.2958, val loss = 1.6444\n",
            "step 61180: train loss = 0.9823, val loss = 1.3146\n",
            "step 61190: train loss = 0.9536, val loss = 1.4863\n",
            "step 61200: train loss = 1.2381, val loss = 1.3746\n",
            "step 61210: train loss = 1.1488, val loss = 1.8276\n",
            "step 61220: train loss = 1.0155, val loss = 1.9685\n",
            "step 61230: train loss = 1.0120, val loss = 1.6364\n",
            "step 61240: train loss = 1.1654, val loss = 1.4207\n",
            "step 61250: train loss = 1.2306, val loss = 1.3802\n",
            "step 61260: train loss = 0.9233, val loss = 1.4724\n",
            "step 61270: train loss = 1.0887, val loss = 1.4218\n",
            "step 61280: train loss = 1.1828, val loss = 1.3796\n",
            "step 61290: train loss = 0.8780, val loss = 1.4306\n",
            "step 61300: train loss = 1.0373, val loss = 1.6105\n",
            "step 61310: train loss = 0.8043, val loss = 1.1644\n",
            "step 61320: train loss = 1.2894, val loss = 1.3720\n",
            "step 61330: train loss = 1.1622, val loss = 1.3210\n",
            "step 61340: train loss = 1.1339, val loss = 1.6543\n",
            "step 61350: train loss = 1.2521, val loss = 1.3282\n",
            "step 61360: train loss = 1.0510, val loss = 1.4736\n",
            "step 61370: train loss = 0.8864, val loss = 1.3279\n",
            "step 61380: train loss = 0.9784, val loss = 1.7236\n",
            "step 61390: train loss = 0.8880, val loss = 1.3966\n",
            "step 61400: train loss = 1.0953, val loss = 1.7416\n",
            "step 61410: train loss = 1.0723, val loss = 1.6184\n",
            "step 61420: train loss = 0.9300, val loss = 1.4363\n",
            "step 61430: train loss = 1.1641, val loss = 1.0555\n",
            "step 61440: train loss = 0.9087, val loss = 1.5652\n",
            "step 61450: train loss = 1.0584, val loss = 1.4715\n",
            "step 61460: train loss = 1.1971, val loss = 1.3097\n",
            "step 61470: train loss = 1.2173, val loss = 1.4573\n",
            "step 61480: train loss = 0.6463, val loss = 1.5634\n",
            "step 61490: train loss = 0.8890, val loss = 1.3740\n",
            "step 61500: train loss = 1.0115, val loss = 2.2155\n",
            "step 61510: train loss = 0.9766, val loss = 1.4772\n",
            "step 61520: train loss = 0.8344, val loss = 1.4483\n",
            "step 61530: train loss = 0.9742, val loss = 1.7606\n",
            "step 61540: train loss = 1.0299, val loss = 1.3645\n",
            "step 61550: train loss = 1.2053, val loss = 1.8604\n",
            "step 61560: train loss = 1.2191, val loss = 1.3625\n",
            "step 61570: train loss = 1.1410, val loss = 1.9120\n",
            "step 61580: train loss = 1.1301, val loss = 1.4391\n",
            "step 61590: train loss = 0.7743, val loss = 1.6619\n",
            "step 61600: train loss = 1.1585, val loss = 1.6902\n",
            "step 61610: train loss = 0.9769, val loss = 1.5412\n",
            "step 61620: train loss = 0.8768, val loss = 1.2485\n",
            "step 61630: train loss = 1.2333, val loss = 1.5096\n",
            "step 61640: train loss = 1.0680, val loss = 1.4000\n",
            "step 61650: train loss = 0.8166, val loss = 1.1677\n",
            "step 61660: train loss = 1.2925, val loss = 1.3324\n",
            "step 61670: train loss = 0.7276, val loss = 1.3545\n",
            "step 61680: train loss = 1.8880, val loss = 1.7885\n",
            "step 61690: train loss = 1.0795, val loss = 1.5171\n",
            "step 61700: train loss = 0.9672, val loss = 1.3687\n",
            "step 61710: train loss = 1.1591, val loss = 1.4694\n",
            "step 61720: train loss = 0.9045, val loss = 1.8858\n",
            "step 61730: train loss = 1.1947, val loss = 1.6812\n",
            "step 61740: train loss = 1.0768, val loss = 1.2753\n",
            "step 61750: train loss = 1.0801, val loss = 1.4228\n",
            "step 61760: train loss = 1.0258, val loss = 2.3633\n",
            "step 61770: train loss = 1.3116, val loss = 1.5017\n",
            "step 61780: train loss = 0.7664, val loss = 1.9464\n",
            "step 61790: train loss = 0.9802, val loss = 1.5427\n",
            "step 61800: train loss = 1.0435, val loss = 1.2880\n",
            "step 61810: train loss = 1.0869, val loss = 1.3767\n",
            "step 61820: train loss = 1.1750, val loss = 1.8587\n",
            "step 61830: train loss = 0.9702, val loss = 1.5130\n",
            "step 61840: train loss = 0.9916, val loss = 1.3340\n",
            "step 61850: train loss = 1.1234, val loss = 1.9267\n",
            "step 61860: train loss = 1.2312, val loss = 1.6010\n",
            "step 61870: train loss = 1.1391, val loss = 1.6481\n",
            "step 61880: train loss = 1.3551, val loss = 1.4582\n",
            "step 61890: train loss = 1.4202, val loss = 1.3698\n",
            "step 61900: train loss = 1.0753, val loss = 1.1904\n",
            "step 61910: train loss = 0.8083, val loss = 1.3387\n",
            "step 61920: train loss = 1.5166, val loss = 1.2205\n",
            "step 61930: train loss = 1.4143, val loss = 1.4589\n",
            "step 61940: train loss = 1.3389, val loss = 1.5198\n",
            "step 61950: train loss = 1.1202, val loss = 1.3661\n",
            "step 61960: train loss = 0.9140, val loss = 1.7165\n",
            "step 61970: train loss = 1.1120, val loss = 1.2969\n",
            "step 61980: train loss = 1.2599, val loss = 1.4191\n",
            "step 61990: train loss = 1.1325, val loss = 1.1639\n",
            "step 62000: train loss = 0.4932, val loss = 1.3049\n",
            "step 62010: train loss = 1.2038, val loss = 1.7504\n",
            "step 62020: train loss = 1.1491, val loss = 1.2879\n",
            "step 62030: train loss = 0.7152, val loss = 1.7805\n",
            "step 62040: train loss = 0.9441, val loss = 1.7840\n",
            "step 62050: train loss = 1.4527, val loss = 1.7888\n",
            "step 62060: train loss = 0.9768, val loss = 1.5715\n",
            "step 62070: train loss = 1.1540, val loss = 1.4606\n",
            "step 62080: train loss = 0.9851, val loss = 1.8469\n",
            "step 62090: train loss = 1.2856, val loss = 1.7702\n",
            "step 62100: train loss = 1.3605, val loss = 1.0770\n",
            "step 62110: train loss = 1.0287, val loss = 1.3311\n",
            "step 62120: train loss = 1.1830, val loss = 1.5194\n",
            "step 62130: train loss = 0.8923, val loss = 1.3701\n",
            "step 62140: train loss = 0.8973, val loss = 1.8606\n",
            "step 62150: train loss = 1.3453, val loss = 1.6288\n",
            "step 62160: train loss = 1.3196, val loss = 1.8598\n",
            "step 62170: train loss = 1.0290, val loss = 1.0613\n",
            "step 62180: train loss = 0.8371, val loss = 1.7036\n",
            "step 62190: train loss = 0.9619, val loss = 1.3887\n",
            "step 62200: train loss = 1.1935, val loss = 1.1976\n",
            "step 62210: train loss = 0.8793, val loss = 1.3311\n",
            "step 62220: train loss = 1.0305, val loss = 1.3519\n",
            "step 62230: train loss = 0.9678, val loss = 1.6935\n",
            "step 62240: train loss = 1.2518, val loss = 1.2892\n",
            "step 62250: train loss = 0.8724, val loss = 1.9887\n",
            "step 62260: train loss = 1.0140, val loss = 1.2909\n",
            "step 62270: train loss = 0.7465, val loss = 1.4538\n",
            "step 62280: train loss = 1.3109, val loss = 1.2042\n",
            "step 62290: train loss = 1.4257, val loss = 1.3102\n",
            "step 62300: train loss = 0.9680, val loss = 1.7097\n",
            "step 62310: train loss = 1.0729, val loss = 1.5998\n",
            "step 62320: train loss = 0.7326, val loss = 1.6114\n",
            "step 62330: train loss = 1.0105, val loss = 1.5373\n",
            "step 62340: train loss = 1.3774, val loss = 1.8661\n",
            "step 62350: train loss = 0.9953, val loss = 1.3300\n",
            "step 62360: train loss = 1.0254, val loss = 1.2707\n",
            "step 62370: train loss = 0.8342, val loss = 1.8487\n",
            "step 62380: train loss = 1.3310, val loss = 1.3680\n",
            "step 62390: train loss = 0.8739, val loss = 1.6946\n",
            "step 62400: train loss = 1.6427, val loss = 1.7496\n",
            "step 62410: train loss = 1.0164, val loss = 1.1228\n",
            "step 62420: train loss = 0.8284, val loss = 1.3505\n",
            "step 62430: train loss = 1.0299, val loss = 1.3267\n",
            "step 62440: train loss = 0.9645, val loss = 1.5734\n",
            "step 62450: train loss = 1.0133, val loss = 1.6440\n",
            "step 62460: train loss = 0.9862, val loss = 1.3789\n",
            "step 62470: train loss = 0.9259, val loss = 1.2886\n",
            "step 62480: train loss = 1.1247, val loss = 1.7805\n",
            "step 62490: train loss = 1.0699, val loss = 1.6817\n",
            "step 62500: train loss = 1.3446, val loss = 1.4189\n",
            "step 62510: train loss = 1.3486, val loss = 1.1314\n",
            "step 62520: train loss = 1.2571, val loss = 1.2943\n",
            "step 62530: train loss = 1.0854, val loss = 1.3382\n",
            "step 62540: train loss = 1.0645, val loss = 1.3388\n",
            "step 62550: train loss = 1.0496, val loss = 1.3549\n",
            "step 62560: train loss = 1.3183, val loss = 1.0739\n",
            "step 62570: train loss = 0.9386, val loss = 1.1359\n",
            "step 62580: train loss = 1.1913, val loss = 1.4743\n",
            "step 62590: train loss = 1.2752, val loss = 1.2966\n",
            "step 62600: train loss = 0.9512, val loss = 1.7271\n",
            "step 62610: train loss = 0.8472, val loss = 1.5414\n",
            "step 62620: train loss = 0.9053, val loss = 1.7172\n",
            "step 62630: train loss = 0.8199, val loss = 1.2873\n",
            "step 62640: train loss = 0.8717, val loss = 1.2792\n",
            "step 62650: train loss = 0.9480, val loss = 1.9762\n",
            "step 62660: train loss = 1.2409, val loss = 1.7268\n",
            "step 62670: train loss = 1.1481, val loss = 1.8356\n",
            "step 62680: train loss = 0.7504, val loss = 1.4942\n",
            "step 62690: train loss = 0.9561, val loss = 1.0982\n",
            "step 62700: train loss = 1.2361, val loss = 1.4883\n",
            "step 62710: train loss = 1.2682, val loss = 1.2108\n",
            "step 62720: train loss = 1.0987, val loss = 1.4747\n",
            "step 62730: train loss = 1.3315, val loss = 1.4580\n",
            "step 62740: train loss = 1.1266, val loss = 1.9453\n",
            "step 62750: train loss = 1.1875, val loss = 2.1547\n",
            "step 62760: train loss = 1.0528, val loss = 1.6737\n",
            "step 62770: train loss = 1.0300, val loss = 1.1551\n",
            "step 62780: train loss = 1.0628, val loss = 1.4560\n",
            "step 62790: train loss = 1.0182, val loss = 1.5616\n",
            "step 62800: train loss = 1.2458, val loss = 1.4527\n",
            "step 62810: train loss = 0.8660, val loss = 1.7769\n",
            "step 62820: train loss = 0.9736, val loss = 1.3520\n",
            "step 62830: train loss = 1.1360, val loss = 1.3216\n",
            "step 62840: train loss = 0.9455, val loss = 1.5964\n",
            "step 62850: train loss = 1.3393, val loss = 1.6341\n",
            "step 62860: train loss = 1.0148, val loss = 1.1009\n",
            "step 62870: train loss = 1.2431, val loss = 1.5097\n",
            "step 62880: train loss = 0.9661, val loss = 1.6105\n",
            "step 62890: train loss = 1.1324, val loss = 1.9386\n",
            "step 62900: train loss = 1.0035, val loss = 1.7662\n",
            "step 62910: train loss = 0.9954, val loss = 1.6699\n",
            "step 62920: train loss = 1.0181, val loss = 1.7083\n",
            "step 62930: train loss = 1.5326, val loss = 1.1618\n",
            "step 62940: train loss = 1.1595, val loss = 1.8369\n",
            "step 62950: train loss = 1.3646, val loss = 1.5681\n",
            "step 62960: train loss = 1.1574, val loss = 1.9353\n",
            "step 62970: train loss = 1.0394, val loss = 1.2865\n",
            "step 62980: train loss = 0.9947, val loss = 1.5855\n",
            "step 62990: train loss = 0.6895, val loss = 1.8211\n",
            "step 63000: train loss = 1.2450, val loss = 1.9498\n",
            "step 63010: train loss = 0.8984, val loss = 1.2152\n",
            "step 63020: train loss = 1.2930, val loss = 1.1307\n",
            "step 63030: train loss = 1.0220, val loss = 1.5355\n",
            "step 63040: train loss = 1.0777, val loss = 1.4020\n",
            "step 63050: train loss = 0.9824, val loss = 1.5035\n",
            "step 63060: train loss = 1.0348, val loss = 1.8123\n",
            "step 63070: train loss = 0.9958, val loss = 1.7460\n",
            "step 63080: train loss = 0.8998, val loss = 1.5887\n",
            "step 63090: train loss = 1.0127, val loss = 1.7874\n",
            "step 63100: train loss = 1.0916, val loss = 1.4808\n",
            "step 63110: train loss = 1.2264, val loss = 1.3545\n",
            "step 63120: train loss = 1.1689, val loss = 1.3784\n",
            "step 63130: train loss = 0.7969, val loss = 1.1238\n",
            "step 63140: train loss = 0.7149, val loss = 1.1735\n",
            "step 63150: train loss = 1.0019, val loss = 1.8137\n",
            "step 63160: train loss = 1.0226, val loss = 1.2887\n",
            "step 63170: train loss = 1.2326, val loss = 1.4305\n",
            "step 63180: train loss = 1.0684, val loss = 1.7364\n",
            "step 63190: train loss = 1.1896, val loss = 1.4965\n",
            "step 63200: train loss = 0.9863, val loss = 1.1297\n",
            "step 63210: train loss = 1.1885, val loss = 1.9903\n",
            "step 63220: train loss = 1.0749, val loss = 1.7630\n",
            "step 63230: train loss = 1.1403, val loss = 1.8064\n",
            "step 63240: train loss = 0.8999, val loss = 1.4750\n",
            "step 63250: train loss = 1.2855, val loss = 1.6490\n",
            "step 63260: train loss = 0.9819, val loss = 1.6782\n",
            "step 63270: train loss = 1.0832, val loss = 1.4300\n",
            "step 63280: train loss = 1.4850, val loss = 1.7170\n",
            "step 63290: train loss = 1.0302, val loss = 1.7103\n",
            "step 63300: train loss = 1.0833, val loss = 1.6897\n",
            "step 63310: train loss = 1.0911, val loss = 1.4695\n",
            "step 63320: train loss = 1.6698, val loss = 1.4967\n",
            "step 63330: train loss = 1.1503, val loss = 1.5114\n",
            "step 63340: train loss = 1.2820, val loss = 1.8015\n",
            "step 63350: train loss = 0.8726, val loss = 1.4056\n",
            "step 63360: train loss = 0.9966, val loss = 1.9122\n",
            "step 63370: train loss = 1.0779, val loss = 1.4683\n",
            "step 63380: train loss = 0.8893, val loss = 1.1503\n",
            "step 63390: train loss = 0.9893, val loss = 1.5723\n",
            "step 63400: train loss = 0.8820, val loss = 2.0363\n",
            "step 63410: train loss = 0.7370, val loss = 1.6647\n",
            "step 63420: train loss = 1.0837, val loss = 1.8302\n",
            "step 63430: train loss = 0.8316, val loss = 1.4030\n",
            "step 63440: train loss = 1.0885, val loss = 1.3483\n",
            "step 63450: train loss = 1.3551, val loss = 1.5231\n",
            "step 63460: train loss = 1.3250, val loss = 1.2102\n",
            "step 63470: train loss = 0.8378, val loss = 1.8117\n",
            "step 63480: train loss = 0.8768, val loss = 1.9712\n",
            "step 63490: train loss = 0.8442, val loss = 1.6991\n",
            "step 63500: train loss = 1.3494, val loss = 1.3384\n",
            "step 63510: train loss = 0.9854, val loss = 1.8755\n",
            "step 63520: train loss = 0.8262, val loss = 1.5414\n",
            "step 63530: train loss = 0.8768, val loss = 1.5436\n",
            "step 63540: train loss = 0.9788, val loss = 1.5180\n",
            "step 63550: train loss = 1.2455, val loss = 1.4333\n",
            "step 63560: train loss = 1.1701, val loss = 1.4918\n",
            "step 63570: train loss = 1.1733, val loss = 1.5136\n",
            "step 63580: train loss = 1.4445, val loss = 1.3979\n",
            "step 63590: train loss = 1.0080, val loss = 1.5182\n",
            "step 63600: train loss = 0.9056, val loss = 1.3023\n",
            "step 63610: train loss = 1.0002, val loss = 1.0234\n",
            "step 63620: train loss = 1.1719, val loss = 1.3077\n",
            "step 63630: train loss = 0.6343, val loss = 1.4700\n",
            "step 63640: train loss = 0.9634, val loss = 1.6565\n",
            "step 63650: train loss = 1.1747, val loss = 1.3203\n",
            "step 63660: train loss = 1.0479, val loss = 1.1083\n",
            "step 63670: train loss = 0.9103, val loss = 1.8889\n",
            "step 63680: train loss = 0.9925, val loss = 1.6733\n",
            "step 63690: train loss = 1.1515, val loss = 1.9968\n",
            "step 63700: train loss = 1.1079, val loss = 1.5681\n",
            "step 63710: train loss = 0.8301, val loss = 1.5609\n",
            "step 63720: train loss = 1.1349, val loss = 2.0669\n",
            "step 63730: train loss = 1.0569, val loss = 1.6244\n",
            "step 63740: train loss = 1.2795, val loss = 1.6198\n",
            "step 63750: train loss = 1.0249, val loss = 1.0783\n",
            "step 63760: train loss = 1.0079, val loss = 1.6187\n",
            "step 63770: train loss = 1.3911, val loss = 1.2615\n",
            "step 63780: train loss = 0.9219, val loss = 1.4131\n",
            "step 63790: train loss = 1.1655, val loss = 1.2482\n",
            "step 63800: train loss = 1.1308, val loss = 1.4229\n",
            "step 63810: train loss = 0.8834, val loss = 1.8486\n",
            "step 63820: train loss = 0.7594, val loss = 1.7944\n",
            "step 63830: train loss = 1.3068, val loss = 1.5626\n",
            "step 63840: train loss = 1.0912, val loss = 1.1958\n",
            "step 63850: train loss = 0.9703, val loss = 1.1393\n",
            "step 63860: train loss = 0.9774, val loss = 1.3252\n",
            "step 63870: train loss = 1.3138, val loss = 1.5156\n",
            "step 63880: train loss = 1.2679, val loss = 1.7977\n",
            "step 63890: train loss = 1.1328, val loss = 1.0741\n",
            "step 63900: train loss = 1.0639, val loss = 1.8732\n",
            "step 63910: train loss = 0.9939, val loss = 1.6269\n",
            "step 63920: train loss = 1.3825, val loss = 1.4744\n",
            "step 63930: train loss = 1.4454, val loss = 1.5395\n",
            "step 63940: train loss = 1.3353, val loss = 1.7843\n",
            "step 63950: train loss = 1.0800, val loss = 1.6980\n",
            "step 63960: train loss = 1.0655, val loss = 1.9261\n",
            "step 63970: train loss = 1.0060, val loss = 1.6100\n",
            "step 63980: train loss = 1.1895, val loss = 1.4829\n",
            "step 63990: train loss = 1.0006, val loss = 1.7269\n",
            "step 64000: train loss = 1.2657, val loss = 1.7999\n",
            "step 64010: train loss = 1.1730, val loss = 1.2971\n",
            "step 64020: train loss = 0.8627, val loss = 1.3057\n",
            "step 64030: train loss = 1.2178, val loss = 1.9462\n",
            "step 64040: train loss = 0.9139, val loss = 1.5613\n",
            "step 64050: train loss = 1.0465, val loss = 1.4625\n",
            "step 64060: train loss = 0.8977, val loss = 1.5567\n",
            "step 64070: train loss = 0.9839, val loss = 1.8594\n",
            "step 64080: train loss = 0.9028, val loss = 1.7428\n",
            "step 64090: train loss = 0.7180, val loss = 1.8987\n",
            "step 64100: train loss = 0.9709, val loss = 1.5274\n",
            "step 64110: train loss = 0.9784, val loss = 1.3727\n",
            "step 64120: train loss = 1.3845, val loss = 1.1900\n",
            "step 64130: train loss = 1.1200, val loss = 1.4134\n",
            "step 64140: train loss = 1.2884, val loss = 1.2631\n",
            "step 64150: train loss = 1.0358, val loss = 1.6905\n",
            "step 64160: train loss = 0.7570, val loss = 1.3955\n",
            "step 64170: train loss = 1.1349, val loss = 1.4280\n",
            "step 64180: train loss = 1.2812, val loss = 1.2924\n",
            "step 64190: train loss = 0.8097, val loss = 1.4887\n",
            "step 64200: train loss = 0.8599, val loss = 1.3199\n",
            "step 64210: train loss = 1.0535, val loss = 1.4080\n",
            "step 64220: train loss = 0.7833, val loss = 1.7445\n",
            "step 64230: train loss = 1.1263, val loss = 1.5449\n",
            "step 64240: train loss = 1.3230, val loss = 1.3935\n",
            "step 64250: train loss = 1.0702, val loss = 2.1738\n",
            "step 64260: train loss = 1.0732, val loss = 1.8760\n",
            "step 64270: train loss = 0.8157, val loss = 1.1377\n",
            "step 64280: train loss = 0.8830, val loss = 1.6372\n",
            "step 64290: train loss = 1.1838, val loss = 1.2871\n",
            "step 64300: train loss = 1.4162, val loss = 1.0380\n",
            "step 64310: train loss = 0.9023, val loss = 1.4760\n",
            "step 64320: train loss = 1.3901, val loss = 1.4428\n",
            "step 64330: train loss = 1.0924, val loss = 1.5175\n",
            "step 64340: train loss = 1.1518, val loss = 1.5174\n",
            "step 64350: train loss = 1.4930, val loss = 1.5260\n",
            "step 64360: train loss = 1.3261, val loss = 1.7581\n",
            "step 64370: train loss = 1.2080, val loss = 0.9122\n",
            "step 64380: train loss = 1.0083, val loss = 1.4719\n",
            "step 64390: train loss = 0.9841, val loss = 1.1789\n",
            "step 64400: train loss = 0.7349, val loss = 1.7806\n",
            "step 64410: train loss = 1.1138, val loss = 1.6377\n",
            "step 64420: train loss = 1.1761, val loss = 1.1540\n",
            "step 64430: train loss = 0.9778, val loss = 1.5906\n",
            "step 64440: train loss = 1.0362, val loss = 1.5498\n",
            "step 64450: train loss = 1.2745, val loss = 1.4303\n",
            "step 64460: train loss = 0.8640, val loss = 1.7077\n",
            "step 64470: train loss = 0.6519, val loss = 1.1762\n",
            "step 64480: train loss = 1.2797, val loss = 1.3119\n",
            "step 64490: train loss = 1.3746, val loss = 1.5562\n",
            "step 64500: train loss = 0.9248, val loss = 1.5531\n",
            "step 64510: train loss = 0.8127, val loss = 1.5709\n",
            "step 64520: train loss = 1.2322, val loss = 1.2762\n",
            "step 64530: train loss = 0.9103, val loss = 1.5085\n",
            "step 64540: train loss = 1.0077, val loss = 1.7341\n",
            "step 64550: train loss = 1.1303, val loss = 1.4917\n",
            "step 64560: train loss = 1.0309, val loss = 1.5173\n",
            "step 64570: train loss = 0.8070, val loss = 1.6780\n",
            "step 64580: train loss = 1.0454, val loss = 1.9900\n",
            "step 64590: train loss = 1.0424, val loss = 1.3482\n",
            "step 64600: train loss = 1.0883, val loss = 1.6879\n",
            "step 64610: train loss = 0.9683, val loss = 1.6837\n",
            "step 64620: train loss = 1.1764, val loss = 1.5127\n",
            "step 64630: train loss = 0.9915, val loss = 1.5420\n",
            "step 64640: train loss = 1.0451, val loss = 1.5958\n",
            "step 64650: train loss = 0.8796, val loss = 1.5906\n",
            "step 64660: train loss = 0.9593, val loss = 1.2882\n",
            "step 64670: train loss = 1.0967, val loss = 1.2191\n",
            "step 64680: train loss = 1.4853, val loss = 1.2756\n",
            "step 64690: train loss = 0.9804, val loss = 1.7412\n",
            "step 64700: train loss = 1.0851, val loss = 1.3492\n",
            "step 64710: train loss = 1.1446, val loss = 1.3367\n",
            "step 64720: train loss = 1.1090, val loss = 1.3138\n",
            "step 64730: train loss = 1.0103, val loss = 1.6876\n",
            "step 64740: train loss = 1.0158, val loss = 1.5818\n",
            "step 64750: train loss = 1.2403, val loss = 1.4018\n",
            "step 64760: train loss = 1.0563, val loss = 1.3866\n",
            "step 64770: train loss = 0.9971, val loss = 1.6170\n",
            "step 64780: train loss = 1.2062, val loss = 1.6051\n",
            "step 64790: train loss = 0.7343, val loss = 1.3866\n",
            "step 64800: train loss = 1.4525, val loss = 1.5561\n",
            "step 64810: train loss = 0.7779, val loss = 1.9180\n",
            "step 64820: train loss = 1.1884, val loss = 1.6533\n",
            "step 64830: train loss = 0.9296, val loss = 1.1683\n",
            "step 64840: train loss = 1.2751, val loss = 1.4786\n",
            "step 64850: train loss = 0.9468, val loss = 1.9112\n",
            "step 64860: train loss = 1.0945, val loss = 1.4564\n",
            "step 64870: train loss = 1.2034, val loss = 1.3614\n",
            "step 64880: train loss = 1.0647, val loss = 1.7323\n",
            "step 64890: train loss = 1.2402, val loss = 1.8084\n",
            "step 64900: train loss = 1.0097, val loss = 2.1954\n",
            "step 64910: train loss = 1.1744, val loss = 1.2085\n",
            "step 64920: train loss = 0.7640, val loss = 1.4907\n",
            "step 64930: train loss = 1.2254, val loss = 1.7060\n",
            "step 64940: train loss = 1.2194, val loss = 1.3893\n",
            "step 64950: train loss = 1.5398, val loss = 1.3078\n",
            "step 64960: train loss = 1.0692, val loss = 1.5236\n",
            "step 64970: train loss = 1.0023, val loss = 1.2037\n",
            "step 64980: train loss = 1.0483, val loss = 1.8740\n",
            "step 64990: train loss = 1.1739, val loss = 1.7733\n",
            "step 65000: train loss = 1.2128, val loss = 1.8159\n",
            "step 65010: train loss = 1.0628, val loss = 1.1963\n",
            "step 65020: train loss = 1.0749, val loss = 1.9082\n",
            "step 65030: train loss = 1.2479, val loss = 1.2818\n",
            "step 65040: train loss = 1.1539, val loss = 1.5979\n",
            "step 65050: train loss = 1.0189, val loss = 1.6286\n",
            "step 65060: train loss = 0.9517, val loss = 1.3887\n",
            "step 65070: train loss = 0.9216, val loss = 1.5836\n",
            "step 65080: train loss = 0.8643, val loss = 1.5063\n",
            "step 65090: train loss = 1.3336, val loss = 1.4115\n",
            "step 65100: train loss = 1.1801, val loss = 1.5791\n",
            "step 65110: train loss = 0.9370, val loss = 1.9332\n",
            "step 65120: train loss = 1.3554, val loss = 1.1563\n",
            "step 65130: train loss = 0.7275, val loss = 1.5998\n",
            "step 65140: train loss = 1.1922, val loss = 1.6797\n",
            "step 65150: train loss = 1.0893, val loss = 1.3002\n",
            "step 65160: train loss = 1.1019, val loss = 1.6488\n",
            "step 65170: train loss = 0.9325, val loss = 1.4152\n",
            "step 65180: train loss = 0.8262, val loss = 1.8909\n",
            "step 65190: train loss = 1.2397, val loss = 1.6244\n",
            "step 65200: train loss = 1.0143, val loss = 1.8771\n",
            "step 65210: train loss = 0.7594, val loss = 1.3993\n",
            "step 65220: train loss = 0.9826, val loss = 1.7248\n",
            "step 65230: train loss = 1.0194, val loss = 1.7140\n",
            "step 65240: train loss = 1.1137, val loss = 1.7085\n",
            "step 65250: train loss = 0.8510, val loss = 1.5742\n",
            "step 65260: train loss = 1.2166, val loss = 1.5330\n",
            "step 65270: train loss = 1.4211, val loss = 1.3067\n",
            "step 65280: train loss = 0.8864, val loss = 1.6981\n",
            "step 65290: train loss = 0.8372, val loss = 1.5863\n",
            "step 65300: train loss = 1.1329, val loss = 1.5861\n",
            "step 65310: train loss = 1.0561, val loss = 1.5077\n",
            "step 65320: train loss = 1.0079, val loss = 1.4564\n",
            "step 65330: train loss = 1.2179, val loss = 1.5750\n",
            "step 65340: train loss = 1.2958, val loss = 1.6067\n",
            "step 65350: train loss = 0.7998, val loss = 1.5169\n",
            "step 65360: train loss = 1.0093, val loss = 1.6303\n",
            "step 65370: train loss = 1.0193, val loss = 1.6926\n",
            "step 65380: train loss = 1.0431, val loss = 1.6951\n",
            "step 65390: train loss = 1.4783, val loss = 1.3537\n",
            "step 65400: train loss = 0.7726, val loss = 1.6662\n",
            "step 65410: train loss = 1.1529, val loss = 1.2642\n",
            "step 65420: train loss = 0.9536, val loss = 1.6217\n",
            "step 65430: train loss = 0.9492, val loss = 1.4636\n",
            "step 65440: train loss = 0.9146, val loss = 1.2237\n",
            "step 65450: train loss = 1.0315, val loss = 1.4015\n",
            "step 65460: train loss = 1.0325, val loss = 2.0997\n",
            "step 65470: train loss = 0.8358, val loss = 1.3709\n",
            "step 65480: train loss = 1.0556, val loss = 1.4375\n",
            "step 65490: train loss = 0.9028, val loss = 1.4559\n",
            "step 65500: train loss = 0.9727, val loss = 1.3168\n",
            "step 65510: train loss = 1.0535, val loss = 1.5693\n",
            "step 65520: train loss = 1.1525, val loss = 1.3940\n",
            "step 65530: train loss = 0.9053, val loss = 1.0895\n",
            "step 65540: train loss = 0.8462, val loss = 1.6185\n",
            "step 65550: train loss = 1.1982, val loss = 1.7832\n",
            "step 65560: train loss = 1.0220, val loss = 1.7766\n",
            "step 65570: train loss = 0.8649, val loss = 1.4058\n",
            "step 65580: train loss = 1.1232, val loss = 1.5493\n",
            "step 65590: train loss = 1.2460, val loss = 1.7727\n",
            "step 65600: train loss = 1.0800, val loss = 1.6587\n",
            "step 65610: train loss = 1.0483, val loss = 1.3575\n",
            "step 65620: train loss = 1.1903, val loss = 1.2969\n",
            "step 65630: train loss = 0.6470, val loss = 1.4004\n",
            "step 65640: train loss = 1.3213, val loss = 1.4782\n",
            "step 65650: train loss = 0.8897, val loss = 1.3829\n",
            "step 65660: train loss = 1.1756, val loss = 1.1253\n",
            "step 65670: train loss = 1.0743, val loss = 1.7247\n",
            "step 65680: train loss = 1.4835, val loss = 1.5105\n",
            "step 65690: train loss = 0.8810, val loss = 2.2854\n",
            "step 65700: train loss = 1.3921, val loss = 1.3625\n",
            "step 65710: train loss = 0.8854, val loss = 1.5107\n",
            "step 65720: train loss = 1.1287, val loss = 1.9154\n",
            "step 65730: train loss = 1.3168, val loss = 1.7102\n",
            "step 65740: train loss = 0.8998, val loss = 1.6683\n",
            "step 65750: train loss = 0.7231, val loss = 1.6354\n",
            "step 65760: train loss = 1.0246, val loss = 1.2228\n",
            "step 65770: train loss = 1.1529, val loss = 1.6918\n",
            "step 65780: train loss = 1.0326, val loss = 1.3698\n",
            "step 65790: train loss = 1.0276, val loss = 1.5271\n",
            "step 65800: train loss = 0.7548, val loss = 1.7915\n",
            "step 65810: train loss = 1.0034, val loss = 1.5628\n",
            "step 65820: train loss = 1.0649, val loss = 1.4597\n",
            "step 65830: train loss = 0.8827, val loss = 1.4698\n",
            "step 65840: train loss = 0.9508, val loss = 1.6615\n",
            "step 65850: train loss = 0.7336, val loss = 1.1571\n",
            "step 65860: train loss = 0.9238, val loss = 1.5826\n",
            "step 65870: train loss = 1.1321, val loss = 1.5390\n",
            "step 65880: train loss = 1.2646, val loss = 1.5405\n",
            "step 65890: train loss = 0.9899, val loss = 1.2743\n",
            "step 65900: train loss = 1.2354, val loss = 1.4707\n",
            "step 65910: train loss = 0.9560, val loss = 1.4048\n",
            "step 65920: train loss = 0.9629, val loss = 0.8266\n",
            "step 65930: train loss = 1.1808, val loss = 1.7480\n",
            "step 65940: train loss = 0.8256, val loss = 1.4378\n",
            "step 65950: train loss = 0.9317, val loss = 1.7395\n",
            "step 65960: train loss = 1.3603, val loss = 1.2809\n",
            "step 65970: train loss = 1.2908, val loss = 1.6009\n",
            "step 65980: train loss = 0.9412, val loss = 1.1513\n",
            "step 65990: train loss = 1.4111, val loss = 1.5727\n",
            "step 66000: train loss = 1.1158, val loss = 1.7721\n",
            "step 66010: train loss = 1.2259, val loss = 1.6666\n",
            "step 66020: train loss = 0.8157, val loss = 1.3401\n",
            "step 66030: train loss = 0.9690, val loss = 1.6583\n",
            "step 66040: train loss = 0.8168, val loss = 1.5826\n",
            "step 66050: train loss = 0.9828, val loss = 1.8845\n",
            "step 66060: train loss = 0.9500, val loss = 1.7731\n",
            "step 66070: train loss = 1.1576, val loss = 1.3207\n",
            "step 66080: train loss = 0.9694, val loss = 1.4683\n",
            "step 66090: train loss = 0.9171, val loss = 1.5195\n",
            "step 66100: train loss = 0.9215, val loss = 2.4029\n",
            "step 66110: train loss = 1.2819, val loss = 1.3249\n",
            "step 66120: train loss = 1.3654, val loss = 1.3264\n",
            "step 66130: train loss = 0.9830, val loss = 1.6534\n",
            "step 66140: train loss = 1.2857, val loss = 2.0524\n",
            "step 66150: train loss = 1.3068, val loss = 1.1679\n",
            "step 66160: train loss = 1.1281, val loss = 1.6270\n",
            "step 66170: train loss = 0.8146, val loss = 1.4071\n",
            "step 66180: train loss = 1.1413, val loss = 1.2361\n",
            "step 66190: train loss = 1.1692, val loss = 1.6370\n",
            "step 66200: train loss = 1.2989, val loss = 1.8056\n",
            "step 66210: train loss = 0.9833, val loss = 1.3538\n",
            "step 66220: train loss = 1.2703, val loss = 1.4583\n",
            "step 66230: train loss = 1.6005, val loss = 1.7549\n",
            "step 66240: train loss = 0.9752, val loss = 1.2162\n",
            "step 66250: train loss = 1.0874, val loss = 1.3698\n",
            "step 66260: train loss = 1.3513, val loss = 1.6924\n",
            "step 66270: train loss = 0.8842, val loss = 1.7163\n",
            "step 66280: train loss = 1.0323, val loss = 1.5740\n",
            "step 66290: train loss = 1.2702, val loss = 1.7483\n",
            "step 66300: train loss = 1.0722, val loss = 1.5381\n",
            "step 66310: train loss = 0.9773, val loss = 0.8696\n",
            "step 66320: train loss = 0.8480, val loss = 2.0874\n",
            "step 66330: train loss = 1.2293, val loss = 1.6389\n",
            "step 66340: train loss = 1.0324, val loss = 1.7881\n",
            "step 66350: train loss = 0.6657, val loss = 1.5398\n",
            "step 66360: train loss = 0.6523, val loss = 1.5601\n",
            "step 66370: train loss = 0.9580, val loss = 1.3658\n",
            "step 66380: train loss = 1.1233, val loss = 1.7407\n",
            "step 66390: train loss = 1.1551, val loss = 1.4817\n",
            "step 66400: train loss = 0.8698, val loss = 1.2851\n",
            "step 66410: train loss = 0.6693, val loss = 1.2779\n",
            "step 66420: train loss = 1.0358, val loss = 1.5938\n",
            "step 66430: train loss = 0.9813, val loss = 1.1811\n",
            "step 66440: train loss = 0.9518, val loss = 1.1287\n",
            "step 66450: train loss = 1.0561, val loss = 1.7737\n",
            "step 66460: train loss = 1.2073, val loss = 1.5532\n",
            "step 66470: train loss = 1.0193, val loss = 1.7714\n",
            "step 66480: train loss = 0.9678, val loss = 1.8658\n",
            "step 66490: train loss = 0.9103, val loss = 1.7422\n",
            "step 66500: train loss = 1.2811, val loss = 1.2479\n",
            "step 66510: train loss = 0.8201, val loss = 1.0195\n",
            "step 66520: train loss = 0.9643, val loss = 1.5641\n",
            "step 66530: train loss = 1.3546, val loss = 1.6401\n",
            "step 66540: train loss = 0.8345, val loss = 2.0637\n",
            "step 66550: train loss = 0.7717, val loss = 1.5386\n",
            "step 66560: train loss = 1.4639, val loss = 1.6250\n",
            "step 66570: train loss = 1.2670, val loss = 1.5017\n",
            "step 66580: train loss = 0.8866, val loss = 1.2628\n",
            "step 66590: train loss = 1.0230, val loss = 1.8228\n",
            "step 66600: train loss = 0.8203, val loss = 1.3460\n",
            "step 66610: train loss = 1.0258, val loss = 1.4038\n",
            "step 66620: train loss = 1.1551, val loss = 1.3198\n",
            "step 66630: train loss = 0.8071, val loss = 1.4861\n",
            "step 66640: train loss = 1.3303, val loss = 2.1192\n",
            "step 66650: train loss = 0.6785, val loss = 1.5994\n",
            "step 66660: train loss = 1.1137, val loss = 1.5506\n",
            "step 66670: train loss = 0.9126, val loss = 1.7136\n",
            "step 66680: train loss = 1.2670, val loss = 1.5238\n",
            "step 66690: train loss = 0.7736, val loss = 1.3953\n",
            "step 66700: train loss = 1.1391, val loss = 1.1761\n",
            "step 66710: train loss = 0.9371, val loss = 1.4249\n",
            "step 66720: train loss = 1.2638, val loss = 1.7922\n",
            "step 66730: train loss = 0.8453, val loss = 1.8159\n",
            "step 66740: train loss = 0.8053, val loss = 1.4587\n",
            "step 66750: train loss = 1.1225, val loss = 1.9105\n",
            "step 66760: train loss = 1.0299, val loss = 1.3131\n",
            "step 66770: train loss = 1.2050, val loss = 1.5778\n",
            "step 66780: train loss = 1.1790, val loss = 1.5221\n",
            "step 66790: train loss = 0.8535, val loss = 1.5841\n",
            "step 66800: train loss = 1.0212, val loss = 1.4330\n",
            "step 66810: train loss = 1.2184, val loss = 1.5298\n",
            "step 66820: train loss = 0.9885, val loss = 1.6594\n",
            "step 66830: train loss = 1.2225, val loss = 1.8875\n",
            "step 66840: train loss = 1.3903, val loss = 1.2290\n",
            "step 66850: train loss = 0.7935, val loss = 1.4489\n",
            "step 66860: train loss = 0.9875, val loss = 1.2103\n",
            "step 66870: train loss = 0.8384, val loss = 1.5217\n",
            "step 66880: train loss = 1.0977, val loss = 1.6338\n",
            "step 66890: train loss = 0.7579, val loss = 1.7061\n",
            "step 66900: train loss = 0.9827, val loss = 1.7670\n",
            "step 66910: train loss = 0.8670, val loss = 1.3049\n",
            "step 66920: train loss = 0.8191, val loss = 1.7670\n",
            "step 66930: train loss = 0.7046, val loss = 2.0267\n",
            "step 66940: train loss = 1.1349, val loss = 1.3382\n",
            "step 66950: train loss = 1.2654, val loss = 1.6846\n",
            "step 66960: train loss = 0.9736, val loss = 1.5944\n",
            "step 66970: train loss = 0.9175, val loss = 1.7827\n",
            "step 66980: train loss = 1.1690, val loss = 1.2323\n",
            "step 66990: train loss = 0.8780, val loss = 1.5859\n",
            "step 67000: train loss = 0.8327, val loss = 1.7133\n",
            "step 67010: train loss = 1.0665, val loss = 1.7009\n",
            "step 67020: train loss = 1.3133, val loss = 1.7912\n",
            "step 67030: train loss = 1.0424, val loss = 1.2798\n",
            "step 67040: train loss = 1.1528, val loss = 1.7395\n",
            "step 67050: train loss = 1.3779, val loss = 1.7493\n",
            "step 67060: train loss = 0.7268, val loss = 1.3461\n",
            "step 67070: train loss = 0.8456, val loss = 1.5435\n",
            "step 67080: train loss = 1.1366, val loss = 1.8183\n",
            "step 67090: train loss = 1.0880, val loss = 1.2896\n",
            "step 67100: train loss = 1.3317, val loss = 1.4505\n",
            "step 67110: train loss = 0.9375, val loss = 1.3602\n",
            "step 67120: train loss = 1.2710, val loss = 1.4623\n",
            "step 67130: train loss = 1.0154, val loss = 1.4309\n",
            "step 67140: train loss = 1.6620, val loss = 1.0634\n",
            "step 67150: train loss = 0.9208, val loss = 1.7356\n",
            "step 67160: train loss = 1.2509, val loss = 1.4208\n",
            "step 67170: train loss = 1.1098, val loss = 1.6218\n",
            "step 67180: train loss = 1.1680, val loss = 1.3230\n",
            "step 67190: train loss = 1.0709, val loss = 1.7843\n",
            "step 67200: train loss = 0.8972, val loss = 1.3562\n",
            "step 67210: train loss = 1.0284, val loss = 1.6559\n",
            "step 67220: train loss = 1.1547, val loss = 2.2653\n",
            "step 67230: train loss = 1.0076, val loss = 2.2174\n",
            "step 67240: train loss = 1.1173, val loss = 1.1098\n",
            "step 67250: train loss = 1.2536, val loss = 1.2338\n",
            "step 67260: train loss = 1.3537, val loss = 1.4079\n",
            "step 67270: train loss = 0.9654, val loss = 2.2740\n",
            "step 67280: train loss = 1.1402, val loss = 2.5008\n",
            "step 67290: train loss = 1.1840, val loss = 1.7872\n",
            "step 67300: train loss = 1.0020, val loss = 1.7678\n",
            "step 67310: train loss = 1.1320, val loss = 1.4191\n",
            "step 67320: train loss = 1.3568, val loss = 1.3708\n",
            "step 67330: train loss = 0.6174, val loss = 1.6930\n",
            "step 67340: train loss = 0.9982, val loss = 1.3896\n",
            "step 67350: train loss = 0.9225, val loss = 1.9044\n",
            "step 67360: train loss = 1.4661, val loss = 1.0684\n",
            "step 67370: train loss = 0.9404, val loss = 1.4862\n",
            "step 67380: train loss = 0.8856, val loss = 0.9292\n",
            "step 67390: train loss = 0.9600, val loss = 1.3998\n",
            "step 67400: train loss = 1.2935, val loss = 1.5649\n",
            "step 67410: train loss = 1.0228, val loss = 1.3964\n",
            "step 67420: train loss = 1.1951, val loss = 1.4680\n",
            "step 67430: train loss = 1.1338, val loss = 1.5372\n",
            "step 67440: train loss = 1.0824, val loss = 1.5354\n",
            "step 67450: train loss = 0.8880, val loss = 1.4344\n",
            "step 67460: train loss = 1.3143, val loss = 1.1656\n",
            "step 67470: train loss = 1.0394, val loss = 1.6839\n",
            "step 67480: train loss = 0.9520, val loss = 1.0825\n",
            "step 67490: train loss = 0.8405, val loss = 1.8741\n",
            "step 67500: train loss = 1.0947, val loss = 1.6428\n",
            "step 67510: train loss = 1.1964, val loss = 1.2851\n",
            "step 67520: train loss = 1.1042, val loss = 1.1592\n",
            "step 67530: train loss = 1.2453, val loss = 1.8356\n",
            "step 67540: train loss = 0.9130, val loss = 1.8940\n",
            "step 67550: train loss = 1.0804, val loss = 1.4624\n",
            "step 67560: train loss = 1.2148, val loss = 1.1942\n",
            "step 67570: train loss = 1.4451, val loss = 1.3066\n",
            "step 67580: train loss = 0.8730, val loss = 1.5766\n",
            "step 67590: train loss = 1.2909, val loss = 2.1318\n",
            "step 67600: train loss = 0.9117, val loss = 1.1367\n",
            "step 67610: train loss = 0.8021, val loss = 1.8586\n",
            "step 67620: train loss = 0.8969, val loss = 1.4104\n",
            "step 67630: train loss = 0.8381, val loss = 1.7573\n",
            "step 67640: train loss = 0.8793, val loss = 1.6097\n",
            "step 67650: train loss = 1.1717, val loss = 1.1972\n",
            "step 67660: train loss = 1.3594, val loss = 1.3120\n",
            "step 67670: train loss = 1.0304, val loss = 1.3382\n",
            "step 67680: train loss = 1.0856, val loss = 1.5048\n",
            "step 67690: train loss = 1.0774, val loss = 1.6894\n",
            "step 67700: train loss = 1.2366, val loss = 1.0873\n",
            "step 67710: train loss = 0.7129, val loss = 1.5613\n",
            "step 67720: train loss = 0.9567, val loss = 1.0334\n",
            "step 67730: train loss = 1.1009, val loss = 2.3211\n",
            "step 67740: train loss = 0.9940, val loss = 1.7980\n",
            "step 67750: train loss = 0.7779, val loss = 1.4215\n",
            "step 67760: train loss = 0.7989, val loss = 1.7657\n",
            "step 67770: train loss = 0.9720, val loss = 1.0826\n",
            "step 67780: train loss = 1.5734, val loss = 1.7842\n",
            "step 67790: train loss = 1.2057, val loss = 1.6595\n",
            "step 67800: train loss = 1.0005, val loss = 1.2869\n",
            "step 67810: train loss = 0.8261, val loss = 2.0911\n",
            "step 67820: train loss = 0.9561, val loss = 1.3733\n",
            "step 67830: train loss = 0.8914, val loss = 1.8739\n",
            "step 67840: train loss = 0.7978, val loss = 1.6510\n",
            "step 67850: train loss = 0.9266, val loss = 1.4745\n",
            "step 67860: train loss = 0.7142, val loss = 1.0969\n",
            "step 67870: train loss = 0.8257, val loss = 1.3156\n",
            "step 67880: train loss = 1.1171, val loss = 1.7625\n",
            "step 67890: train loss = 0.9298, val loss = 1.4315\n",
            "step 67900: train loss = 1.0962, val loss = 1.2271\n",
            "step 67910: train loss = 1.1849, val loss = 1.6991\n",
            "step 67920: train loss = 1.1096, val loss = 1.6539\n",
            "step 67930: train loss = 0.8889, val loss = 1.6866\n",
            "step 67940: train loss = 0.9771, val loss = 1.5637\n",
            "step 67950: train loss = 1.1473, val loss = 1.3861\n",
            "step 67960: train loss = 1.1840, val loss = 1.7425\n",
            "step 67970: train loss = 1.0618, val loss = 1.7671\n",
            "step 67980: train loss = 0.6979, val loss = 1.7808\n",
            "step 67990: train loss = 1.0021, val loss = 1.9650\n",
            "step 68000: train loss = 0.9936, val loss = 1.2788\n",
            "step 68010: train loss = 1.0697, val loss = 2.0305\n",
            "step 68020: train loss = 0.8121, val loss = 1.7815\n",
            "step 68030: train loss = 1.1445, val loss = 1.6574\n",
            "step 68040: train loss = 1.2407, val loss = 1.7779\n",
            "step 68050: train loss = 1.3237, val loss = 1.1917\n",
            "step 68060: train loss = 1.3275, val loss = 1.5049\n",
            "step 68070: train loss = 1.0646, val loss = 1.5468\n",
            "step 68080: train loss = 1.2299, val loss = 1.6845\n",
            "step 68090: train loss = 1.0789, val loss = 1.7045\n",
            "step 68100: train loss = 0.8590, val loss = 1.3664\n",
            "step 68110: train loss = 1.0878, val loss = 1.2996\n",
            "step 68120: train loss = 1.5551, val loss = 1.3682\n",
            "step 68130: train loss = 1.1654, val loss = 1.6646\n",
            "step 68140: train loss = 1.1744, val loss = 1.5735\n",
            "step 68150: train loss = 1.6159, val loss = 1.2308\n",
            "step 68160: train loss = 0.7918, val loss = 1.3296\n",
            "step 68170: train loss = 0.9864, val loss = 1.7513\n",
            "step 68180: train loss = 1.2444, val loss = 1.5668\n",
            "step 68190: train loss = 0.7959, val loss = 1.5275\n",
            "step 68200: train loss = 0.9150, val loss = 2.1673\n",
            "step 68210: train loss = 0.7368, val loss = 1.9075\n",
            "step 68220: train loss = 0.9712, val loss = 1.4335\n",
            "step 68230: train loss = 1.2446, val loss = 1.5156\n",
            "step 68240: train loss = 1.0253, val loss = 1.5567\n",
            "step 68250: train loss = 1.1397, val loss = 1.4053\n",
            "step 68260: train loss = 0.8262, val loss = 1.3153\n",
            "step 68270: train loss = 0.8547, val loss = 1.5135\n",
            "step 68280: train loss = 1.0087, val loss = 1.9203\n",
            "step 68290: train loss = 1.2392, val loss = 1.6727\n",
            "step 68300: train loss = 1.0329, val loss = 1.4510\n",
            "step 68310: train loss = 1.0558, val loss = 1.1568\n",
            "step 68320: train loss = 1.0868, val loss = 1.2315\n",
            "step 68330: train loss = 1.0191, val loss = 1.8653\n",
            "step 68340: train loss = 0.9252, val loss = 1.4390\n",
            "step 68350: train loss = 1.0446, val loss = 2.3658\n",
            "step 68360: train loss = 0.9168, val loss = 1.2598\n",
            "step 68370: train loss = 1.2963, val loss = 1.5004\n",
            "step 68380: train loss = 1.3918, val loss = 1.7902\n",
            "step 68390: train loss = 0.7761, val loss = 1.7474\n",
            "step 68400: train loss = 1.2871, val loss = 1.0805\n",
            "step 68410: train loss = 1.0653, val loss = 1.3696\n",
            "step 68420: train loss = 1.5064, val loss = 1.3241\n",
            "step 68430: train loss = 0.9708, val loss = 0.9862\n",
            "step 68440: train loss = 0.9089, val loss = 1.2799\n",
            "step 68450: train loss = 0.7676, val loss = 1.0297\n",
            "step 68460: train loss = 0.8019, val loss = 0.8700\n",
            "step 68470: train loss = 1.0006, val loss = 1.4150\n",
            "step 68480: train loss = 1.0071, val loss = 1.7530\n",
            "step 68490: train loss = 0.8118, val loss = 1.5417\n",
            "step 68500: train loss = 0.9579, val loss = 1.6723\n",
            "step 68510: train loss = 1.0563, val loss = 2.0974\n",
            "step 68520: train loss = 0.9583, val loss = 1.5216\n",
            "step 68530: train loss = 1.1789, val loss = 1.5364\n",
            "step 68540: train loss = 0.9677, val loss = 1.3600\n",
            "step 68550: train loss = 0.8148, val loss = 1.3893\n",
            "step 68560: train loss = 1.1583, val loss = 1.6273\n",
            "step 68570: train loss = 1.0101, val loss = 1.4922\n",
            "step 68580: train loss = 1.1346, val loss = 1.3894\n",
            "step 68590: train loss = 1.3335, val loss = 1.4371\n",
            "step 68600: train loss = 1.2052, val loss = 2.0509\n",
            "step 68610: train loss = 1.2040, val loss = 1.0419\n",
            "step 68620: train loss = 1.4023, val loss = 1.2123\n",
            "step 68630: train loss = 0.8515, val loss = 1.4861\n",
            "step 68640: train loss = 1.0339, val loss = 1.6350\n",
            "step 68650: train loss = 1.0267, val loss = 1.9048\n",
            "step 68660: train loss = 1.3010, val loss = 1.2556\n",
            "step 68670: train loss = 0.9371, val loss = 1.9498\n",
            "step 68680: train loss = 1.0015, val loss = 2.3435\n",
            "step 68690: train loss = 1.2432, val loss = 1.4210\n",
            "step 68700: train loss = 0.8213, val loss = 1.6011\n",
            "step 68710: train loss = 1.3186, val loss = 1.4612\n",
            "step 68720: train loss = 0.6807, val loss = 1.7025\n",
            "step 68730: train loss = 1.4034, val loss = 1.4808\n",
            "step 68740: train loss = 0.7098, val loss = 1.6521\n",
            "step 68750: train loss = 1.1543, val loss = 1.4892\n",
            "step 68760: train loss = 0.9960, val loss = 1.6065\n",
            "step 68770: train loss = 0.9760, val loss = 1.6326\n",
            "step 68780: train loss = 0.5893, val loss = 1.2572\n",
            "step 68790: train loss = 1.1034, val loss = 1.3460\n",
            "step 68800: train loss = 1.1876, val loss = 1.7032\n",
            "step 68810: train loss = 0.9365, val loss = 1.5563\n",
            "step 68820: train loss = 1.1143, val loss = 1.4758\n",
            "step 68830: train loss = 1.1747, val loss = 1.7327\n",
            "step 68840: train loss = 0.7368, val loss = 1.8595\n",
            "step 68850: train loss = 1.2146, val loss = 1.3424\n",
            "step 68860: train loss = 0.9766, val loss = 1.9044\n",
            "step 68870: train loss = 0.8943, val loss = 1.4954\n",
            "step 68880: train loss = 0.8776, val loss = 2.1350\n",
            "step 68890: train loss = 1.0493, val loss = 1.9966\n",
            "step 68900: train loss = 0.9174, val loss = 1.8055\n",
            "step 68910: train loss = 0.9948, val loss = 1.1130\n",
            "step 68920: train loss = 1.2492, val loss = 1.7105\n",
            "step 68930: train loss = 0.7859, val loss = 1.4639\n",
            "step 68940: train loss = 0.9811, val loss = 1.4974\n",
            "step 68950: train loss = 1.0015, val loss = 1.4811\n",
            "step 68960: train loss = 0.9737, val loss = 1.6324\n",
            "step 68970: train loss = 0.7989, val loss = 1.4836\n",
            "step 68980: train loss = 1.4946, val loss = 1.3300\n",
            "step 68990: train loss = 1.0420, val loss = 2.0026\n",
            "step 69000: train loss = 0.8584, val loss = 2.0771\n",
            "step 69010: train loss = 0.9609, val loss = 2.1156\n",
            "step 69020: train loss = 1.0968, val loss = 1.3291\n",
            "step 69030: train loss = 1.0801, val loss = 1.5486\n",
            "step 69040: train loss = 1.1593, val loss = 1.5822\n",
            "step 69050: train loss = 1.2243, val loss = 1.2385\n",
            "step 69060: train loss = 0.9032, val loss = 1.6814\n",
            "step 69070: train loss = 0.7549, val loss = 1.3056\n",
            "step 69080: train loss = 1.0898, val loss = 1.3606\n",
            "step 69090: train loss = 0.9922, val loss = 1.4787\n",
            "step 69100: train loss = 1.0354, val loss = 1.1823\n",
            "step 69110: train loss = 0.9869, val loss = 1.4233\n",
            "step 69120: train loss = 0.8906, val loss = 1.5645\n",
            "step 69130: train loss = 0.7877, val loss = 1.8728\n",
            "step 69140: train loss = 0.8173, val loss = 1.9887\n",
            "step 69150: train loss = 1.3378, val loss = 1.2940\n",
            "step 69160: train loss = 1.0003, val loss = 1.3599\n",
            "step 69170: train loss = 1.2914, val loss = 1.6110\n",
            "step 69180: train loss = 0.9179, val loss = 1.5033\n",
            "step 69190: train loss = 0.9541, val loss = 2.1606\n",
            "step 69200: train loss = 1.1436, val loss = 1.1550\n",
            "step 69210: train loss = 1.1784, val loss = 1.4043\n",
            "step 69220: train loss = 1.1984, val loss = 1.5455\n",
            "step 69230: train loss = 1.2209, val loss = 1.4370\n",
            "step 69240: train loss = 0.8508, val loss = 1.9570\n",
            "step 69250: train loss = 0.8091, val loss = 1.2965\n",
            "step 69260: train loss = 0.8039, val loss = 1.9131\n",
            "step 69270: train loss = 0.8590, val loss = 1.3843\n",
            "step 69280: train loss = 0.6080, val loss = 1.9335\n",
            "step 69290: train loss = 0.9908, val loss = 1.2203\n",
            "step 69300: train loss = 1.1274, val loss = 1.5141\n",
            "step 69310: train loss = 1.1856, val loss = 1.5519\n",
            "step 69320: train loss = 1.2949, val loss = 1.7055\n",
            "step 69330: train loss = 0.8386, val loss = 2.0795\n",
            "step 69340: train loss = 1.4394, val loss = 1.3568\n",
            "step 69350: train loss = 0.9702, val loss = 1.5514\n",
            "step 69360: train loss = 0.7987, val loss = 1.8496\n",
            "step 69370: train loss = 0.8970, val loss = 1.4087\n",
            "step 69380: train loss = 1.1533, val loss = 1.6238\n",
            "step 69390: train loss = 0.8218, val loss = 1.3455\n",
            "step 69400: train loss = 1.0657, val loss = 1.6769\n",
            "step 69410: train loss = 0.9095, val loss = 1.3206\n",
            "step 69420: train loss = 1.0478, val loss = 1.5754\n",
            "step 69430: train loss = 0.9650, val loss = 1.4374\n",
            "step 69440: train loss = 1.0712, val loss = 1.2293\n",
            "step 69450: train loss = 1.2789, val loss = 1.2698\n",
            "step 69460: train loss = 0.7619, val loss = 1.6453\n",
            "step 69470: train loss = 0.8113, val loss = 1.4160\n",
            "step 69480: train loss = 1.4561, val loss = 1.4846\n",
            "step 69490: train loss = 1.0537, val loss = 1.4057\n",
            "step 69500: train loss = 0.7904, val loss = 1.5282\n",
            "step 69510: train loss = 1.1764, val loss = 1.8142\n",
            "step 69520: train loss = 0.7284, val loss = 1.4005\n",
            "step 69530: train loss = 0.9301, val loss = 1.3522\n",
            "step 69540: train loss = 0.8937, val loss = 1.8231\n",
            "step 69550: train loss = 0.9533, val loss = 1.7158\n",
            "step 69560: train loss = 0.7988, val loss = 1.2859\n",
            "step 69570: train loss = 0.7926, val loss = 1.4130\n",
            "step 69580: train loss = 0.9429, val loss = 1.4853\n",
            "step 69590: train loss = 1.2322, val loss = 1.3869\n",
            "step 69600: train loss = 1.0781, val loss = 1.5548\n",
            "step 69610: train loss = 1.2590, val loss = 1.4548\n",
            "step 69620: train loss = 0.6870, val loss = 1.0615\n",
            "step 69630: train loss = 1.5513, val loss = 1.5261\n",
            "step 69640: train loss = 1.0953, val loss = 1.3738\n",
            "step 69650: train loss = 0.8238, val loss = 1.6758\n",
            "step 69660: train loss = 1.0515, val loss = 1.6725\n",
            "step 69670: train loss = 1.0590, val loss = 1.2908\n",
            "step 69680: train loss = 1.4438, val loss = 1.4934\n",
            "step 69690: train loss = 1.1088, val loss = 1.8339\n",
            "step 69700: train loss = 0.8224, val loss = 1.4267\n",
            "step 69710: train loss = 1.0469, val loss = 2.0271\n",
            "step 69720: train loss = 0.6119, val loss = 1.7412\n",
            "step 69730: train loss = 0.9724, val loss = 1.4362\n",
            "step 69740: train loss = 1.1745, val loss = 1.2503\n",
            "step 69750: train loss = 0.8238, val loss = 1.2563\n",
            "step 69760: train loss = 1.0733, val loss = 1.4112\n",
            "step 69770: train loss = 1.5033, val loss = 1.6962\n",
            "step 69780: train loss = 1.0807, val loss = 2.0964\n",
            "step 69790: train loss = 1.0438, val loss = 1.3833\n",
            "step 69800: train loss = 0.8450, val loss = 1.2634\n",
            "step 69810: train loss = 0.6787, val loss = 1.6745\n",
            "step 69820: train loss = 1.4271, val loss = 1.3612\n",
            "step 69830: train loss = 0.9812, val loss = 1.9381\n",
            "step 69840: train loss = 1.1186, val loss = 1.6556\n",
            "step 69850: train loss = 1.2799, val loss = 1.8712\n",
            "step 69860: train loss = 0.8659, val loss = 1.0950\n",
            "step 69870: train loss = 1.2094, val loss = 1.2257\n",
            "step 69880: train loss = 1.0638, val loss = 1.3437\n",
            "step 69890: train loss = 0.9406, val loss = 1.3945\n",
            "step 69900: train loss = 1.0586, val loss = 1.3326\n",
            "step 69910: train loss = 0.6839, val loss = 1.7843\n",
            "step 69920: train loss = 0.9644, val loss = 1.9195\n",
            "step 69930: train loss = 0.7010, val loss = 1.7519\n",
            "step 69940: train loss = 1.0767, val loss = 1.4494\n",
            "step 69950: train loss = 0.9569, val loss = 1.0407\n",
            "step 69960: train loss = 0.8776, val loss = 1.2522\n",
            "step 69970: train loss = 1.2458, val loss = 1.6535\n",
            "step 69980: train loss = 0.7376, val loss = 0.9858\n",
            "step 69990: train loss = 1.0338, val loss = 1.6152\n",
            "step 70000: train loss = 1.1230, val loss = 1.5073\n",
            "step 70010: train loss = 1.2695, val loss = 1.3879\n",
            "step 70020: train loss = 0.9954, val loss = 1.2819\n",
            "step 70030: train loss = 0.6892, val loss = 1.1115\n",
            "step 70040: train loss = 1.1216, val loss = 1.6830\n",
            "step 70050: train loss = 0.8681, val loss = 1.5608\n",
            "step 70060: train loss = 1.1987, val loss = 2.2545\n",
            "step 70070: train loss = 1.0808, val loss = 1.2156\n",
            "step 70080: train loss = 0.9501, val loss = 1.9660\n",
            "step 70090: train loss = 1.2620, val loss = 1.6402\n",
            "step 70100: train loss = 0.9620, val loss = 2.0828\n",
            "step 70110: train loss = 0.8802, val loss = 1.7041\n",
            "step 70120: train loss = 1.1788, val loss = 1.4325\n",
            "step 70130: train loss = 1.0282, val loss = 1.6368\n",
            "step 70140: train loss = 0.9151, val loss = 1.7219\n",
            "step 70150: train loss = 0.9218, val loss = 1.7249\n",
            "step 70160: train loss = 0.7797, val loss = 1.1082\n",
            "step 70170: train loss = 1.1534, val loss = 1.8092\n",
            "step 70180: train loss = 1.0444, val loss = 1.6730\n",
            "step 70190: train loss = 1.0211, val loss = 1.7658\n",
            "step 70200: train loss = 1.1147, val loss = 1.7402\n",
            "step 70210: train loss = 0.7968, val loss = 1.5910\n",
            "step 70220: train loss = 0.8425, val loss = 1.4922\n",
            "step 70230: train loss = 0.9231, val loss = 1.3687\n",
            "step 70240: train loss = 0.9124, val loss = 1.3824\n",
            "step 70250: train loss = 0.9705, val loss = 1.2989\n",
            "step 70260: train loss = 0.8093, val loss = 1.5350\n",
            "step 70270: train loss = 1.0608, val loss = 1.1172\n",
            "step 70280: train loss = 1.2738, val loss = 1.4805\n",
            "step 70290: train loss = 1.1327, val loss = 1.7749\n",
            "step 70300: train loss = 0.7092, val loss = 1.3026\n",
            "step 70310: train loss = 0.9900, val loss = 1.8190\n",
            "step 70320: train loss = 0.8989, val loss = 1.3301\n",
            "step 70330: train loss = 1.1359, val loss = 1.5753\n",
            "step 70340: train loss = 1.1971, val loss = 1.8196\n",
            "step 70350: train loss = 1.1439, val loss = 1.6407\n",
            "step 70360: train loss = 0.9796, val loss = 1.5996\n",
            "step 70370: train loss = 0.9256, val loss = 1.1876\n",
            "step 70380: train loss = 1.0545, val loss = 1.7558\n",
            "step 70390: train loss = 1.1712, val loss = 1.7092\n",
            "step 70400: train loss = 0.7840, val loss = 1.4826\n",
            "step 70410: train loss = 1.0452, val loss = 1.7421\n",
            "step 70420: train loss = 0.7626, val loss = 1.3393\n",
            "step 70430: train loss = 1.3137, val loss = 1.4527\n",
            "step 70440: train loss = 1.2567, val loss = 1.5019\n",
            "step 70450: train loss = 1.0482, val loss = 1.5546\n",
            "step 70460: train loss = 0.8871, val loss = 1.7723\n",
            "step 70470: train loss = 0.8716, val loss = 1.2880\n",
            "step 70480: train loss = 0.8310, val loss = 1.5096\n",
            "step 70490: train loss = 1.3634, val loss = 1.3486\n",
            "step 70500: train loss = 1.5280, val loss = 1.8923\n",
            "step 70510: train loss = 0.8710, val loss = 1.4707\n",
            "step 70520: train loss = 1.3037, val loss = 1.3254\n",
            "step 70530: train loss = 0.7757, val loss = 1.7880\n",
            "step 70540: train loss = 0.9158, val loss = 2.3249\n",
            "step 70550: train loss = 0.8008, val loss = 1.1000\n",
            "step 70560: train loss = 1.1090, val loss = 1.4803\n",
            "step 70570: train loss = 1.0739, val loss = 1.4570\n",
            "step 70580: train loss = 0.7986, val loss = 1.3306\n",
            "step 70590: train loss = 0.8643, val loss = 1.1042\n",
            "step 70600: train loss = 1.2220, val loss = 1.6026\n",
            "step 70610: train loss = 1.2781, val loss = 1.5706\n",
            "step 70620: train loss = 1.1567, val loss = 2.0983\n",
            "step 70630: train loss = 1.1143, val loss = 1.5201\n",
            "step 70640: train loss = 0.6526, val loss = 1.8664\n",
            "step 70650: train loss = 1.4795, val loss = 2.2666\n",
            "step 70660: train loss = 0.9287, val loss = 1.8540\n",
            "step 70670: train loss = 0.9672, val loss = 1.2869\n",
            "step 70680: train loss = 1.0560, val loss = 1.0567\n",
            "step 70690: train loss = 0.6619, val loss = 1.3208\n",
            "step 70700: train loss = 0.8227, val loss = 1.3002\n",
            "step 70710: train loss = 1.0491, val loss = 1.6112\n",
            "step 70720: train loss = 1.1174, val loss = 1.5480\n",
            "step 70730: train loss = 1.1304, val loss = 1.3810\n",
            "step 70740: train loss = 1.0346, val loss = 1.7175\n",
            "step 70750: train loss = 1.1344, val loss = 1.5074\n",
            "step 70760: train loss = 0.8187, val loss = 1.6787\n",
            "step 70770: train loss = 1.0852, val loss = 1.7349\n",
            "step 70780: train loss = 1.1289, val loss = 1.5320\n",
            "step 70790: train loss = 1.2072, val loss = 1.7046\n",
            "step 70800: train loss = 1.0262, val loss = 1.8869\n",
            "step 70810: train loss = 0.8854, val loss = 1.5829\n",
            "step 70820: train loss = 1.1953, val loss = 1.6922\n",
            "step 70830: train loss = 1.1108, val loss = 1.1695\n",
            "step 70840: train loss = 1.1157, val loss = 1.7335\n",
            "step 70850: train loss = 0.7602, val loss = 1.6093\n",
            "step 70860: train loss = 0.9815, val loss = 1.5457\n",
            "step 70870: train loss = 0.8597, val loss = 1.1065\n",
            "step 70880: train loss = 0.9992, val loss = 1.6956\n",
            "step 70890: train loss = 1.0840, val loss = 1.5862\n",
            "step 70900: train loss = 0.8084, val loss = 1.4849\n",
            "step 70910: train loss = 0.9767, val loss = 1.6472\n",
            "step 70920: train loss = 1.0995, val loss = 1.7968\n",
            "step 70930: train loss = 0.6164, val loss = 1.3947\n",
            "step 70940: train loss = 1.2003, val loss = 1.2847\n",
            "step 70950: train loss = 0.8393, val loss = 1.4945\n",
            "step 70960: train loss = 0.8783, val loss = 1.8365\n",
            "step 70970: train loss = 0.7420, val loss = 1.4822\n",
            "step 70980: train loss = 0.9760, val loss = 1.3444\n",
            "step 70990: train loss = 1.2635, val loss = 1.6425\n",
            "step 71000: train loss = 0.9482, val loss = 1.3061\n",
            "step 71010: train loss = 0.9149, val loss = 1.6278\n",
            "step 71020: train loss = 1.0704, val loss = 1.3180\n",
            "step 71030: train loss = 0.6985, val loss = 2.0606\n",
            "step 71040: train loss = 0.7978, val loss = 1.8109\n",
            "step 71050: train loss = 1.2360, val loss = 1.2882\n",
            "step 71060: train loss = 0.9902, val loss = 1.8306\n",
            "step 71070: train loss = 1.0075, val loss = 1.5192\n",
            "step 71080: train loss = 0.8741, val loss = 1.5518\n",
            "step 71090: train loss = 0.8340, val loss = 1.2700\n",
            "step 71100: train loss = 0.8774, val loss = 1.6164\n",
            "step 71110: train loss = 1.2005, val loss = 1.3686\n",
            "step 71120: train loss = 0.9535, val loss = 1.3546\n",
            "step 71130: train loss = 0.8705, val loss = 1.4449\n",
            "step 71140: train loss = 0.9942, val loss = 1.1813\n",
            "step 71150: train loss = 0.6580, val loss = 2.1835\n",
            "step 71160: train loss = 0.9128, val loss = 1.7776\n",
            "step 71170: train loss = 1.2918, val loss = 1.7223\n",
            "step 71180: train loss = 1.1621, val loss = 1.3343\n",
            "step 71190: train loss = 1.0088, val loss = 1.2667\n",
            "step 71200: train loss = 0.9039, val loss = 1.8559\n",
            "step 71210: train loss = 0.9842, val loss = 1.8576\n",
            "step 71220: train loss = 1.0019, val loss = 1.7755\n",
            "step 71230: train loss = 0.9729, val loss = 1.8731\n",
            "step 71240: train loss = 1.2875, val loss = 2.0707\n",
            "step 71250: train loss = 1.2588, val loss = 1.5400\n",
            "step 71260: train loss = 0.9396, val loss = 1.2205\n",
            "step 71270: train loss = 1.0880, val loss = 1.4538\n",
            "step 71280: train loss = 1.0897, val loss = 1.7170\n",
            "step 71290: train loss = 1.0757, val loss = 1.8870\n",
            "step 71300: train loss = 0.6382, val loss = 1.4376\n",
            "step 71310: train loss = 0.6230, val loss = 1.1523\n",
            "step 71320: train loss = 0.8532, val loss = 1.7215\n",
            "step 71330: train loss = 1.2514, val loss = 1.5101\n",
            "step 71340: train loss = 1.1448, val loss = 1.7261\n",
            "step 71350: train loss = 1.2926, val loss = 1.7585\n",
            "step 71360: train loss = 1.0368, val loss = 1.3142\n",
            "step 71370: train loss = 0.7448, val loss = 1.4248\n",
            "step 71380: train loss = 1.3116, val loss = 1.8120\n",
            "step 71390: train loss = 1.0343, val loss = 1.5178\n",
            "step 71400: train loss = 0.9480, val loss = 1.5631\n",
            "step 71410: train loss = 1.2955, val loss = 1.3767\n",
            "step 71420: train loss = 1.4350, val loss = 1.4318\n",
            "step 71430: train loss = 1.1733, val loss = 1.7345\n",
            "step 71440: train loss = 0.9658, val loss = 1.6050\n",
            "step 71450: train loss = 1.0126, val loss = 2.0720\n",
            "step 71460: train loss = 0.7531, val loss = 1.7914\n",
            "step 71470: train loss = 0.6023, val loss = 1.4996\n",
            "step 71480: train loss = 0.9642, val loss = 1.5209\n",
            "step 71490: train loss = 1.2025, val loss = 1.6277\n",
            "step 71500: train loss = 0.7765, val loss = 1.6919\n",
            "step 71510: train loss = 0.9967, val loss = 1.7104\n",
            "step 71520: train loss = 0.8081, val loss = 1.5250\n",
            "step 71530: train loss = 0.7439, val loss = 1.6379\n",
            "step 71540: train loss = 0.8197, val loss = 1.6751\n",
            "step 71550: train loss = 1.2715, val loss = 1.6259\n",
            "step 71560: train loss = 0.9284, val loss = 1.3425\n",
            "step 71570: train loss = 1.0752, val loss = 1.3740\n",
            "step 71580: train loss = 0.7875, val loss = 1.1470\n",
            "step 71590: train loss = 0.7783, val loss = 1.7232\n",
            "step 71600: train loss = 1.3839, val loss = 1.3929\n",
            "step 71610: train loss = 0.9322, val loss = 1.5240\n",
            "step 71620: train loss = 0.9401, val loss = 1.6867\n",
            "step 71630: train loss = 0.9517, val loss = 1.3965\n",
            "step 71640: train loss = 1.1218, val loss = 1.1487\n",
            "step 71650: train loss = 0.9518, val loss = 1.3520\n",
            "step 71660: train loss = 0.9436, val loss = 1.2914\n",
            "step 71670: train loss = 1.1127, val loss = 2.2859\n",
            "step 71680: train loss = 0.9836, val loss = 1.3514\n",
            "step 71690: train loss = 0.9407, val loss = 1.4028\n",
            "step 71700: train loss = 1.2110, val loss = 1.3695\n",
            "step 71710: train loss = 1.1512, val loss = 1.4946\n",
            "step 71720: train loss = 0.8729, val loss = 1.1975\n",
            "step 71730: train loss = 0.7589, val loss = 1.3636\n",
            "step 71740: train loss = 0.7816, val loss = 1.4080\n",
            "step 71750: train loss = 1.0815, val loss = 1.6564\n",
            "step 71760: train loss = 0.9737, val loss = 1.5445\n",
            "step 71770: train loss = 0.6886, val loss = 1.2789\n",
            "step 71780: train loss = 1.0597, val loss = 2.1231\n",
            "step 71790: train loss = 1.3502, val loss = 1.5879\n",
            "step 71800: train loss = 1.0431, val loss = 1.5678\n",
            "step 71810: train loss = 0.7705, val loss = 1.4392\n",
            "step 71820: train loss = 0.9795, val loss = 2.0522\n",
            "step 71830: train loss = 0.8832, val loss = 1.7842\n",
            "step 71840: train loss = 1.0542, val loss = 1.7581\n",
            "step 71850: train loss = 1.2750, val loss = 1.8227\n",
            "step 71860: train loss = 1.2129, val loss = 1.8257\n",
            "step 71870: train loss = 0.4776, val loss = 1.6233\n",
            "step 71880: train loss = 0.8855, val loss = 1.8739\n",
            "step 71890: train loss = 0.7307, val loss = 1.6704\n",
            "step 71900: train loss = 0.7565, val loss = 1.7328\n",
            "step 71910: train loss = 0.8828, val loss = 1.4570\n",
            "step 71920: train loss = 0.7787, val loss = 1.9699\n",
            "step 71930: train loss = 0.8536, val loss = 1.5126\n",
            "step 71940: train loss = 0.8165, val loss = 2.1182\n",
            "step 71950: train loss = 1.2300, val loss = 1.2266\n",
            "step 71960: train loss = 1.0760, val loss = 1.2378\n",
            "step 71970: train loss = 0.9587, val loss = 1.6590\n",
            "step 71980: train loss = 1.2155, val loss = 2.2018\n",
            "step 71990: train loss = 1.1973, val loss = 1.6847\n",
            "step 72000: train loss = 1.0268, val loss = 1.5140\n",
            "step 72010: train loss = 0.7759, val loss = 2.0420\n",
            "step 72020: train loss = 0.8779, val loss = 1.9866\n",
            "step 72030: train loss = 0.9364, val loss = 1.6285\n",
            "step 72040: train loss = 1.0481, val loss = 1.3663\n",
            "step 72050: train loss = 1.0162, val loss = 1.3209\n",
            "step 72060: train loss = 1.1515, val loss = 1.8135\n",
            "step 72070: train loss = 1.1882, val loss = 2.3188\n",
            "step 72080: train loss = 1.1835, val loss = 1.9921\n",
            "step 72090: train loss = 1.3218, val loss = 1.9464\n",
            "step 72100: train loss = 0.9151, val loss = 1.6822\n",
            "step 72110: train loss = 0.7469, val loss = 2.1692\n",
            "step 72120: train loss = 1.1180, val loss = 1.4946\n",
            "step 72130: train loss = 0.5559, val loss = 1.3158\n",
            "step 72140: train loss = 0.5297, val loss = 1.5036\n",
            "step 72150: train loss = 0.9032, val loss = 1.5636\n",
            "step 72160: train loss = 0.8937, val loss = 1.8578\n",
            "step 72170: train loss = 1.1674, val loss = 1.9857\n",
            "step 72180: train loss = 0.8605, val loss = 1.3611\n",
            "step 72190: train loss = 1.2984, val loss = 1.3993\n",
            "step 72200: train loss = 0.8415, val loss = 1.6470\n",
            "step 72210: train loss = 1.1186, val loss = 1.8340\n",
            "step 72220: train loss = 0.6517, val loss = 1.7411\n",
            "step 72230: train loss = 0.8534, val loss = 1.5043\n",
            "step 72240: train loss = 0.6761, val loss = 1.8215\n",
            "step 72250: train loss = 0.8622, val loss = 1.8198\n",
            "step 72260: train loss = 1.1410, val loss = 1.1316\n",
            "step 72270: train loss = 0.9242, val loss = 1.8466\n",
            "step 72280: train loss = 0.7976, val loss = 2.0555\n",
            "step 72290: train loss = 1.5452, val loss = 1.9266\n",
            "step 72300: train loss = 0.7021, val loss = 1.4889\n",
            "step 72310: train loss = 1.1105, val loss = 1.6295\n",
            "step 72320: train loss = 1.3085, val loss = 1.3051\n",
            "step 72330: train loss = 0.9670, val loss = 1.7745\n",
            "step 72340: train loss = 1.1723, val loss = 1.1624\n",
            "step 72350: train loss = 1.2484, val loss = 1.4453\n",
            "step 72360: train loss = 1.1437, val loss = 2.0159\n",
            "step 72370: train loss = 0.6314, val loss = 1.2218\n",
            "step 72380: train loss = 0.9151, val loss = 1.4875\n",
            "step 72390: train loss = 1.0869, val loss = 1.2649\n",
            "step 72400: train loss = 1.0474, val loss = 1.5489\n",
            "step 72410: train loss = 1.1773, val loss = 1.9361\n",
            "step 72420: train loss = 1.2105, val loss = 1.0223\n",
            "step 72430: train loss = 1.1966, val loss = 1.8741\n",
            "step 72440: train loss = 1.1709, val loss = 1.2338\n",
            "step 72450: train loss = 0.9422, val loss = 1.0207\n",
            "step 72460: train loss = 0.8644, val loss = 1.3877\n",
            "step 72470: train loss = 0.8907, val loss = 1.5830\n",
            "step 72480: train loss = 0.7977, val loss = 1.6762\n",
            "step 72490: train loss = 1.0281, val loss = 1.4245\n",
            "step 72500: train loss = 1.1405, val loss = 1.3603\n",
            "step 72510: train loss = 1.2134, val loss = 1.8346\n",
            "step 72520: train loss = 0.9720, val loss = 1.5657\n",
            "step 72530: train loss = 1.0046, val loss = 1.3541\n",
            "step 72540: train loss = 1.0543, val loss = 1.6780\n",
            "step 72550: train loss = 0.7414, val loss = 1.5090\n",
            "step 72560: train loss = 1.0003, val loss = 1.8237\n",
            "step 72570: train loss = 1.0498, val loss = 1.1766\n",
            "step 72580: train loss = 0.9480, val loss = 1.3374\n",
            "step 72590: train loss = 0.8689, val loss = 1.4810\n",
            "step 72600: train loss = 0.9544, val loss = 1.5581\n",
            "step 72610: train loss = 0.6781, val loss = 1.2734\n",
            "step 72620: train loss = 0.7461, val loss = 1.2782\n",
            "step 72630: train loss = 0.9405, val loss = 1.2304\n",
            "step 72640: train loss = 0.9278, val loss = 1.9475\n",
            "step 72650: train loss = 0.7560, val loss = 1.5663\n",
            "step 72660: train loss = 0.5602, val loss = 2.1560\n",
            "step 72670: train loss = 1.5200, val loss = 1.2226\n",
            "step 72680: train loss = 1.2515, val loss = 1.8701\n",
            "step 72690: train loss = 0.9312, val loss = 1.7882\n",
            "step 72700: train loss = 0.8563, val loss = 1.4634\n",
            "step 72710: train loss = 0.8823, val loss = 2.3496\n",
            "step 72720: train loss = 0.7904, val loss = 1.5826\n",
            "step 72730: train loss = 1.0772, val loss = 1.0824\n",
            "step 72740: train loss = 0.9099, val loss = 1.6194\n",
            "step 72750: train loss = 0.9382, val loss = 1.5340\n",
            "step 72760: train loss = 0.7606, val loss = 1.3205\n",
            "step 72770: train loss = 0.8871, val loss = 1.5878\n",
            "step 72780: train loss = 0.9139, val loss = 1.9636\n",
            "step 72790: train loss = 0.6927, val loss = 1.4726\n",
            "step 72800: train loss = 0.9053, val loss = 1.5218\n",
            "step 72810: train loss = 0.8287, val loss = 1.6767\n",
            "step 72820: train loss = 0.9078, val loss = 1.7528\n",
            "step 72830: train loss = 1.2334, val loss = 1.9994\n",
            "step 72840: train loss = 0.9656, val loss = 1.8037\n",
            "step 72850: train loss = 0.9806, val loss = 1.4828\n",
            "step 72860: train loss = 1.0019, val loss = 1.5449\n",
            "step 72870: train loss = 0.8965, val loss = 1.5539\n",
            "step 72880: train loss = 0.8537, val loss = 2.0804\n",
            "step 72890: train loss = 0.8699, val loss = 1.6487\n",
            "step 72900: train loss = 0.8821, val loss = 1.0715\n",
            "step 72910: train loss = 0.9554, val loss = 1.4057\n",
            "step 72920: train loss = 0.8943, val loss = 1.6722\n",
            "step 72930: train loss = 1.0177, val loss = 1.8183\n",
            "step 72940: train loss = 0.8928, val loss = 1.5014\n",
            "step 72950: train loss = 0.9249, val loss = 1.5883\n",
            "step 72960: train loss = 0.8804, val loss = 1.4268\n",
            "step 72970: train loss = 1.2232, val loss = 1.4267\n",
            "step 72980: train loss = 0.8601, val loss = 1.7847\n",
            "step 72990: train loss = 1.3083, val loss = 1.8836\n",
            "step 73000: train loss = 0.6857, val loss = 1.4972\n",
            "step 73010: train loss = 0.7454, val loss = 1.7352\n",
            "step 73020: train loss = 0.8786, val loss = 1.5434\n",
            "step 73030: train loss = 1.0325, val loss = 1.5074\n",
            "step 73040: train loss = 1.0825, val loss = 1.5854\n",
            "step 73050: train loss = 1.1056, val loss = 1.4538\n",
            "step 73060: train loss = 0.8118, val loss = 1.4076\n",
            "step 73070: train loss = 1.2756, val loss = 1.7704\n",
            "step 73080: train loss = 0.9164, val loss = 1.2586\n",
            "step 73090: train loss = 0.8504, val loss = 1.3658\n",
            "step 73100: train loss = 1.1617, val loss = 1.4757\n",
            "step 73110: train loss = 1.0290, val loss = 1.9782\n",
            "step 73120: train loss = 1.0253, val loss = 1.0252\n",
            "step 73130: train loss = 0.9280, val loss = 1.1336\n",
            "step 73140: train loss = 0.9772, val loss = 1.6208\n",
            "step 73150: train loss = 0.7522, val loss = 1.1027\n",
            "step 73160: train loss = 1.1469, val loss = 1.8313\n",
            "step 73170: train loss = 0.8923, val loss = 1.4578\n",
            "step 73180: train loss = 0.7571, val loss = 1.8088\n",
            "step 73190: train loss = 0.8073, val loss = 1.7561\n",
            "step 73200: train loss = 0.7870, val loss = 1.8452\n",
            "step 73210: train loss = 0.9118, val loss = 1.8579\n",
            "step 73220: train loss = 1.0854, val loss = 1.9360\n",
            "step 73230: train loss = 1.1489, val loss = 1.5714\n",
            "step 73240: train loss = 1.3416, val loss = 1.0620\n",
            "step 73250: train loss = 0.8531, val loss = 1.5396\n",
            "step 73260: train loss = 0.7541, val loss = 1.9887\n",
            "step 73270: train loss = 0.7611, val loss = 1.4702\n",
            "step 73280: train loss = 0.8440, val loss = 1.4627\n",
            "step 73290: train loss = 0.9359, val loss = 1.9569\n",
            "step 73300: train loss = 0.8302, val loss = 1.4604\n",
            "step 73310: train loss = 1.2046, val loss = 1.2816\n",
            "step 73320: train loss = 0.8868, val loss = 1.3598\n",
            "step 73330: train loss = 1.2421, val loss = 1.8552\n",
            "step 73340: train loss = 0.7958, val loss = 1.4166\n",
            "step 73350: train loss = 0.9298, val loss = 1.4772\n",
            "step 73360: train loss = 0.8734, val loss = 1.4614\n",
            "step 73370: train loss = 1.1518, val loss = 1.7758\n",
            "step 73380: train loss = 1.2415, val loss = 1.6760\n",
            "step 73390: train loss = 0.6069, val loss = 1.5244\n",
            "step 73400: train loss = 0.9178, val loss = 1.5759\n",
            "step 73410: train loss = 0.6018, val loss = 1.3093\n",
            "step 73420: train loss = 0.9324, val loss = 1.1761\n",
            "step 73430: train loss = 0.8156, val loss = 1.8365\n",
            "step 73440: train loss = 0.8954, val loss = 1.2879\n",
            "step 73450: train loss = 1.1791, val loss = 1.1744\n",
            "step 73460: train loss = 0.8454, val loss = 1.4354\n",
            "step 73470: train loss = 0.7137, val loss = 1.6328\n",
            "step 73480: train loss = 1.1508, val loss = 1.1958\n",
            "step 73490: train loss = 0.8969, val loss = 1.8033\n",
            "step 73500: train loss = 0.7794, val loss = 2.0879\n",
            "step 73510: train loss = 1.0318, val loss = 1.3475\n",
            "step 73520: train loss = 1.1621, val loss = 1.9113\n",
            "step 73530: train loss = 0.9770, val loss = 1.4045\n",
            "step 73540: train loss = 1.0481, val loss = 1.7470\n",
            "step 73550: train loss = 0.6222, val loss = 1.9487\n",
            "step 73560: train loss = 0.6848, val loss = 1.6255\n",
            "step 73570: train loss = 1.0059, val loss = 1.7288\n",
            "step 73580: train loss = 0.9825, val loss = 1.9812\n",
            "step 73590: train loss = 0.8137, val loss = 1.6572\n",
            "step 73600: train loss = 1.2071, val loss = 1.4232\n",
            "step 73610: train loss = 1.1922, val loss = 1.2992\n",
            "step 73620: train loss = 1.2424, val loss = 1.6965\n",
            "step 73630: train loss = 1.0188, val loss = 1.5894\n",
            "step 73640: train loss = 1.0249, val loss = 2.0946\n",
            "step 73650: train loss = 0.8846, val loss = 1.0587\n",
            "step 73660: train loss = 0.8044, val loss = 1.2264\n",
            "step 73670: train loss = 0.8523, val loss = 1.4771\n",
            "step 73680: train loss = 0.7762, val loss = 1.7221\n",
            "step 73690: train loss = 0.6077, val loss = 1.4337\n",
            "step 73700: train loss = 0.8459, val loss = 2.1730\n",
            "step 73710: train loss = 1.0356, val loss = 1.7297\n",
            "step 73720: train loss = 0.9107, val loss = 1.7777\n",
            "step 73730: train loss = 0.7443, val loss = 1.7757\n",
            "step 73740: train loss = 0.4792, val loss = 1.1216\n",
            "step 73750: train loss = 1.4587, val loss = 1.5458\n",
            "step 73760: train loss = 0.7660, val loss = 1.5585\n",
            "step 73770: train loss = 1.3920, val loss = 1.7169\n",
            "step 73780: train loss = 0.7940, val loss = 1.0269\n",
            "step 73790: train loss = 0.5298, val loss = 1.3140\n",
            "step 73800: train loss = 0.9021, val loss = 0.9861\n",
            "step 73810: train loss = 0.9572, val loss = 1.5790\n",
            "step 73820: train loss = 0.9645, val loss = 1.4152\n",
            "step 73830: train loss = 0.8447, val loss = 1.1160\n",
            "step 73840: train loss = 0.9115, val loss = 1.3448\n",
            "step 73850: train loss = 1.0290, val loss = 1.5383\n",
            "step 73860: train loss = 0.9723, val loss = 1.3679\n",
            "step 73870: train loss = 0.9991, val loss = 2.0035\n",
            "step 73880: train loss = 1.2706, val loss = 1.3962\n",
            "step 73890: train loss = 1.0884, val loss = 1.4507\n",
            "step 73900: train loss = 0.8815, val loss = 1.7315\n",
            "step 73910: train loss = 1.2970, val loss = 2.0953\n",
            "step 73920: train loss = 1.1448, val loss = 1.4061\n",
            "step 73930: train loss = 0.7318, val loss = 1.5997\n",
            "step 73940: train loss = 1.0171, val loss = 1.4178\n",
            "step 73950: train loss = 0.6876, val loss = 1.9764\n",
            "step 73960: train loss = 0.7423, val loss = 1.9641\n",
            "step 73970: train loss = 0.9646, val loss = 1.8919\n",
            "step 73980: train loss = 0.9319, val loss = 1.3391\n",
            "step 73990: train loss = 0.8023, val loss = 1.9020\n",
            "step 74000: train loss = 0.5557, val loss = 1.5804\n",
            "step 74010: train loss = 0.9835, val loss = 1.8306\n",
            "step 74020: train loss = 1.2108, val loss = 1.1684\n",
            "step 74030: train loss = 1.0930, val loss = 1.6309\n",
            "step 74040: train loss = 0.8040, val loss = 1.1801\n",
            "step 74050: train loss = 0.9342, val loss = 1.0402\n",
            "step 74060: train loss = 0.7805, val loss = 2.0089\n",
            "step 74070: train loss = 0.6617, val loss = 1.5072\n",
            "step 74080: train loss = 0.8369, val loss = 1.9256\n",
            "step 74090: train loss = 1.1594, val loss = 1.2449\n",
            "step 74100: train loss = 0.9534, val loss = 1.6289\n",
            "step 74110: train loss = 0.6439, val loss = 1.4484\n",
            "step 74120: train loss = 0.8590, val loss = 1.8031\n",
            "step 74130: train loss = 1.0719, val loss = 1.3373\n",
            "step 74140: train loss = 1.0102, val loss = 1.5446\n",
            "step 74150: train loss = 1.2321, val loss = 1.5147\n",
            "step 74160: train loss = 0.9656, val loss = 1.7968\n",
            "step 74170: train loss = 0.6269, val loss = 1.2957\n",
            "step 74180: train loss = 1.0173, val loss = 1.5281\n",
            "step 74190: train loss = 1.1030, val loss = 1.2516\n",
            "step 74200: train loss = 0.9526, val loss = 1.8832\n",
            "step 74210: train loss = 0.9586, val loss = 1.7362\n",
            "step 74220: train loss = 0.8575, val loss = 1.7987\n",
            "step 74230: train loss = 1.0573, val loss = 1.6751\n",
            "step 74240: train loss = 0.8125, val loss = 1.2580\n",
            "step 74250: train loss = 1.0825, val loss = 1.4500\n",
            "step 74260: train loss = 1.1249, val loss = 1.8018\n",
            "step 74270: train loss = 1.2904, val loss = 1.5698\n",
            "step 74280: train loss = 0.9827, val loss = 1.1657\n",
            "step 74290: train loss = 1.1154, val loss = 1.1779\n",
            "step 74300: train loss = 0.9108, val loss = 1.5843\n",
            "step 74310: train loss = 1.0135, val loss = 1.7718\n",
            "step 74320: train loss = 1.0878, val loss = 1.5427\n",
            "step 74330: train loss = 1.3868, val loss = 1.2813\n",
            "step 74340: train loss = 0.9400, val loss = 1.8052\n",
            "step 74350: train loss = 1.0720, val loss = 1.6885\n",
            "step 74360: train loss = 0.8175, val loss = 1.0138\n",
            "step 74370: train loss = 1.0010, val loss = 1.2039\n",
            "step 74380: train loss = 1.2330, val loss = 1.9104\n",
            "step 74390: train loss = 1.1523, val loss = 1.9954\n",
            "step 74400: train loss = 0.8708, val loss = 1.4695\n",
            "step 74410: train loss = 0.6848, val loss = 1.2361\n",
            "step 74420: train loss = 0.9912, val loss = 1.6847\n",
            "step 74430: train loss = 0.9989, val loss = 1.5209\n",
            "step 74440: train loss = 0.9381, val loss = 1.5206\n",
            "step 74450: train loss = 0.8574, val loss = 2.0228\n",
            "step 74460: train loss = 0.9217, val loss = 2.2442\n",
            "step 74470: train loss = 0.9846, val loss = 1.8029\n",
            "step 74480: train loss = 0.7592, val loss = 1.3051\n",
            "step 74490: train loss = 1.2128, val loss = 1.4511\n",
            "step 74500: train loss = 0.9471, val loss = 1.7797\n",
            "step 74510: train loss = 0.8096, val loss = 1.6638\n",
            "step 74520: train loss = 0.9356, val loss = 1.6884\n",
            "step 74530: train loss = 1.0857, val loss = 1.5858\n",
            "step 74540: train loss = 1.1272, val loss = 1.1705\n",
            "step 74550: train loss = 0.7703, val loss = 1.9178\n",
            "step 74560: train loss = 0.9893, val loss = 2.1767\n",
            "step 74570: train loss = 0.9383, val loss = 1.8074\n",
            "step 74580: train loss = 1.2059, val loss = 1.1917\n",
            "step 74590: train loss = 1.0428, val loss = 1.7675\n",
            "step 74600: train loss = 0.9759, val loss = 2.0040\n",
            "step 74610: train loss = 0.9428, val loss = 1.3738\n",
            "step 74620: train loss = 0.8459, val loss = 1.5829\n",
            "step 74630: train loss = 0.6665, val loss = 1.7716\n",
            "step 74640: train loss = 0.7440, val loss = 1.4675\n",
            "step 74650: train loss = 0.9680, val loss = 1.6315\n",
            "step 74660: train loss = 1.0773, val loss = 1.4191\n",
            "step 74670: train loss = 0.7583, val loss = 1.3119\n",
            "step 74680: train loss = 0.7049, val loss = 1.3920\n",
            "step 74690: train loss = 1.0858, val loss = 2.0282\n",
            "step 74700: train loss = 1.5060, val loss = 1.4830\n",
            "step 74710: train loss = 1.1051, val loss = 1.2277\n",
            "step 74720: train loss = 1.0935, val loss = 1.8911\n",
            "step 74730: train loss = 0.9944, val loss = 1.8437\n",
            "step 74740: train loss = 1.4514, val loss = 1.6549\n",
            "step 74750: train loss = 0.7192, val loss = 1.1369\n",
            "step 74760: train loss = 0.8452, val loss = 1.2697\n",
            "step 74770: train loss = 1.0235, val loss = 1.8028\n",
            "step 74780: train loss = 0.8310, val loss = 1.1579\n",
            "step 74790: train loss = 0.7854, val loss = 1.8876\n",
            "step 74800: train loss = 1.0117, val loss = 1.3171\n",
            "step 74810: train loss = 1.0514, val loss = 2.1725\n",
            "step 74820: train loss = 0.5763, val loss = 1.6207\n",
            "step 74830: train loss = 0.9003, val loss = 1.5840\n",
            "step 74840: train loss = 0.7247, val loss = 1.8446\n",
            "step 74850: train loss = 0.9581, val loss = 1.1379\n",
            "step 74860: train loss = 0.6200, val loss = 1.7064\n",
            "step 74870: train loss = 1.2841, val loss = 1.1392\n",
            "step 74880: train loss = 0.8891, val loss = 1.1898\n",
            "step 74890: train loss = 0.7239, val loss = 1.2516\n",
            "step 74900: train loss = 0.9171, val loss = 1.8548\n",
            "step 74910: train loss = 0.9571, val loss = 1.5485\n",
            "step 74920: train loss = 0.9894, val loss = 1.7448\n",
            "step 74930: train loss = 0.8175, val loss = 1.7914\n",
            "step 74940: train loss = 0.8632, val loss = 1.6164\n",
            "step 74950: train loss = 0.9945, val loss = 1.7277\n",
            "step 74960: train loss = 1.0374, val loss = 1.4347\n",
            "step 74970: train loss = 0.9397, val loss = 1.6498\n",
            "step 74980: train loss = 1.1533, val loss = 1.6805\n",
            "step 74990: train loss = 1.2289, val loss = 1.1586\n",
            "step 75000: train loss = 0.8905, val loss = 1.9560\n",
            "step 75010: train loss = 0.9006, val loss = 0.8424\n",
            "step 75020: train loss = 1.1265, val loss = 1.2553\n",
            "step 75030: train loss = 0.8454, val loss = 1.3973\n",
            "step 75040: train loss = 1.1661, val loss = 2.0061\n",
            "step 75050: train loss = 1.1355, val loss = 1.6974\n",
            "step 75060: train loss = 0.6419, val loss = 1.7420\n",
            "step 75070: train loss = 1.0811, val loss = 1.7023\n",
            "step 75080: train loss = 1.2786, val loss = 1.6064\n",
            "step 75090: train loss = 1.4037, val loss = 1.1530\n",
            "step 75100: train loss = 1.1479, val loss = 1.3340\n",
            "step 75110: train loss = 0.7561, val loss = 1.3320\n",
            "step 75120: train loss = 0.4851, val loss = 1.5721\n",
            "step 75130: train loss = 0.8353, val loss = 0.9998\n",
            "step 75140: train loss = 0.8932, val loss = 1.6405\n",
            "step 75150: train loss = 1.2869, val loss = 1.6783\n",
            "step 75160: train loss = 1.1227, val loss = 1.3190\n",
            "step 75170: train loss = 1.2838, val loss = 1.7499\n",
            "step 75180: train loss = 0.8884, val loss = 1.4550\n",
            "step 75190: train loss = 1.0399, val loss = 1.1827\n",
            "step 75200: train loss = 0.8008, val loss = 1.2805\n",
            "step 75210: train loss = 0.6305, val loss = 1.6567\n",
            "step 75220: train loss = 1.1413, val loss = 1.4498\n",
            "step 75230: train loss = 1.2323, val loss = 1.8574\n",
            "step 75240: train loss = 1.0462, val loss = 1.4200\n",
            "step 75250: train loss = 1.1981, val loss = 1.1980\n",
            "step 75260: train loss = 0.9322, val loss = 1.5003\n",
            "step 75270: train loss = 1.0577, val loss = 1.7815\n",
            "step 75280: train loss = 1.2595, val loss = 2.1258\n",
            "step 75290: train loss = 0.8419, val loss = 1.0799\n",
            "step 75300: train loss = 1.3203, val loss = 1.6862\n",
            "step 75310: train loss = 0.7890, val loss = 1.8453\n",
            "step 75320: train loss = 1.1654, val loss = 1.4361\n",
            "step 75330: train loss = 1.1456, val loss = 1.1098\n",
            "step 75340: train loss = 1.0026, val loss = 1.4358\n",
            "step 75350: train loss = 1.3854, val loss = 1.1662\n",
            "step 75360: train loss = 1.1866, val loss = 1.5280\n",
            "step 75370: train loss = 0.8220, val loss = 1.5320\n",
            "step 75380: train loss = 0.9489, val loss = 1.7162\n",
            "step 75390: train loss = 0.7171, val loss = 1.3676\n",
            "step 75400: train loss = 0.7559, val loss = 2.0344\n",
            "step 75410: train loss = 0.7729, val loss = 1.7876\n",
            "step 75420: train loss = 0.7375, val loss = 1.6983\n",
            "step 75430: train loss = 1.1711, val loss = 1.9142\n",
            "step 75440: train loss = 0.5296, val loss = 1.7456\n",
            "step 75450: train loss = 0.9471, val loss = 1.3639\n",
            "step 75460: train loss = 0.7736, val loss = 1.6501\n",
            "step 75470: train loss = 0.8177, val loss = 1.8033\n",
            "step 75480: train loss = 0.8487, val loss = 1.4300\n",
            "step 75490: train loss = 0.8556, val loss = 2.1878\n",
            "step 75500: train loss = 0.8612, val loss = 1.6235\n",
            "step 75510: train loss = 1.0021, val loss = 2.1245\n",
            "step 75520: train loss = 0.8613, val loss = 1.8943\n",
            "step 75530: train loss = 1.0878, val loss = 1.6149\n",
            "step 75540: train loss = 1.0799, val loss = 1.5379\n",
            "step 75550: train loss = 1.0336, val loss = 0.9665\n",
            "step 75560: train loss = 1.3279, val loss = 1.7340\n",
            "step 75570: train loss = 0.8797, val loss = 1.8860\n",
            "step 75580: train loss = 0.7127, val loss = 1.8389\n",
            "step 75590: train loss = 0.8776, val loss = 1.5845\n",
            "step 75600: train loss = 0.8321, val loss = 1.4482\n",
            "step 75610: train loss = 1.3295, val loss = 1.4666\n",
            "step 75620: train loss = 0.9151, val loss = 1.5958\n",
            "step 75630: train loss = 0.8329, val loss = 1.3087\n",
            "step 75640: train loss = 0.8941, val loss = 1.6316\n",
            "step 75650: train loss = 1.1259, val loss = 1.5766\n",
            "step 75660: train loss = 1.4698, val loss = 1.7980\n",
            "step 75670: train loss = 0.9951, val loss = 1.1432\n",
            "step 75680: train loss = 0.7452, val loss = 1.3269\n",
            "step 75690: train loss = 0.7822, val loss = 1.9715\n",
            "step 75700: train loss = 1.0222, val loss = 1.4263\n",
            "step 75710: train loss = 0.7543, val loss = 1.7555\n",
            "step 75720: train loss = 0.9044, val loss = 1.3714\n",
            "step 75730: train loss = 1.2000, val loss = 1.3508\n",
            "step 75740: train loss = 1.2339, val loss = 1.6481\n",
            "step 75750: train loss = 1.1245, val loss = 1.3315\n",
            "step 75760: train loss = 1.1250, val loss = 1.6480\n",
            "step 75770: train loss = 0.8524, val loss = 1.6231\n",
            "step 75780: train loss = 1.1413, val loss = 1.7378\n",
            "step 75790: train loss = 0.8175, val loss = 1.7473\n",
            "step 75800: train loss = 0.7273, val loss = 1.2495\n",
            "step 75810: train loss = 1.0328, val loss = 1.8373\n",
            "step 75820: train loss = 0.9901, val loss = 1.7034\n",
            "step 75830: train loss = 1.1372, val loss = 1.8745\n",
            "step 75840: train loss = 1.1394, val loss = 1.3757\n",
            "step 75850: train loss = 0.9848, val loss = 1.5317\n",
            "step 75860: train loss = 1.1685, val loss = 2.0689\n",
            "step 75870: train loss = 0.7381, val loss = 2.0617\n",
            "step 75880: train loss = 1.0027, val loss = 1.6741\n",
            "step 75890: train loss = 1.0308, val loss = 2.0655\n",
            "step 75900: train loss = 0.9703, val loss = 1.3490\n",
            "step 75910: train loss = 0.9783, val loss = 1.3647\n",
            "step 75920: train loss = 1.0532, val loss = 1.6967\n",
            "step 75930: train loss = 0.6565, val loss = 1.7415\n",
            "step 75940: train loss = 0.9141, val loss = 1.0181\n",
            "step 75950: train loss = 0.8641, val loss = 1.5173\n",
            "step 75960: train loss = 0.9901, val loss = 1.7437\n",
            "step 75970: train loss = 0.9433, val loss = 1.6045\n",
            "step 75980: train loss = 0.9834, val loss = 1.7550\n",
            "step 75990: train loss = 0.9792, val loss = 0.9165\n",
            "step 76000: train loss = 0.7112, val loss = 1.6719\n",
            "step 76010: train loss = 1.3410, val loss = 1.5999\n",
            "step 76020: train loss = 0.6015, val loss = 1.4853\n",
            "step 76030: train loss = 1.5517, val loss = 1.4696\n",
            "step 76040: train loss = 0.8973, val loss = 1.5254\n",
            "step 76050: train loss = 0.9826, val loss = 1.8010\n",
            "step 76060: train loss = 1.2152, val loss = 2.4223\n",
            "step 76070: train loss = 0.7811, val loss = 1.9787\n",
            "step 76080: train loss = 1.0767, val loss = 1.7374\n",
            "step 76090: train loss = 1.0963, val loss = 1.4461\n",
            "step 76100: train loss = 0.9920, val loss = 1.2790\n",
            "step 76110: train loss = 0.8372, val loss = 1.6837\n",
            "step 76120: train loss = 0.7635, val loss = 1.7185\n",
            "step 76130: train loss = 1.0622, val loss = 1.3554\n",
            "step 76140: train loss = 1.0243, val loss = 1.6940\n",
            "step 76150: train loss = 0.7791, val loss = 1.5126\n",
            "step 76160: train loss = 0.9091, val loss = 1.8053\n",
            "step 76170: train loss = 1.6212, val loss = 1.1263\n",
            "step 76180: train loss = 0.7947, val loss = 1.0702\n",
            "step 76190: train loss = 1.1582, val loss = 1.4963\n",
            "step 76200: train loss = 0.9355, val loss = 1.7257\n",
            "step 76210: train loss = 0.8411, val loss = 1.2656\n",
            "step 76220: train loss = 0.6826, val loss = 2.1884\n",
            "step 76230: train loss = 0.8154, val loss = 1.5233\n",
            "step 76240: train loss = 1.0995, val loss = 1.5433\n",
            "step 76250: train loss = 0.8372, val loss = 1.4310\n",
            "step 76260: train loss = 0.9188, val loss = 1.3168\n",
            "step 76270: train loss = 1.0775, val loss = 1.6933\n",
            "step 76280: train loss = 0.6711, val loss = 1.6039\n",
            "step 76290: train loss = 1.0871, val loss = 1.7243\n",
            "step 76300: train loss = 0.6929, val loss = 1.6276\n",
            "step 76310: train loss = 0.6520, val loss = 1.5445\n",
            "step 76320: train loss = 1.0374, val loss = 1.5420\n",
            "step 76330: train loss = 1.0161, val loss = 2.1007\n",
            "step 76340: train loss = 0.7918, val loss = 1.5185\n",
            "step 76350: train loss = 1.0293, val loss = 1.4908\n",
            "step 76360: train loss = 1.1742, val loss = 1.9053\n",
            "step 76370: train loss = 0.7141, val loss = 1.7400\n",
            "step 76380: train loss = 1.0440, val loss = 1.1861\n",
            "step 76390: train loss = 1.0396, val loss = 1.2653\n",
            "step 76400: train loss = 0.7106, val loss = 1.8854\n",
            "step 76410: train loss = 0.8488, val loss = 1.6561\n",
            "step 76420: train loss = 0.8252, val loss = 1.6969\n",
            "step 76430: train loss = 0.8049, val loss = 0.7382\n",
            "step 76440: train loss = 1.3404, val loss = 1.9781\n",
            "step 76450: train loss = 1.1012, val loss = 1.3602\n",
            "step 76460: train loss = 1.1786, val loss = 1.3035\n",
            "step 76470: train loss = 0.8757, val loss = 1.8934\n",
            "step 76480: train loss = 0.7655, val loss = 1.6776\n",
            "step 76490: train loss = 1.0682, val loss = 1.6165\n",
            "step 76500: train loss = 0.8763, val loss = 1.7182\n",
            "step 76510: train loss = 1.1220, val loss = 1.4751\n",
            "step 76520: train loss = 0.7146, val loss = 1.3290\n",
            "step 76530: train loss = 0.9216, val loss = 1.7989\n",
            "step 76540: train loss = 0.9568, val loss = 1.5564\n",
            "step 76550: train loss = 1.0330, val loss = 1.3780\n",
            "step 76560: train loss = 1.1189, val loss = 1.9942\n",
            "step 76570: train loss = 0.7794, val loss = 1.5831\n",
            "step 76580: train loss = 1.2469, val loss = 1.6532\n",
            "step 76590: train loss = 0.8428, val loss = 1.9528\n",
            "step 76600: train loss = 0.7751, val loss = 2.2326\n",
            "step 76610: train loss = 0.8133, val loss = 1.5792\n",
            "step 76620: train loss = 0.9038, val loss = 1.6580\n",
            "step 76630: train loss = 1.3430, val loss = 1.3553\n",
            "step 76640: train loss = 0.9366, val loss = 1.5060\n",
            "step 76650: train loss = 0.8429, val loss = 1.2734\n",
            "step 76660: train loss = 0.9439, val loss = 2.2185\n",
            "step 76670: train loss = 0.8796, val loss = 1.1571\n",
            "step 76680: train loss = 0.7674, val loss = 1.3904\n",
            "step 76690: train loss = 1.1744, val loss = 1.8377\n",
            "step 76700: train loss = 1.0680, val loss = 1.5991\n",
            "step 76710: train loss = 0.5950, val loss = 1.6239\n",
            "step 76720: train loss = 1.1188, val loss = 2.0296\n",
            "step 76730: train loss = 0.8498, val loss = 1.6489\n",
            "step 76740: train loss = 0.6935, val loss = 1.3907\n",
            "step 76750: train loss = 0.5542, val loss = 1.3944\n",
            "step 76760: train loss = 1.1112, val loss = 1.8758\n",
            "step 76770: train loss = 1.0940, val loss = 2.0273\n",
            "step 76780: train loss = 0.8229, val loss = 1.6686\n",
            "step 76790: train loss = 0.4306, val loss = 1.5322\n",
            "step 76800: train loss = 0.5475, val loss = 1.3534\n",
            "step 76810: train loss = 0.8830, val loss = 1.2123\n",
            "step 76820: train loss = 0.7096, val loss = 1.3814\n",
            "step 76830: train loss = 1.0506, val loss = 1.7128\n",
            "step 76840: train loss = 0.8118, val loss = 1.3275\n",
            "step 76850: train loss = 1.1556, val loss = 1.7151\n",
            "step 76860: train loss = 0.6358, val loss = 1.3223\n",
            "step 76870: train loss = 0.8069, val loss = 1.1770\n",
            "step 76880: train loss = 0.7198, val loss = 1.1138\n",
            "step 76890: train loss = 0.4968, val loss = 1.6376\n",
            "step 76900: train loss = 0.6047, val loss = 1.6966\n",
            "step 76910: train loss = 1.0251, val loss = 1.5764\n",
            "step 76920: train loss = 0.7086, val loss = 1.7781\n",
            "step 76930: train loss = 0.8492, val loss = 1.6267\n",
            "step 76940: train loss = 1.1759, val loss = 1.8654\n",
            "step 76950: train loss = 0.7464, val loss = 1.8072\n",
            "step 76960: train loss = 1.0668, val loss = 1.5142\n",
            "step 76970: train loss = 0.8652, val loss = 1.5959\n",
            "step 76980: train loss = 0.8765, val loss = 1.4742\n",
            "step 76990: train loss = 1.2924, val loss = 1.4148\n",
            "step 77000: train loss = 0.6664, val loss = 1.5559\n",
            "step 77010: train loss = 1.0243, val loss = 1.4862\n",
            "step 77020: train loss = 1.1346, val loss = 1.3644\n",
            "step 77030: train loss = 1.0761, val loss = 1.9280\n",
            "step 77040: train loss = 0.8426, val loss = 2.1589\n",
            "step 77050: train loss = 1.1029, val loss = 1.6446\n",
            "step 77060: train loss = 0.6778, val loss = 1.0882\n",
            "step 77070: train loss = 0.8470, val loss = 1.4212\n",
            "step 77080: train loss = 0.6835, val loss = 1.6802\n",
            "step 77090: train loss = 0.9239, val loss = 2.5254\n",
            "step 77100: train loss = 0.8445, val loss = 1.4351\n",
            "step 77110: train loss = 0.6106, val loss = 1.2200\n",
            "step 77120: train loss = 0.7257, val loss = 1.8596\n",
            "step 77130: train loss = 0.7977, val loss = 1.3234\n",
            "step 77140: train loss = 0.7950, val loss = 1.0990\n",
            "step 77150: train loss = 0.9214, val loss = 1.4811\n",
            "step 77160: train loss = 0.9837, val loss = 1.9330\n",
            "step 77170: train loss = 0.7793, val loss = 1.2838\n",
            "step 77180: train loss = 0.9468, val loss = 2.0493\n",
            "step 77190: train loss = 0.8086, val loss = 1.1990\n",
            "step 77200: train loss = 1.2098, val loss = 1.0762\n",
            "step 77210: train loss = 1.1221, val loss = 1.3526\n",
            "step 77220: train loss = 0.7855, val loss = 1.2197\n",
            "step 77230: train loss = 1.2106, val loss = 1.3521\n",
            "step 77240: train loss = 0.6421, val loss = 1.8009\n",
            "step 77250: train loss = 0.7470, val loss = 2.3179\n",
            "step 77260: train loss = 0.8215, val loss = 1.3910\n",
            "step 77270: train loss = 0.9343, val loss = 2.1582\n",
            "step 77280: train loss = 0.6827, val loss = 2.1682\n",
            "step 77290: train loss = 0.8745, val loss = 1.1008\n",
            "step 77300: train loss = 0.6638, val loss = 1.4390\n",
            "step 77310: train loss = 1.1135, val loss = 1.3546\n",
            "step 77320: train loss = 1.0207, val loss = 1.6810\n",
            "step 77330: train loss = 1.2230, val loss = 2.1306\n",
            "step 77340: train loss = 0.7918, val loss = 2.0075\n",
            "step 77350: train loss = 1.2745, val loss = 1.5840\n",
            "step 77360: train loss = 0.6397, val loss = 2.1221\n",
            "step 77370: train loss = 1.0275, val loss = 1.8770\n",
            "step 77380: train loss = 0.9083, val loss = 1.8519\n",
            "step 77390: train loss = 0.9811, val loss = 1.3805\n",
            "step 77400: train loss = 0.8924, val loss = 1.6738\n",
            "step 77410: train loss = 0.7937, val loss = 1.5408\n",
            "step 77420: train loss = 0.8396, val loss = 1.6727\n",
            "step 77430: train loss = 0.6624, val loss = 1.3274\n",
            "step 77440: train loss = 0.8642, val loss = 1.8113\n",
            "step 77450: train loss = 1.1226, val loss = 1.4164\n",
            "step 77460: train loss = 1.2081, val loss = 1.4993\n",
            "step 77470: train loss = 1.5053, val loss = 2.0572\n",
            "step 77480: train loss = 0.8742, val loss = 1.8835\n",
            "step 77490: train loss = 0.9794, val loss = 1.7379\n",
            "step 77500: train loss = 0.5535, val loss = 1.6143\n",
            "step 77510: train loss = 0.8785, val loss = 1.7050\n",
            "step 77520: train loss = 0.7304, val loss = 1.2034\n",
            "step 77530: train loss = 0.9775, val loss = 1.6655\n",
            "step 77540: train loss = 0.9279, val loss = 1.3613\n",
            "step 77550: train loss = 0.6799, val loss = 1.7275\n",
            "step 77560: train loss = 0.8904, val loss = 1.0804\n",
            "step 77570: train loss = 1.0363, val loss = 1.3712\n",
            "step 77580: train loss = 0.7229, val loss = 1.8666\n",
            "step 77590: train loss = 1.1783, val loss = 0.9752\n",
            "step 77600: train loss = 1.4383, val loss = 1.1339\n",
            "step 77610: train loss = 1.1616, val loss = 1.5667\n",
            "step 77620: train loss = 1.1766, val loss = 1.2807\n",
            "step 77630: train loss = 0.9029, val loss = 1.6621\n",
            "step 77640: train loss = 0.8172, val loss = 1.7100\n",
            "step 77650: train loss = 0.8528, val loss = 1.6990\n",
            "step 77660: train loss = 0.7842, val loss = 1.6428\n",
            "step 77670: train loss = 0.7476, val loss = 1.4722\n",
            "step 77680: train loss = 1.3267, val loss = 1.9083\n",
            "step 77690: train loss = 0.7208, val loss = 1.6512\n",
            "step 77700: train loss = 0.4072, val loss = 2.0834\n",
            "step 77710: train loss = 0.7641, val loss = 1.1862\n",
            "step 77720: train loss = 1.0354, val loss = 1.6396\n",
            "step 77730: train loss = 0.9334, val loss = 1.5394\n",
            "step 77740: train loss = 0.9217, val loss = 1.3290\n",
            "step 77750: train loss = 1.0714, val loss = 1.3487\n",
            "step 77760: train loss = 0.7250, val loss = 1.6927\n",
            "step 77770: train loss = 1.2016, val loss = 1.3553\n",
            "step 77780: train loss = 0.5794, val loss = 1.1332\n",
            "step 77790: train loss = 0.9221, val loss = 1.7519\n",
            "step 77800: train loss = 0.9940, val loss = 1.6472\n",
            "step 77810: train loss = 1.2424, val loss = 1.6345\n",
            "step 77820: train loss = 0.8390, val loss = 1.9503\n",
            "step 77830: train loss = 1.0496, val loss = 1.7444\n",
            "step 77840: train loss = 0.7099, val loss = 1.6862\n",
            "step 77850: train loss = 1.0018, val loss = 1.4092\n",
            "step 77860: train loss = 1.0427, val loss = 1.2435\n",
            "step 77870: train loss = 0.6570, val loss = 1.1077\n",
            "step 77880: train loss = 0.8566, val loss = 1.4492\n",
            "step 77890: train loss = 0.8597, val loss = 1.9082\n",
            "step 77900: train loss = 0.8327, val loss = 1.7327\n",
            "step 77910: train loss = 1.2976, val loss = 1.0669\n",
            "step 77920: train loss = 0.8671, val loss = 1.2277\n",
            "step 77930: train loss = 0.8203, val loss = 1.7538\n",
            "step 77940: train loss = 0.6179, val loss = 1.8216\n",
            "step 77950: train loss = 0.8672, val loss = 1.0838\n",
            "step 77960: train loss = 1.2039, val loss = 2.1189\n",
            "step 77970: train loss = 1.0506, val loss = 1.8570\n",
            "step 77980: train loss = 0.7438, val loss = 2.1096\n",
            "step 77990: train loss = 0.6974, val loss = 1.5710\n",
            "step 78000: train loss = 0.8831, val loss = 1.7614\n",
            "step 78010: train loss = 0.8337, val loss = 1.5057\n",
            "step 78020: train loss = 0.8245, val loss = 1.2177\n",
            "step 78030: train loss = 0.8215, val loss = 2.1113\n",
            "step 78040: train loss = 0.8491, val loss = 1.9705\n",
            "step 78050: train loss = 1.0665, val loss = 1.4831\n",
            "step 78060: train loss = 0.9959, val loss = 1.1715\n",
            "step 78070: train loss = 0.8026, val loss = 1.3734\n",
            "step 78080: train loss = 0.4859, val loss = 1.5082\n",
            "step 78090: train loss = 0.9421, val loss = 1.3146\n",
            "step 78100: train loss = 0.9841, val loss = 1.2079\n",
            "step 78110: train loss = 1.1306, val loss = 2.1500\n",
            "step 78120: train loss = 1.2179, val loss = 1.4445\n",
            "step 78130: train loss = 1.3287, val loss = 1.9277\n",
            "step 78140: train loss = 0.9696, val loss = 2.1672\n",
            "step 78150: train loss = 0.8669, val loss = 1.2576\n",
            "step 78160: train loss = 1.1577, val loss = 1.1896\n",
            "step 78170: train loss = 1.4101, val loss = 0.9626\n",
            "step 78180: train loss = 0.8298, val loss = 2.3858\n",
            "step 78190: train loss = 0.6967, val loss = 1.8900\n",
            "step 78200: train loss = 0.7208, val loss = 1.2208\n",
            "step 78210: train loss = 0.9786, val loss = 2.2997\n",
            "step 78220: train loss = 0.7226, val loss = 1.1373\n",
            "step 78230: train loss = 1.0194, val loss = 1.4726\n",
            "step 78240: train loss = 1.1855, val loss = 1.9708\n",
            "step 78250: train loss = 0.9448, val loss = 1.7282\n",
            "step 78260: train loss = 0.6720, val loss = 1.6288\n",
            "step 78270: train loss = 0.6526, val loss = 1.5579\n",
            "step 78280: train loss = 0.7724, val loss = 1.3178\n",
            "step 78290: train loss = 0.6417, val loss = 1.0435\n",
            "step 78300: train loss = 0.7233, val loss = 2.1001\n",
            "step 78310: train loss = 0.9554, val loss = 1.7543\n",
            "step 78320: train loss = 0.9548, val loss = 1.9613\n",
            "step 78330: train loss = 1.0368, val loss = 1.3520\n",
            "step 78340: train loss = 0.7163, val loss = 1.2310\n",
            "step 78350: train loss = 0.9852, val loss = 1.4460\n",
            "step 78360: train loss = 0.9178, val loss = 1.5487\n",
            "step 78370: train loss = 0.9184, val loss = 1.4013\n",
            "step 78380: train loss = 0.7385, val loss = 1.3229\n",
            "step 78390: train loss = 0.9935, val loss = 1.4077\n",
            "step 78400: train loss = 0.8958, val loss = 1.7171\n",
            "step 78410: train loss = 1.0092, val loss = 1.2345\n",
            "step 78420: train loss = 0.8395, val loss = 1.0775\n",
            "step 78430: train loss = 0.7291, val loss = 1.1230\n",
            "step 78440: train loss = 1.3088, val loss = 1.6194\n",
            "step 78450: train loss = 0.8061, val loss = 2.2540\n",
            "step 78460: train loss = 0.8335, val loss = 1.9154\n",
            "step 78470: train loss = 0.9014, val loss = 1.0473\n",
            "step 78480: train loss = 0.6834, val loss = 1.6318\n",
            "step 78490: train loss = 1.0438, val loss = 1.4020\n",
            "step 78500: train loss = 0.4662, val loss = 1.8714\n",
            "step 78510: train loss = 1.2430, val loss = 1.2776\n",
            "step 78520: train loss = 0.9488, val loss = 1.3934\n",
            "step 78530: train loss = 0.7968, val loss = 1.3747\n",
            "step 78540: train loss = 0.9741, val loss = 1.7434\n",
            "step 78550: train loss = 0.8484, val loss = 1.3516\n",
            "step 78560: train loss = 0.5724, val loss = 1.8512\n",
            "step 78570: train loss = 1.0822, val loss = 2.0255\n",
            "step 78580: train loss = 0.6412, val loss = 1.4553\n",
            "step 78590: train loss = 0.9189, val loss = 1.8674\n",
            "step 78600: train loss = 0.6761, val loss = 1.5790\n",
            "step 78610: train loss = 0.9029, val loss = 1.6461\n",
            "step 78620: train loss = 0.7331, val loss = 1.8618\n",
            "step 78630: train loss = 0.7807, val loss = 1.4642\n",
            "step 78640: train loss = 1.0936, val loss = 1.5816\n",
            "step 78650: train loss = 1.0208, val loss = 1.2963\n",
            "step 78660: train loss = 0.8738, val loss = 1.9035\n",
            "step 78670: train loss = 0.7308, val loss = 1.2226\n",
            "step 78680: train loss = 0.7796, val loss = 1.1911\n",
            "step 78690: train loss = 0.9793, val loss = 1.9046\n",
            "step 78700: train loss = 0.6836, val loss = 1.8416\n",
            "step 78710: train loss = 1.1063, val loss = 1.9776\n",
            "step 78720: train loss = 0.8182, val loss = 1.1856\n",
            "step 78730: train loss = 0.9629, val loss = 1.5748\n",
            "step 78740: train loss = 1.0917, val loss = 1.8217\n",
            "step 78750: train loss = 0.8185, val loss = 1.1086\n",
            "step 78760: train loss = 0.9433, val loss = 1.8667\n",
            "step 78770: train loss = 0.7627, val loss = 1.5733\n",
            "step 78780: train loss = 0.9578, val loss = 1.5576\n",
            "step 78790: train loss = 1.0167, val loss = 1.4172\n",
            "step 78800: train loss = 0.5792, val loss = 1.5024\n",
            "step 78810: train loss = 1.1566, val loss = 1.0414\n",
            "step 78820: train loss = 0.8977, val loss = 1.7062\n",
            "step 78830: train loss = 1.0072, val loss = 2.1514\n",
            "step 78840: train loss = 1.0246, val loss = 1.5297\n",
            "step 78850: train loss = 0.9473, val loss = 1.3614\n",
            "step 78860: train loss = 0.8288, val loss = 1.4175\n",
            "step 78870: train loss = 1.1605, val loss = 1.7863\n",
            "step 78880: train loss = 0.8763, val loss = 2.1918\n",
            "step 78890: train loss = 0.7703, val loss = 1.9318\n",
            "step 78900: train loss = 0.7921, val loss = 1.7240\n",
            "step 78910: train loss = 0.9698, val loss = 1.9146\n",
            "step 78920: train loss = 1.0204, val loss = 1.1744\n",
            "step 78930: train loss = 0.9991, val loss = 1.6955\n",
            "step 78940: train loss = 0.6373, val loss = 1.4627\n",
            "step 78950: train loss = 1.1184, val loss = 2.3378\n",
            "step 78960: train loss = 1.2395, val loss = 1.4790\n",
            "step 78970: train loss = 1.3267, val loss = 1.9753\n",
            "step 78980: train loss = 1.1641, val loss = 1.9967\n",
            "step 78990: train loss = 0.9405, val loss = 1.4416\n",
            "step 79000: train loss = 0.7901, val loss = 1.7570\n",
            "step 79010: train loss = 0.7782, val loss = 1.2304\n",
            "step 79020: train loss = 0.8511, val loss = 1.6256\n",
            "step 79030: train loss = 0.5580, val loss = 1.6919\n",
            "step 79040: train loss = 0.7315, val loss = 1.6400\n",
            "step 79050: train loss = 1.1198, val loss = 1.5988\n",
            "step 79060: train loss = 0.9152, val loss = 1.3407\n",
            "step 79070: train loss = 0.8764, val loss = 1.8225\n",
            "step 79080: train loss = 0.8562, val loss = 1.5185\n",
            "step 79090: train loss = 1.1631, val loss = 1.9875\n",
            "step 79100: train loss = 0.9213, val loss = 1.4182\n",
            "step 79110: train loss = 1.0800, val loss = 1.2206\n",
            "step 79120: train loss = 1.0930, val loss = 2.7821\n",
            "step 79130: train loss = 1.4062, val loss = 1.6712\n",
            "step 79140: train loss = 1.0514, val loss = 1.1297\n",
            "step 79150: train loss = 0.6478, val loss = 1.6247\n",
            "step 79160: train loss = 0.9885, val loss = 1.6986\n",
            "step 79170: train loss = 1.1188, val loss = 1.6134\n",
            "step 79180: train loss = 0.6963, val loss = 1.6625\n",
            "step 79190: train loss = 0.7542, val loss = 2.0183\n",
            "step 79200: train loss = 1.0064, val loss = 1.3782\n",
            "step 79210: train loss = 0.8212, val loss = 1.3112\n",
            "step 79220: train loss = 0.4701, val loss = 1.6667\n",
            "step 79230: train loss = 0.9665, val loss = 1.8333\n",
            "step 79240: train loss = 0.9419, val loss = 2.2019\n",
            "step 79250: train loss = 0.8991, val loss = 2.2731\n",
            "step 79260: train loss = 0.7205, val loss = 0.9956\n",
            "step 79270: train loss = 0.5149, val loss = 1.6150\n",
            "step 79280: train loss = 0.7353, val loss = 1.5454\n",
            "step 79290: train loss = 0.9112, val loss = 1.9142\n",
            "step 79300: train loss = 0.7666, val loss = 2.0201\n",
            "step 79310: train loss = 1.1693, val loss = 1.6506\n",
            "step 79320: train loss = 1.1518, val loss = 1.5890\n",
            "step 79330: train loss = 1.0577, val loss = 1.8889\n",
            "step 79340: train loss = 0.9778, val loss = 2.1954\n",
            "step 79350: train loss = 0.7244, val loss = 1.6908\n",
            "step 79360: train loss = 1.0769, val loss = 1.5858\n",
            "step 79370: train loss = 1.2215, val loss = 1.3466\n",
            "step 79380: train loss = 0.9922, val loss = 1.3878\n",
            "step 79390: train loss = 0.8603, val loss = 1.3650\n",
            "step 79400: train loss = 0.9813, val loss = 1.6484\n",
            "step 79410: train loss = 0.7424, val loss = 1.7616\n",
            "step 79420: train loss = 0.9332, val loss = 2.2230\n",
            "step 79430: train loss = 0.7357, val loss = 1.1445\n",
            "step 79440: train loss = 1.1252, val loss = 1.3666\n",
            "step 79450: train loss = 0.5932, val loss = 1.7024\n",
            "step 79460: train loss = 0.8498, val loss = 1.4884\n",
            "step 79470: train loss = 0.6566, val loss = 1.8002\n",
            "step 79480: train loss = 0.6501, val loss = 2.2321\n",
            "step 79490: train loss = 0.6369, val loss = 1.4634\n",
            "step 79500: train loss = 0.8514, val loss = 1.2779\n",
            "step 79510: train loss = 1.0023, val loss = 1.5012\n",
            "step 79520: train loss = 1.1976, val loss = 1.9078\n",
            "step 79530: train loss = 1.2876, val loss = 2.1219\n",
            "step 79540: train loss = 0.9060, val loss = 1.1369\n",
            "step 79550: train loss = 1.2788, val loss = 1.1248\n",
            "step 79560: train loss = 0.8316, val loss = 1.2932\n",
            "step 79570: train loss = 0.6112, val loss = 1.2398\n",
            "step 79580: train loss = 0.7988, val loss = 1.2053\n",
            "step 79590: train loss = 0.6713, val loss = 1.7645\n",
            "step 79600: train loss = 0.8115, val loss = 1.6354\n",
            "step 79610: train loss = 1.1025, val loss = 1.2209\n",
            "step 79620: train loss = 0.9738, val loss = 1.5756\n",
            "step 79630: train loss = 1.2758, val loss = 1.7436\n",
            "step 79640: train loss = 0.7301, val loss = 1.3606\n",
            "step 79650: train loss = 0.9703, val loss = 1.4415\n",
            "step 79660: train loss = 1.1027, val loss = 1.1927\n",
            "step 79670: train loss = 0.6779, val loss = 1.8665\n",
            "step 79680: train loss = 1.0014, val loss = 1.7843\n",
            "step 79690: train loss = 1.0772, val loss = 1.5971\n",
            "step 79700: train loss = 0.8765, val loss = 1.2886\n",
            "step 79710: train loss = 1.0061, val loss = 1.4621\n",
            "step 79720: train loss = 0.8259, val loss = 1.7492\n",
            "step 79730: train loss = 0.6139, val loss = 1.6257\n",
            "step 79740: train loss = 1.0883, val loss = 1.4178\n",
            "step 79750: train loss = 0.8014, val loss = 1.6628\n",
            "step 79760: train loss = 0.6751, val loss = 1.9751\n",
            "step 79770: train loss = 0.9034, val loss = 2.2217\n",
            "step 79780: train loss = 0.8954, val loss = 1.8098\n",
            "step 79790: train loss = 1.1222, val loss = 1.2903\n",
            "step 79800: train loss = 0.9713, val loss = 1.7859\n",
            "step 79810: train loss = 1.0670, val loss = 1.6224\n",
            "step 79820: train loss = 0.6501, val loss = 1.3830\n",
            "step 79830: train loss = 0.5803, val loss = 1.2335\n",
            "step 79840: train loss = 0.8271, val loss = 1.2911\n",
            "step 79850: train loss = 0.6820, val loss = 1.7238\n",
            "step 79860: train loss = 1.3052, val loss = 1.3683\n",
            "step 79870: train loss = 0.7979, val loss = 2.1208\n",
            "step 79880: train loss = 1.0449, val loss = 1.6599\n",
            "step 79890: train loss = 1.0656, val loss = 1.9847\n",
            "step 79900: train loss = 0.9699, val loss = 2.1690\n",
            "step 79910: train loss = 0.6567, val loss = 1.4178\n",
            "step 79920: train loss = 0.8986, val loss = 2.0362\n",
            "step 79930: train loss = 0.9540, val loss = 1.3789\n",
            "step 79940: train loss = 0.6093, val loss = 2.2610\n",
            "step 79950: train loss = 0.9345, val loss = 1.5740\n",
            "step 79960: train loss = 0.7558, val loss = 1.4787\n",
            "step 79970: train loss = 0.7672, val loss = 1.4083\n",
            "step 79980: train loss = 0.8597, val loss = 1.2875\n",
            "step 79990: train loss = 0.7299, val loss = 1.5348\n",
            "step 80000: train loss = 0.5287, val loss = 1.5935\n",
            "step 80010: train loss = 0.8403, val loss = 1.2199\n",
            "step 80020: train loss = 0.7136, val loss = 1.5122\n",
            "step 80030: train loss = 0.7418, val loss = 1.5213\n",
            "step 80040: train loss = 0.9828, val loss = 1.7053\n",
            "step 80050: train loss = 0.6310, val loss = 1.6128\n",
            "step 80060: train loss = 1.2904, val loss = 1.3177\n",
            "step 80070: train loss = 1.0361, val loss = 1.0426\n",
            "step 80080: train loss = 0.7784, val loss = 1.8800\n",
            "step 80090: train loss = 0.7362, val loss = 1.6788\n",
            "step 80100: train loss = 1.0543, val loss = 1.4606\n",
            "step 80110: train loss = 0.7408, val loss = 1.5091\n",
            "step 80120: train loss = 0.7616, val loss = 1.6208\n",
            "step 80130: train loss = 0.7094, val loss = 1.2368\n",
            "step 80140: train loss = 0.9284, val loss = 1.6520\n",
            "step 80150: train loss = 1.1329, val loss = 1.7835\n",
            "step 80160: train loss = 0.5762, val loss = 1.4104\n",
            "step 80170: train loss = 0.9172, val loss = 1.4052\n",
            "step 80180: train loss = 1.0812, val loss = 1.7407\n",
            "step 80190: train loss = 0.7883, val loss = 1.6817\n",
            "step 80200: train loss = 0.7681, val loss = 1.3454\n",
            "step 80210: train loss = 0.5968, val loss = 1.3387\n",
            "step 80220: train loss = 1.0921, val loss = 1.6941\n",
            "step 80230: train loss = 0.7801, val loss = 1.5600\n",
            "step 80240: train loss = 0.6923, val loss = 2.5759\n",
            "step 80250: train loss = 0.8401, val loss = 1.7700\n",
            "step 80260: train loss = 0.8344, val loss = 2.1334\n",
            "step 80270: train loss = 0.9247, val loss = 1.4519\n",
            "step 80280: train loss = 0.8751, val loss = 1.3527\n",
            "step 80290: train loss = 0.9090, val loss = 1.7557\n",
            "step 80300: train loss = 0.6947, val loss = 1.9731\n",
            "step 80310: train loss = 0.9746, val loss = 2.1823\n",
            "step 80320: train loss = 0.9269, val loss = 1.6538\n",
            "step 80330: train loss = 0.9327, val loss = 1.3510\n",
            "step 80340: train loss = 0.6151, val loss = 1.7158\n",
            "step 80350: train loss = 0.7901, val loss = 1.6897\n",
            "step 80360: train loss = 0.9359, val loss = 1.8234\n",
            "step 80370: train loss = 0.7352, val loss = 1.0824\n",
            "step 80380: train loss = 0.8026, val loss = 1.4619\n",
            "step 80390: train loss = 0.7620, val loss = 1.8208\n",
            "step 80400: train loss = 0.8276, val loss = 1.4950\n",
            "step 80410: train loss = 1.0444, val loss = 1.4080\n",
            "step 80420: train loss = 1.1108, val loss = 1.6506\n",
            "step 80430: train loss = 0.7711, val loss = 1.3215\n",
            "step 80440: train loss = 0.6854, val loss = 1.7131\n",
            "step 80450: train loss = 0.7139, val loss = 2.2397\n",
            "step 80460: train loss = 0.9720, val loss = 1.7171\n",
            "step 80470: train loss = 1.1479, val loss = 1.8656\n",
            "step 80480: train loss = 0.9845, val loss = 1.4690\n",
            "step 80490: train loss = 0.8079, val loss = 1.4498\n",
            "step 80500: train loss = 0.9926, val loss = 1.9319\n",
            "step 80510: train loss = 1.0186, val loss = 1.5668\n",
            "step 80520: train loss = 0.8602, val loss = 2.2186\n",
            "step 80530: train loss = 1.2694, val loss = 1.9341\n",
            "step 80540: train loss = 0.7549, val loss = 1.8567\n",
            "step 80550: train loss = 0.8343, val loss = 1.6634\n",
            "step 80560: train loss = 0.8502, val loss = 1.2785\n",
            "step 80570: train loss = 1.0535, val loss = 1.6347\n",
            "step 80580: train loss = 0.8752, val loss = 1.7747\n",
            "step 80590: train loss = 1.1153, val loss = 1.6418\n",
            "step 80600: train loss = 0.7332, val loss = 1.2330\n",
            "step 80610: train loss = 0.8421, val loss = 1.1220\n",
            "step 80620: train loss = 0.8112, val loss = 2.0093\n",
            "step 80630: train loss = 0.7607, val loss = 1.5101\n",
            "step 80640: train loss = 0.7620, val loss = 1.0244\n",
            "step 80650: train loss = 1.0057, val loss = 1.6566\n",
            "step 80660: train loss = 1.0206, val loss = 2.0335\n",
            "step 80670: train loss = 0.9069, val loss = 1.8488\n",
            "step 80680: train loss = 0.8839, val loss = 1.6773\n",
            "step 80690: train loss = 0.5380, val loss = 1.6048\n",
            "step 80700: train loss = 0.9997, val loss = 1.4565\n",
            "step 80710: train loss = 0.7426, val loss = 1.4361\n",
            "step 80720: train loss = 0.8852, val loss = 1.3692\n",
            "step 80730: train loss = 0.7508, val loss = 1.6780\n",
            "step 80740: train loss = 0.5208, val loss = 1.7109\n",
            "step 80750: train loss = 0.6714, val loss = 1.9795\n",
            "step 80760: train loss = 0.6770, val loss = 1.7245\n",
            "step 80770: train loss = 0.8591, val loss = 1.7356\n",
            "step 80780: train loss = 0.9443, val loss = 1.7511\n",
            "step 80790: train loss = 0.9689, val loss = 1.6724\n",
            "step 80800: train loss = 0.9661, val loss = 1.4811\n",
            "step 80810: train loss = 0.8990, val loss = 1.3203\n",
            "step 80820: train loss = 0.7670, val loss = 1.6165\n",
            "step 80830: train loss = 0.9983, val loss = 1.8188\n",
            "step 80840: train loss = 1.0497, val loss = 1.4836\n",
            "step 80850: train loss = 0.6956, val loss = 1.6104\n",
            "step 80860: train loss = 0.9483, val loss = 1.4546\n",
            "step 80870: train loss = 0.8355, val loss = 1.6613\n",
            "step 80880: train loss = 0.8556, val loss = 1.8260\n",
            "step 80890: train loss = 1.0708, val loss = 2.0524\n",
            "step 80900: train loss = 0.6731, val loss = 1.4636\n",
            "step 80910: train loss = 0.7163, val loss = 1.8873\n",
            "step 80920: train loss = 1.1203, val loss = 1.8338\n",
            "step 80930: train loss = 0.7345, val loss = 1.4729\n",
            "step 80940: train loss = 0.8525, val loss = 1.6624\n",
            "step 80950: train loss = 0.7762, val loss = 1.2337\n",
            "step 80960: train loss = 0.8442, val loss = 1.4028\n",
            "step 80970: train loss = 0.7691, val loss = 1.6341\n",
            "step 80980: train loss = 0.8474, val loss = 1.6426\n",
            "step 80990: train loss = 1.4418, val loss = 1.7195\n",
            "step 81000: train loss = 0.6423, val loss = 1.2877\n",
            "step 81010: train loss = 0.6766, val loss = 1.7284\n",
            "step 81020: train loss = 1.0092, val loss = 1.2469\n",
            "step 81030: train loss = 0.8653, val loss = 1.5606\n",
            "step 81040: train loss = 0.5725, val loss = 1.5255\n",
            "step 81050: train loss = 0.8569, val loss = 1.1450\n",
            "step 81060: train loss = 0.7142, val loss = 1.4103\n",
            "step 81070: train loss = 0.7034, val loss = 1.4358\n",
            "step 81080: train loss = 0.9508, val loss = 1.8580\n",
            "step 81090: train loss = 0.8644, val loss = 1.3381\n",
            "step 81100: train loss = 0.8247, val loss = 1.5245\n",
            "step 81110: train loss = 0.8336, val loss = 1.3567\n",
            "step 81120: train loss = 1.1532, val loss = 2.1534\n",
            "step 81130: train loss = 0.6804, val loss = 1.8246\n",
            "step 81140: train loss = 1.1434, val loss = 2.1193\n",
            "step 81150: train loss = 0.9127, val loss = 2.0714\n",
            "step 81160: train loss = 0.6558, val loss = 1.2642\n",
            "step 81170: train loss = 1.0309, val loss = 1.6470\n",
            "step 81180: train loss = 1.2296, val loss = 1.9043\n",
            "step 81190: train loss = 1.0795, val loss = 1.7087\n",
            "step 81200: train loss = 1.4178, val loss = 2.2119\n",
            "step 81210: train loss = 0.9445, val loss = 1.7977\n",
            "step 81220: train loss = 1.2670, val loss = 2.1768\n",
            "step 81230: train loss = 1.0874, val loss = 1.2738\n",
            "step 81240: train loss = 1.0599, val loss = 1.2779\n",
            "step 81250: train loss = 1.0190, val loss = 1.5797\n",
            "step 81260: train loss = 0.6955, val loss = 2.2305\n",
            "step 81270: train loss = 0.7543, val loss = 1.9134\n",
            "step 81280: train loss = 0.9671, val loss = 1.8657\n",
            "step 81290: train loss = 0.9625, val loss = 2.0581\n",
            "step 81300: train loss = 1.0628, val loss = 1.7397\n",
            "step 81310: train loss = 0.7652, val loss = 1.5913\n",
            "step 81320: train loss = 0.7807, val loss = 1.1819\n",
            "step 81330: train loss = 1.0474, val loss = 1.5150\n",
            "step 81340: train loss = 0.9943, val loss = 1.4410\n",
            "step 81350: train loss = 0.7424, val loss = 1.7391\n",
            "step 81360: train loss = 0.6482, val loss = 1.5396\n",
            "step 81370: train loss = 0.6910, val loss = 1.7502\n",
            "step 81380: train loss = 1.0498, val loss = 2.0084\n",
            "step 81390: train loss = 0.9094, val loss = 1.5903\n",
            "step 81400: train loss = 0.6531, val loss = 1.2947\n",
            "step 81410: train loss = 0.4545, val loss = 1.4620\n",
            "step 81420: train loss = 0.7486, val loss = 1.3429\n",
            "step 81430: train loss = 0.8444, val loss = 2.3719\n",
            "step 81440: train loss = 0.9224, val loss = 1.7810\n",
            "step 81450: train loss = 0.7757, val loss = 1.9217\n",
            "step 81460: train loss = 0.9518, val loss = 1.7237\n",
            "step 81470: train loss = 1.0401, val loss = 1.9350\n",
            "step 81480: train loss = 0.7782, val loss = 1.4717\n",
            "step 81490: train loss = 0.9807, val loss = 1.6866\n",
            "step 81500: train loss = 0.8557, val loss = 1.4858\n",
            "step 81510: train loss = 0.7992, val loss = 1.7867\n",
            "step 81520: train loss = 0.7602, val loss = 1.7475\n",
            "step 81530: train loss = 1.2721, val loss = 1.3356\n",
            "step 81540: train loss = 0.8021, val loss = 1.7746\n",
            "step 81550: train loss = 0.9595, val loss = 2.1261\n",
            "step 81560: train loss = 1.0769, val loss = 1.9647\n",
            "step 81570: train loss = 0.8684, val loss = 1.4875\n",
            "step 81580: train loss = 0.9950, val loss = 1.9196\n",
            "step 81590: train loss = 0.8888, val loss = 1.3298\n",
            "step 81600: train loss = 0.8313, val loss = 1.7077\n",
            "step 81610: train loss = 0.6053, val loss = 1.4052\n",
            "step 81620: train loss = 0.9148, val loss = 1.4338\n",
            "step 81630: train loss = 0.8473, val loss = 1.8792\n",
            "step 81640: train loss = 1.0192, val loss = 1.4450\n",
            "step 81650: train loss = 0.4941, val loss = 1.7353\n",
            "step 81660: train loss = 1.3264, val loss = 1.2181\n",
            "step 81670: train loss = 0.8929, val loss = 2.5602\n",
            "step 81680: train loss = 0.9396, val loss = 1.0827\n",
            "step 81690: train loss = 0.8852, val loss = 1.5662\n",
            "step 81700: train loss = 0.8037, val loss = 2.2589\n",
            "step 81710: train loss = 0.8815, val loss = 1.6893\n",
            "step 81720: train loss = 0.9340, val loss = 1.4066\n",
            "step 81730: train loss = 0.8193, val loss = 2.0415\n",
            "step 81740: train loss = 0.6147, val loss = 1.6581\n",
            "step 81750: train loss = 0.9039, val loss = 1.3926\n",
            "step 81760: train loss = 0.8728, val loss = 1.5588\n",
            "step 81770: train loss = 1.0272, val loss = 1.6890\n",
            "step 81780: train loss = 1.0026, val loss = 1.2904\n",
            "step 81790: train loss = 0.5769, val loss = 1.5937\n",
            "step 81800: train loss = 1.2319, val loss = 2.0666\n",
            "step 81810: train loss = 0.9757, val loss = 1.3277\n",
            "step 81820: train loss = 0.9361, val loss = 1.5801\n",
            "step 81830: train loss = 1.0996, val loss = 2.1854\n",
            "step 81840: train loss = 0.9700, val loss = 1.2458\n",
            "step 81850: train loss = 0.9862, val loss = 1.6472\n",
            "step 81860: train loss = 0.8147, val loss = 2.0001\n",
            "step 81870: train loss = 0.8405, val loss = 1.8052\n",
            "step 81880: train loss = 1.2516, val loss = 1.9892\n",
            "step 81890: train loss = 0.6177, val loss = 1.5385\n",
            "step 81900: train loss = 0.8226, val loss = 1.4667\n",
            "step 81910: train loss = 0.9078, val loss = 1.9334\n",
            "step 81920: train loss = 0.7712, val loss = 1.2470\n",
            "step 81930: train loss = 0.6277, val loss = 1.8676\n",
            "step 81940: train loss = 0.7983, val loss = 1.4890\n",
            "step 81950: train loss = 0.8569, val loss = 1.5335\n",
            "step 81960: train loss = 0.9557, val loss = 1.6109\n",
            "step 81970: train loss = 0.7690, val loss = 1.1773\n",
            "step 81980: train loss = 0.9474, val loss = 1.4558\n",
            "step 81990: train loss = 0.6679, val loss = 1.5016\n",
            "step 82000: train loss = 0.7002, val loss = 1.2344\n",
            "step 82010: train loss = 0.9059, val loss = 1.6192\n",
            "step 82020: train loss = 0.9184, val loss = 1.5804\n",
            "step 82030: train loss = 0.9371, val loss = 1.7244\n",
            "step 82040: train loss = 0.7784, val loss = 1.9782\n",
            "step 82050: train loss = 0.8396, val loss = 1.9807\n",
            "step 82060: train loss = 1.1837, val loss = 1.5053\n",
            "step 82070: train loss = 0.7849, val loss = 1.4339\n",
            "step 82080: train loss = 0.6635, val loss = 1.5343\n",
            "step 82090: train loss = 0.7102, val loss = 1.2879\n",
            "step 82100: train loss = 0.8927, val loss = 1.6658\n",
            "step 82110: train loss = 1.0817, val loss = 2.0572\n",
            "step 82120: train loss = 1.0722, val loss = 2.0477\n",
            "step 82130: train loss = 0.9361, val loss = 1.8418\n",
            "step 82140: train loss = 0.7898, val loss = 1.4914\n",
            "step 82150: train loss = 1.0466, val loss = 1.5844\n",
            "step 82160: train loss = 0.8043, val loss = 1.2733\n",
            "step 82170: train loss = 1.2056, val loss = 1.6893\n",
            "step 82180: train loss = 0.9742, val loss = 1.5786\n",
            "step 82190: train loss = 0.9252, val loss = 1.2787\n",
            "step 82200: train loss = 0.6780, val loss = 1.7136\n",
            "step 82210: train loss = 1.0058, val loss = 1.6590\n",
            "step 82220: train loss = 1.2596, val loss = 1.2836\n",
            "step 82230: train loss = 1.1540, val loss = 1.9768\n",
            "step 82240: train loss = 0.7732, val loss = 1.8695\n",
            "step 82250: train loss = 0.9941, val loss = 2.0937\n",
            "step 82260: train loss = 0.8753, val loss = 1.3536\n",
            "step 82270: train loss = 0.7278, val loss = 1.6021\n",
            "step 82280: train loss = 0.8876, val loss = 1.4239\n",
            "step 82290: train loss = 0.8430, val loss = 1.5526\n",
            "step 82300: train loss = 0.8447, val loss = 1.5890\n",
            "step 82310: train loss = 0.9917, val loss = 1.3771\n",
            "step 82320: train loss = 0.9090, val loss = 1.8078\n",
            "step 82330: train loss = 0.9567, val loss = 1.4563\n",
            "step 82340: train loss = 0.5414, val loss = 1.5928\n",
            "step 82350: train loss = 0.9000, val loss = 1.4792\n",
            "step 82360: train loss = 0.8990, val loss = 1.8974\n",
            "step 82370: train loss = 0.9392, val loss = 1.1089\n",
            "step 82380: train loss = 0.9629, val loss = 1.7220\n",
            "step 82390: train loss = 1.5653, val loss = 2.0492\n",
            "step 82400: train loss = 0.6409, val loss = 1.4155\n",
            "step 82410: train loss = 1.2724, val loss = 1.5837\n",
            "step 82420: train loss = 1.0090, val loss = 1.6814\n",
            "step 82430: train loss = 1.0480, val loss = 1.5009\n",
            "step 82440: train loss = 0.7341, val loss = 1.8466\n",
            "step 82450: train loss = 0.9063, val loss = 1.6298\n",
            "step 82460: train loss = 0.9172, val loss = 1.4211\n",
            "step 82470: train loss = 0.6606, val loss = 1.3451\n",
            "step 82480: train loss = 0.7676, val loss = 1.2365\n",
            "step 82490: train loss = 0.6185, val loss = 1.5715\n",
            "step 82500: train loss = 0.6588, val loss = 1.5320\n",
            "step 82510: train loss = 0.7217, val loss = 1.8451\n",
            "step 82520: train loss = 0.7026, val loss = 1.3993\n",
            "step 82530: train loss = 0.6866, val loss = 1.5163\n",
            "step 82540: train loss = 0.5788, val loss = 1.8621\n",
            "step 82550: train loss = 0.9590, val loss = 1.7497\n",
            "step 82560: train loss = 0.7769, val loss = 1.9662\n",
            "step 82570: train loss = 0.7487, val loss = 1.4307\n",
            "step 82580: train loss = 0.8709, val loss = 2.2467\n",
            "step 82590: train loss = 0.5391, val loss = 1.6790\n",
            "step 82600: train loss = 0.9546, val loss = 1.1559\n",
            "step 82610: train loss = 0.9569, val loss = 1.5358\n",
            "step 82620: train loss = 0.5626, val loss = 1.7668\n",
            "step 82630: train loss = 0.6939, val loss = 1.5924\n",
            "step 82640: train loss = 1.1576, val loss = 1.9226\n",
            "step 82650: train loss = 0.8301, val loss = 1.6333\n",
            "step 82660: train loss = 0.7411, val loss = 1.3051\n",
            "step 82670: train loss = 0.8964, val loss = 1.4204\n",
            "step 82680: train loss = 0.9382, val loss = 1.3844\n",
            "step 82690: train loss = 0.8935, val loss = 1.3154\n",
            "step 82700: train loss = 0.7632, val loss = 2.3739\n",
            "step 82710: train loss = 0.8630, val loss = 1.6884\n",
            "step 82720: train loss = 0.9410, val loss = 1.7467\n",
            "step 82730: train loss = 0.7550, val loss = 2.2230\n",
            "step 82740: train loss = 0.7650, val loss = 1.7401\n",
            "step 82750: train loss = 0.9759, val loss = 1.3514\n",
            "step 82760: train loss = 1.0182, val loss = 1.8085\n",
            "step 82770: train loss = 0.9811, val loss = 1.7243\n",
            "step 82780: train loss = 0.8274, val loss = 1.9623\n",
            "step 82790: train loss = 0.6707, val loss = 1.7869\n",
            "step 82800: train loss = 0.9936, val loss = 1.8102\n",
            "step 82810: train loss = 0.9913, val loss = 2.0146\n",
            "step 82820: train loss = 0.9177, val loss = 1.8189\n",
            "step 82830: train loss = 0.8184, val loss = 1.6744\n",
            "step 82840: train loss = 0.8213, val loss = 1.7361\n",
            "step 82850: train loss = 1.4963, val loss = 1.7839\n",
            "step 82860: train loss = 0.9089, val loss = 1.2790\n",
            "step 82870: train loss = 0.5218, val loss = 1.5988\n",
            "step 82880: train loss = 0.7420, val loss = 1.1859\n",
            "step 82890: train loss = 0.8817, val loss = 1.8146\n",
            "step 82900: train loss = 0.8273, val loss = 1.5664\n",
            "step 82910: train loss = 0.8099, val loss = 1.9635\n",
            "step 82920: train loss = 0.4851, val loss = 1.6258\n",
            "step 82930: train loss = 0.4766, val loss = 1.4809\n",
            "step 82940: train loss = 1.1020, val loss = 2.2401\n",
            "step 82950: train loss = 0.8643, val loss = 1.6562\n",
            "step 82960: train loss = 0.8842, val loss = 1.5778\n",
            "step 82970: train loss = 1.1726, val loss = 1.9961\n",
            "step 82980: train loss = 1.3221, val loss = 1.6322\n",
            "step 82990: train loss = 0.7455, val loss = 1.2559\n",
            "step 83000: train loss = 0.6398, val loss = 1.7133\n",
            "step 83010: train loss = 1.1075, val loss = 1.4811\n",
            "step 83020: train loss = 0.9220, val loss = 1.6248\n",
            "step 83030: train loss = 0.7653, val loss = 1.5089\n",
            "step 83040: train loss = 0.7958, val loss = 1.2641\n",
            "step 83050: train loss = 0.9663, val loss = 2.1793\n",
            "step 83060: train loss = 0.8266, val loss = 1.7194\n",
            "step 83070: train loss = 1.0168, val loss = 1.5804\n",
            "step 83080: train loss = 1.0909, val loss = 2.0673\n",
            "step 83090: train loss = 0.8199, val loss = 1.5748\n",
            "step 83100: train loss = 0.7131, val loss = 1.9277\n",
            "step 83110: train loss = 1.1653, val loss = 1.5259\n",
            "step 83120: train loss = 0.6284, val loss = 1.8631\n",
            "step 83130: train loss = 0.7554, val loss = 1.4288\n",
            "step 83140: train loss = 0.7825, val loss = 1.4113\n",
            "step 83150: train loss = 0.6698, val loss = 1.6877\n",
            "step 83160: train loss = 0.5787, val loss = 1.3925\n",
            "step 83170: train loss = 0.8960, val loss = 1.3888\n",
            "step 83180: train loss = 0.8878, val loss = 1.7089\n",
            "step 83190: train loss = 1.0406, val loss = 1.6093\n",
            "step 83200: train loss = 0.9310, val loss = 1.3159\n",
            "step 83210: train loss = 0.6375, val loss = 1.5135\n",
            "step 83220: train loss = 0.9857, val loss = 2.1008\n",
            "step 83230: train loss = 1.0111, val loss = 1.3706\n",
            "step 83240: train loss = 0.7523, val loss = 1.4560\n",
            "step 83250: train loss = 1.1341, val loss = 1.0821\n",
            "step 83260: train loss = 0.7200, val loss = 1.7841\n",
            "step 83270: train loss = 0.9344, val loss = 1.9654\n",
            "step 83280: train loss = 0.9279, val loss = 2.5262\n",
            "step 83290: train loss = 0.7845, val loss = 2.0237\n",
            "step 83300: train loss = 0.6672, val loss = 1.2690\n",
            "step 83310: train loss = 1.0223, val loss = 1.9175\n",
            "step 83320: train loss = 0.7231, val loss = 1.5866\n",
            "step 83330: train loss = 0.9801, val loss = 1.4481\n",
            "step 83340: train loss = 0.6275, val loss = 2.4736\n",
            "step 83350: train loss = 0.7543, val loss = 1.3703\n",
            "step 83360: train loss = 0.9902, val loss = 1.6185\n",
            "step 83370: train loss = 0.7797, val loss = 1.6159\n",
            "step 83380: train loss = 0.6572, val loss = 1.6524\n",
            "step 83390: train loss = 0.7012, val loss = 1.9239\n",
            "step 83400: train loss = 0.6743, val loss = 1.7908\n",
            "step 83410: train loss = 1.1046, val loss = 2.2488\n",
            "step 83420: train loss = 0.7369, val loss = 1.8392\n",
            "step 83430: train loss = 0.7920, val loss = 1.2986\n",
            "step 83440: train loss = 0.8179, val loss = 1.1688\n",
            "step 83450: train loss = 0.8239, val loss = 2.2159\n",
            "step 83460: train loss = 0.7647, val loss = 1.5633\n",
            "step 83470: train loss = 0.7384, val loss = 1.1127\n",
            "step 83480: train loss = 0.7089, val loss = 1.6278\n",
            "step 83490: train loss = 0.4161, val loss = 2.1953\n",
            "step 83500: train loss = 0.8631, val loss = 1.7018\n",
            "step 83510: train loss = 0.9877, val loss = 1.7369\n",
            "step 83520: train loss = 0.8096, val loss = 1.6797\n",
            "step 83530: train loss = 0.4705, val loss = 1.6605\n",
            "step 83540: train loss = 0.6776, val loss = 1.3076\n",
            "step 83550: train loss = 0.7437, val loss = 2.0196\n",
            "step 83560: train loss = 0.9739, val loss = 1.9690\n",
            "step 83570: train loss = 0.8532, val loss = 1.3914\n",
            "step 83580: train loss = 0.5553, val loss = 1.8740\n",
            "step 83590: train loss = 0.9338, val loss = 1.7084\n",
            "step 83600: train loss = 0.6890, val loss = 1.5521\n",
            "step 83610: train loss = 0.8053, val loss = 1.8591\n",
            "step 83620: train loss = 0.5291, val loss = 1.6728\n",
            "step 83630: train loss = 0.7085, val loss = 2.0401\n",
            "step 83640: train loss = 0.9032, val loss = 1.7712\n",
            "step 83650: train loss = 0.8576, val loss = 2.4091\n",
            "step 83660: train loss = 0.7726, val loss = 1.5433\n",
            "step 83670: train loss = 0.6644, val loss = 1.0999\n",
            "step 83680: train loss = 0.7212, val loss = 1.5763\n",
            "step 83690: train loss = 0.7651, val loss = 1.0487\n",
            "step 83700: train loss = 0.8148, val loss = 1.7847\n",
            "step 83710: train loss = 0.9887, val loss = 1.8011\n",
            "step 83720: train loss = 0.8642, val loss = 1.5666\n",
            "step 83730: train loss = 0.8340, val loss = 1.4301\n",
            "step 83740: train loss = 1.3339, val loss = 1.8520\n",
            "step 83750: train loss = 0.7490, val loss = 1.9372\n",
            "step 83760: train loss = 0.6968, val loss = 2.3959\n",
            "step 83770: train loss = 1.0684, val loss = 1.5360\n",
            "step 83780: train loss = 0.7707, val loss = 0.9658\n",
            "step 83790: train loss = 0.8230, val loss = 1.6780\n",
            "step 83800: train loss = 0.9291, val loss = 1.9908\n",
            "step 83810: train loss = 0.5834, val loss = 1.4023\n",
            "step 83820: train loss = 0.7349, val loss = 1.5122\n",
            "step 83830: train loss = 0.6752, val loss = 1.5924\n",
            "step 83840: train loss = 0.9523, val loss = 1.2015\n",
            "step 83850: train loss = 0.8928, val loss = 1.9637\n",
            "step 83860: train loss = 0.6074, val loss = 1.7020\n",
            "step 83870: train loss = 0.8334, val loss = 2.4018\n",
            "step 83880: train loss = 0.9182, val loss = 1.5941\n",
            "step 83890: train loss = 1.0278, val loss = 1.6063\n",
            "step 83900: train loss = 0.8725, val loss = 1.4748\n",
            "step 83910: train loss = 0.8200, val loss = 1.5350\n",
            "step 83920: train loss = 0.5314, val loss = 1.3913\n",
            "step 83930: train loss = 0.9746, val loss = 1.1408\n",
            "step 83940: train loss = 0.8573, val loss = 1.7545\n",
            "step 83950: train loss = 0.7533, val loss = 1.6207\n",
            "step 83960: train loss = 0.8459, val loss = 1.8830\n",
            "step 83970: train loss = 0.8703, val loss = 1.4787\n",
            "step 83980: train loss = 0.8098, val loss = 1.4286\n",
            "step 83990: train loss = 0.9852, val loss = 1.5991\n",
            "step 84000: train loss = 1.0208, val loss = 1.6731\n",
            "step 84010: train loss = 0.6803, val loss = 1.3460\n",
            "step 84020: train loss = 0.7599, val loss = 0.9782\n",
            "step 84030: train loss = 0.8267, val loss = 1.6637\n",
            "step 84040: train loss = 0.8396, val loss = 1.8239\n",
            "step 84050: train loss = 0.9156, val loss = 1.7604\n",
            "step 84060: train loss = 1.1198, val loss = 1.8146\n",
            "step 84070: train loss = 0.8989, val loss = 1.3370\n",
            "step 84080: train loss = 1.0245, val loss = 1.3338\n",
            "step 84090: train loss = 0.7917, val loss = 2.3344\n",
            "step 84100: train loss = 0.9990, val loss = 1.9991\n",
            "step 84110: train loss = 0.8750, val loss = 1.5932\n",
            "step 84120: train loss = 0.6877, val loss = 1.3403\n",
            "step 84130: train loss = 0.6055, val loss = 1.4217\n",
            "step 84140: train loss = 0.9040, val loss = 2.2460\n",
            "step 84150: train loss = 0.7099, val loss = 2.3169\n",
            "step 84160: train loss = 0.8357, val loss = 1.3088\n",
            "step 84170: train loss = 0.6552, val loss = 1.5983\n",
            "step 84180: train loss = 0.6947, val loss = 2.3841\n",
            "step 84190: train loss = 0.6589, val loss = 1.6021\n",
            "step 84200: train loss = 0.8142, val loss = 0.9880\n",
            "step 84210: train loss = 0.5563, val loss = 1.7202\n",
            "step 84220: train loss = 0.6522, val loss = 1.7973\n",
            "step 84230: train loss = 0.7275, val loss = 1.9518\n",
            "step 84240: train loss = 0.8572, val loss = 2.0504\n",
            "step 84250: train loss = 0.7524, val loss = 1.9197\n",
            "step 84260: train loss = 0.7040, val loss = 2.2920\n",
            "step 84270: train loss = 1.2672, val loss = 1.4569\n",
            "step 84280: train loss = 0.6354, val loss = 1.5575\n",
            "step 84290: train loss = 0.8464, val loss = 1.6884\n",
            "step 84300: train loss = 0.7799, val loss = 1.5769\n",
            "step 84310: train loss = 0.9069, val loss = 1.7500\n",
            "step 84320: train loss = 1.0022, val loss = 2.5728\n",
            "step 84330: train loss = 0.8443, val loss = 2.0699\n",
            "step 84340: train loss = 0.7591, val loss = 1.6259\n",
            "step 84350: train loss = 0.5864, val loss = 2.2084\n",
            "step 84360: train loss = 1.1263, val loss = 1.9297\n",
            "step 84370: train loss = 1.1072, val loss = 1.7022\n",
            "step 84380: train loss = 0.7484, val loss = 1.4406\n",
            "step 84390: train loss = 0.8543, val loss = 1.5984\n",
            "step 84400: train loss = 0.9027, val loss = 1.6744\n",
            "step 84410: train loss = 0.9686, val loss = 0.8374\n",
            "step 84420: train loss = 0.9097, val loss = 1.7367\n",
            "step 84430: train loss = 0.6323, val loss = 1.1571\n",
            "step 84440: train loss = 0.8379, val loss = 1.0395\n",
            "step 84450: train loss = 0.9215, val loss = 1.2037\n",
            "step 84460: train loss = 0.7113, val loss = 1.6638\n",
            "step 84470: train loss = 0.6220, val loss = 1.6001\n",
            "step 84480: train loss = 0.9756, val loss = 1.5001\n",
            "step 84490: train loss = 0.7950, val loss = 1.9559\n",
            "step 84500: train loss = 0.8746, val loss = 2.1426\n",
            "step 84510: train loss = 0.9109, val loss = 1.7034\n",
            "step 84520: train loss = 0.7787, val loss = 1.6431\n",
            "step 84530: train loss = 0.6293, val loss = 1.7052\n",
            "step 84540: train loss = 0.6806, val loss = 2.2581\n",
            "step 84550: train loss = 1.0318, val loss = 1.5056\n",
            "step 84560: train loss = 1.2137, val loss = 1.6232\n",
            "step 84570: train loss = 0.7369, val loss = 1.0575\n",
            "step 84580: train loss = 0.8008, val loss = 1.9928\n",
            "step 84590: train loss = 0.9012, val loss = 1.8027\n",
            "step 84600: train loss = 0.7624, val loss = 1.5722\n",
            "step 84610: train loss = 1.1500, val loss = 1.1097\n",
            "step 84620: train loss = 0.8186, val loss = 1.6737\n",
            "step 84630: train loss = 0.9687, val loss = 1.4609\n",
            "step 84640: train loss = 1.0768, val loss = 1.7384\n",
            "step 84650: train loss = 0.8129, val loss = 1.8125\n",
            "step 84660: train loss = 0.6390, val loss = 1.6923\n",
            "step 84670: train loss = 0.9503, val loss = 1.7587\n",
            "step 84680: train loss = 0.8679, val loss = 1.9538\n",
            "step 84690: train loss = 0.6593, val loss = 1.5867\n",
            "step 84700: train loss = 0.8366, val loss = 1.5892\n",
            "step 84710: train loss = 1.1089, val loss = 1.3796\n",
            "step 84720: train loss = 1.0360, val loss = 2.0376\n",
            "step 84730: train loss = 1.0467, val loss = 1.9891\n",
            "step 84740: train loss = 0.9851, val loss = 1.4234\n",
            "step 84750: train loss = 0.9382, val loss = 0.8256\n",
            "step 84760: train loss = 0.8144, val loss = 1.2708\n",
            "step 84770: train loss = 0.7286, val loss = 2.1166\n",
            "step 84780: train loss = 0.8870, val loss = 1.7009\n",
            "step 84790: train loss = 0.6515, val loss = 1.6208\n",
            "step 84800: train loss = 0.8042, val loss = 1.8109\n",
            "step 84810: train loss = 0.7513, val loss = 1.2646\n",
            "step 84820: train loss = 0.9268, val loss = 1.5535\n",
            "step 84830: train loss = 0.8184, val loss = 1.5109\n",
            "step 84840: train loss = 0.8901, val loss = 1.7874\n",
            "step 84850: train loss = 1.0515, val loss = 1.7502\n",
            "step 84860: train loss = 0.8940, val loss = 1.6142\n",
            "step 84870: train loss = 0.7712, val loss = 2.1284\n",
            "step 84880: train loss = 1.0250, val loss = 1.3433\n",
            "step 84890: train loss = 1.3973, val loss = 1.4414\n",
            "step 84900: train loss = 0.8567, val loss = 2.1538\n",
            "step 84910: train loss = 1.0617, val loss = 2.0423\n",
            "step 84920: train loss = 0.6880, val loss = 1.2534\n",
            "step 84930: train loss = 1.1228, val loss = 1.1622\n",
            "step 84940: train loss = 0.6500, val loss = 1.6313\n",
            "step 84950: train loss = 0.9092, val loss = 2.0143\n",
            "step 84960: train loss = 0.8411, val loss = 2.1894\n",
            "step 84970: train loss = 0.7544, val loss = 1.5921\n",
            "step 84980: train loss = 0.9905, val loss = 1.8396\n",
            "step 84990: train loss = 0.6389, val loss = 1.9890\n",
            "step 85000: train loss = 0.7893, val loss = 1.3188\n",
            "step 85010: train loss = 0.8155, val loss = 1.8518\n",
            "step 85020: train loss = 1.0404, val loss = 1.7801\n",
            "step 85030: train loss = 0.6457, val loss = 1.9307\n",
            "step 85040: train loss = 0.6084, val loss = 1.2727\n",
            "step 85050: train loss = 0.6960, val loss = 1.5940\n",
            "step 85060: train loss = 0.6633, val loss = 1.6506\n",
            "step 85070: train loss = 0.5309, val loss = 1.8269\n",
            "step 85080: train loss = 0.8828, val loss = 1.8726\n",
            "step 85090: train loss = 0.9144, val loss = 1.2875\n",
            "step 85100: train loss = 1.0052, val loss = 1.9133\n",
            "step 85110: train loss = 0.7681, val loss = 1.0942\n",
            "step 85120: train loss = 0.6879, val loss = 1.3490\n",
            "step 85130: train loss = 0.5386, val loss = 1.3438\n",
            "step 85140: train loss = 0.9535, val loss = 1.7597\n",
            "step 85150: train loss = 0.5951, val loss = 1.6349\n",
            "step 85160: train loss = 1.1730, val loss = 1.9268\n",
            "step 85170: train loss = 0.7111, val loss = 1.2470\n",
            "step 85180: train loss = 0.9423, val loss = 1.4924\n",
            "step 85190: train loss = 0.7720, val loss = 1.5558\n",
            "step 85200: train loss = 1.0217, val loss = 2.1531\n",
            "step 85210: train loss = 0.7811, val loss = 1.7592\n",
            "step 85220: train loss = 0.8785, val loss = 1.4276\n",
            "step 85230: train loss = 0.8102, val loss = 1.4372\n",
            "step 85240: train loss = 1.2645, val loss = 0.9951\n",
            "step 85250: train loss = 0.9376, val loss = 1.7363\n",
            "step 85260: train loss = 0.7519, val loss = 1.8425\n",
            "step 85270: train loss = 0.5461, val loss = 1.7135\n",
            "step 85280: train loss = 0.7424, val loss = 1.5637\n",
            "step 85290: train loss = 0.8819, val loss = 1.6701\n",
            "step 85300: train loss = 0.5877, val loss = 1.9471\n",
            "step 85310: train loss = 0.8988, val loss = 1.3820\n",
            "step 85320: train loss = 0.7153, val loss = 1.6290\n",
            "step 85330: train loss = 0.5018, val loss = 2.1110\n",
            "step 85340: train loss = 0.9571, val loss = 1.6653\n",
            "step 85350: train loss = 0.6050, val loss = 2.0292\n",
            "step 85360: train loss = 0.7971, val loss = 1.5926\n",
            "step 85370: train loss = 0.6927, val loss = 1.6355\n",
            "step 85380: train loss = 1.1143, val loss = 1.2636\n",
            "step 85390: train loss = 0.5613, val loss = 1.7800\n",
            "step 85400: train loss = 0.8465, val loss = 1.5047\n",
            "step 85410: train loss = 0.6199, val loss = 1.6022\n",
            "step 85420: train loss = 0.7477, val loss = 1.4604\n",
            "step 85430: train loss = 1.1193, val loss = 1.9142\n",
            "step 85440: train loss = 0.6785, val loss = 1.5822\n",
            "step 85450: train loss = 0.6040, val loss = 1.8770\n",
            "step 85460: train loss = 0.8689, val loss = 2.0038\n",
            "step 85470: train loss = 0.9837, val loss = 2.4315\n",
            "step 85480: train loss = 0.8693, val loss = 1.9986\n",
            "step 85490: train loss = 1.1259, val loss = 1.4123\n",
            "step 85500: train loss = 0.8195, val loss = 1.7685\n",
            "step 85510: train loss = 0.6063, val loss = 2.1843\n",
            "step 85520: train loss = 0.6738, val loss = 1.6022\n",
            "step 85530: train loss = 0.7728, val loss = 1.9236\n",
            "step 85540: train loss = 1.0489, val loss = 1.4673\n",
            "step 85550: train loss = 0.8194, val loss = 1.4370\n",
            "step 85560: train loss = 0.7839, val loss = 1.5422\n",
            "step 85570: train loss = 0.6612, val loss = 1.9238\n",
            "step 85580: train loss = 1.0770, val loss = 2.1749\n",
            "step 85590: train loss = 0.8143, val loss = 1.7552\n",
            "step 85600: train loss = 0.8990, val loss = 1.6543\n",
            "step 85610: train loss = 0.6509, val loss = 2.1735\n",
            "step 85620: train loss = 0.8325, val loss = 1.6168\n",
            "step 85630: train loss = 0.6887, val loss = 1.7389\n",
            "step 85640: train loss = 0.6497, val loss = 1.9899\n",
            "step 85650: train loss = 0.6656, val loss = 2.2742\n",
            "step 85660: train loss = 0.6397, val loss = 1.4267\n",
            "step 85670: train loss = 0.8836, val loss = 1.8858\n",
            "step 85680: train loss = 0.8501, val loss = 1.4244\n",
            "step 85690: train loss = 0.9020, val loss = 1.2756\n",
            "step 85700: train loss = 0.8630, val loss = 2.0001\n",
            "step 85710: train loss = 0.6633, val loss = 1.6180\n",
            "step 85720: train loss = 0.9110, val loss = 2.0434\n",
            "step 85730: train loss = 0.7354, val loss = 1.6018\n",
            "step 85740: train loss = 0.9468, val loss = 1.5529\n",
            "step 85750: train loss = 0.7030, val loss = 2.2186\n",
            "step 85760: train loss = 0.8069, val loss = 1.6931\n",
            "step 85770: train loss = 0.6434, val loss = 1.8681\n",
            "step 85780: train loss = 1.0529, val loss = 1.8539\n",
            "step 85790: train loss = 0.5779, val loss = 1.7306\n",
            "step 85800: train loss = 0.7338, val loss = 2.0221\n",
            "step 85810: train loss = 0.8847, val loss = 1.4896\n",
            "step 85820: train loss = 0.6681, val loss = 1.8121\n",
            "step 85830: train loss = 0.9187, val loss = 1.6157\n",
            "step 85840: train loss = 0.6977, val loss = 1.1784\n",
            "step 85850: train loss = 0.9444, val loss = 2.3496\n",
            "step 85860: train loss = 0.8785, val loss = 1.7584\n",
            "step 85870: train loss = 0.7634, val loss = 2.1132\n",
            "step 85880: train loss = 0.5074, val loss = 2.1388\n",
            "step 85890: train loss = 1.1583, val loss = 1.7433\n",
            "step 85900: train loss = 1.0613, val loss = 1.6353\n",
            "step 85910: train loss = 0.6670, val loss = 1.3927\n",
            "step 85920: train loss = 0.6998, val loss = 1.5895\n",
            "step 85930: train loss = 1.1615, val loss = 2.0393\n",
            "step 85940: train loss = 0.6682, val loss = 2.0515\n",
            "step 85950: train loss = 0.6934, val loss = 1.5210\n",
            "step 85960: train loss = 0.9406, val loss = 1.6641\n",
            "step 85970: train loss = 0.9065, val loss = 1.8297\n",
            "step 85980: train loss = 0.7297, val loss = 1.7172\n",
            "step 85990: train loss = 0.5421, val loss = 1.8389\n",
            "step 86000: train loss = 0.6401, val loss = 2.1405\n",
            "step 86010: train loss = 0.9602, val loss = 1.0787\n",
            "step 86020: train loss = 0.8096, val loss = 2.0630\n",
            "step 86030: train loss = 0.9641, val loss = 1.5383\n",
            "step 86040: train loss = 0.6799, val loss = 1.1612\n",
            "step 86050: train loss = 0.9961, val loss = 1.1535\n",
            "step 86060: train loss = 0.5220, val loss = 1.5450\n",
            "step 86070: train loss = 0.7805, val loss = 1.7220\n",
            "step 86080: train loss = 0.8959, val loss = 1.5082\n",
            "step 86090: train loss = 0.9088, val loss = 1.4939\n",
            "step 86100: train loss = 0.9392, val loss = 1.8709\n",
            "step 86110: train loss = 0.6188, val loss = 1.3361\n",
            "step 86120: train loss = 0.6638, val loss = 2.0635\n",
            "step 86130: train loss = 1.1965, val loss = 1.5300\n",
            "step 86140: train loss = 0.9206, val loss = 1.7636\n",
            "step 86150: train loss = 0.6732, val loss = 1.2957\n",
            "step 86160: train loss = 0.5823, val loss = 2.1808\n",
            "step 86170: train loss = 0.7137, val loss = 2.3241\n",
            "step 86180: train loss = 0.6219, val loss = 1.1864\n",
            "step 86190: train loss = 0.9482, val loss = 1.3964\n",
            "step 86200: train loss = 0.8242, val loss = 1.5808\n",
            "step 86210: train loss = 1.0017, val loss = 1.7752\n",
            "step 86220: train loss = 0.9673, val loss = 1.7409\n",
            "step 86230: train loss = 0.9523, val loss = 1.7553\n",
            "step 86240: train loss = 0.7238, val loss = 2.1755\n",
            "step 86250: train loss = 0.8656, val loss = 1.9491\n",
            "step 86260: train loss = 1.2235, val loss = 2.0594\n",
            "step 86270: train loss = 0.5203, val loss = 1.3305\n",
            "step 86280: train loss = 0.9001, val loss = 2.2016\n",
            "step 86290: train loss = 0.8532, val loss = 2.2813\n",
            "step 86300: train loss = 0.7664, val loss = 1.4142\n",
            "step 86310: train loss = 0.6966, val loss = 2.3296\n",
            "step 86320: train loss = 0.7395, val loss = 1.5177\n",
            "step 86330: train loss = 1.2627, val loss = 1.6423\n",
            "step 86340: train loss = 0.8424, val loss = 2.4263\n",
            "step 86350: train loss = 1.1045, val loss = 1.2271\n",
            "step 86360: train loss = 0.9082, val loss = 1.9341\n",
            "step 86370: train loss = 0.6517, val loss = 1.3146\n",
            "step 86380: train loss = 1.0963, val loss = 1.3907\n",
            "step 86390: train loss = 0.6875, val loss = 1.9434\n",
            "step 86400: train loss = 0.8384, val loss = 1.9458\n",
            "step 86410: train loss = 0.9070, val loss = 2.0369\n",
            "step 86420: train loss = 1.0672, val loss = 0.9339\n",
            "step 86430: train loss = 1.1134, val loss = 2.1416\n",
            "step 86440: train loss = 0.6958, val loss = 2.1291\n",
            "step 86450: train loss = 0.7967, val loss = 1.8063\n",
            "step 86460: train loss = 0.9314, val loss = 1.6209\n",
            "step 86470: train loss = 0.8295, val loss = 2.1825\n",
            "step 86480: train loss = 1.0444, val loss = 2.1764\n",
            "step 86490: train loss = 0.6456, val loss = 1.9566\n",
            "step 86500: train loss = 0.7525, val loss = 1.5858\n",
            "step 86510: train loss = 0.9492, val loss = 2.1017\n",
            "step 86520: train loss = 0.9033, val loss = 2.0737\n",
            "step 86530: train loss = 0.6872, val loss = 1.9206\n",
            "step 86540: train loss = 0.9504, val loss = 1.5948\n",
            "step 86550: train loss = 0.6305, val loss = 2.1477\n",
            "step 86560: train loss = 0.7260, val loss = 1.5857\n",
            "step 86570: train loss = 0.8274, val loss = 1.9110\n",
            "step 86580: train loss = 0.9366, val loss = 1.9169\n",
            "step 86590: train loss = 0.7410, val loss = 1.8157\n",
            "step 86600: train loss = 0.5731, val loss = 1.7574\n",
            "step 86610: train loss = 0.6919, val loss = 1.4086\n",
            "step 86620: train loss = 0.4145, val loss = 1.7468\n",
            "step 86630: train loss = 0.7039, val loss = 2.0851\n",
            "step 86640: train loss = 0.6673, val loss = 1.8883\n",
            "step 86650: train loss = 0.5921, val loss = 1.6819\n",
            "step 86660: train loss = 0.7351, val loss = 1.0868\n",
            "step 86670: train loss = 0.6840, val loss = 1.7025\n",
            "step 86680: train loss = 0.9345, val loss = 1.2075\n",
            "step 86690: train loss = 0.7945, val loss = 1.8689\n",
            "step 86700: train loss = 0.5882, val loss = 1.6127\n",
            "step 86710: train loss = 0.6581, val loss = 1.8329\n",
            "step 86720: train loss = 0.9095, val loss = 1.7886\n",
            "step 86730: train loss = 0.5582, val loss = 1.8282\n",
            "step 86740: train loss = 0.8032, val loss = 1.1078\n",
            "step 86750: train loss = 0.8767, val loss = 1.8273\n",
            "step 86760: train loss = 0.6564, val loss = 1.3896\n",
            "step 86770: train loss = 0.7569, val loss = 2.1153\n",
            "step 86780: train loss = 0.7303, val loss = 2.0230\n",
            "step 86790: train loss = 0.8818, val loss = 1.4875\n",
            "step 86800: train loss = 0.8014, val loss = 1.7333\n",
            "step 86810: train loss = 0.9338, val loss = 1.4120\n",
            "step 86820: train loss = 0.7096, val loss = 1.5903\n",
            "step 86830: train loss = 0.6357, val loss = 1.5652\n",
            "step 86840: train loss = 0.7432, val loss = 2.2666\n",
            "step 86850: train loss = 0.7644, val loss = 1.7905\n",
            "step 86860: train loss = 0.6754, val loss = 1.6850\n",
            "step 86870: train loss = 0.8683, val loss = 2.0177\n",
            "step 86880: train loss = 0.9903, val loss = 1.3975\n",
            "step 86890: train loss = 0.6570, val loss = 1.9778\n",
            "step 86900: train loss = 0.6136, val loss = 1.5538\n",
            "step 86910: train loss = 1.0462, val loss = 1.7312\n",
            "step 86920: train loss = 1.0528, val loss = 2.1314\n",
            "step 86930: train loss = 0.7245, val loss = 2.6665\n",
            "step 86940: train loss = 0.7577, val loss = 1.1031\n",
            "step 86950: train loss = 0.4729, val loss = 1.9996\n",
            "step 86960: train loss = 0.4874, val loss = 1.6434\n",
            "step 86970: train loss = 0.7601, val loss = 1.7812\n",
            "step 86980: train loss = 0.8962, val loss = 2.2361\n",
            "step 86990: train loss = 0.6178, val loss = 1.8217\n",
            "step 87000: train loss = 0.7212, val loss = 1.0497\n",
            "step 87010: train loss = 0.5912, val loss = 1.6790\n",
            "step 87020: train loss = 0.7853, val loss = 1.3220\n",
            "step 87030: train loss = 0.6869, val loss = 1.5584\n",
            "step 87040: train loss = 1.0249, val loss = 1.7497\n",
            "step 87050: train loss = 0.6815, val loss = 1.3917\n",
            "step 87060: train loss = 0.8735, val loss = 1.6524\n",
            "step 87070: train loss = 0.8831, val loss = 1.1843\n",
            "step 87080: train loss = 1.1415, val loss = 1.7430\n",
            "step 87090: train loss = 0.5200, val loss = 1.4373\n",
            "step 87100: train loss = 0.9605, val loss = 1.2514\n",
            "step 87110: train loss = 0.7806, val loss = 1.2573\n",
            "step 87120: train loss = 0.9105, val loss = 1.8922\n",
            "step 87130: train loss = 0.9514, val loss = 1.3720\n",
            "step 87140: train loss = 0.6691, val loss = 1.9469\n",
            "step 87150: train loss = 0.8412, val loss = 1.3770\n",
            "step 87160: train loss = 0.7531, val loss = 1.9085\n",
            "step 87170: train loss = 0.7420, val loss = 1.5611\n",
            "step 87180: train loss = 0.8079, val loss = 1.9564\n",
            "step 87190: train loss = 0.9742, val loss = 1.8588\n",
            "step 87200: train loss = 0.8230, val loss = 1.5869\n",
            "step 87210: train loss = 0.7451, val loss = 1.6441\n",
            "step 87220: train loss = 0.5986, val loss = 1.8823\n",
            "step 87230: train loss = 0.7193, val loss = 1.7987\n",
            "step 87240: train loss = 0.7398, val loss = 1.6836\n",
            "step 87250: train loss = 0.7533, val loss = 1.8358\n",
            "step 87260: train loss = 0.6885, val loss = 2.9194\n",
            "step 87270: train loss = 0.9818, val loss = 2.0660\n",
            "step 87280: train loss = 0.8496, val loss = 2.5548\n",
            "step 87290: train loss = 0.6717, val loss = 1.4803\n",
            "step 87300: train loss = 0.6918, val loss = 1.6792\n",
            "step 87310: train loss = 1.1144, val loss = 1.2895\n",
            "step 87320: train loss = 1.0116, val loss = 1.5660\n",
            "step 87330: train loss = 0.7739, val loss = 1.3411\n",
            "step 87340: train loss = 0.8371, val loss = 1.7548\n",
            "step 87350: train loss = 1.0694, val loss = 1.9258\n",
            "step 87360: train loss = 0.7716, val loss = 2.0404\n",
            "step 87370: train loss = 0.6816, val loss = 1.6045\n",
            "step 87380: train loss = 0.7322, val loss = 1.0672\n",
            "step 87390: train loss = 0.9847, val loss = 1.1765\n",
            "step 87400: train loss = 0.5788, val loss = 1.3693\n",
            "step 87410: train loss = 1.0109, val loss = 1.1180\n",
            "step 87420: train loss = 0.8026, val loss = 1.8677\n",
            "step 87430: train loss = 0.8020, val loss = 1.5270\n",
            "step 87440: train loss = 0.7343, val loss = 1.7352\n",
            "step 87450: train loss = 0.5371, val loss = 1.4153\n",
            "step 87460: train loss = 0.8796, val loss = 1.5114\n",
            "step 87470: train loss = 1.0060, val loss = 1.5987\n",
            "step 87480: train loss = 0.7283, val loss = 1.4530\n",
            "step 87490: train loss = 1.1984, val loss = 1.8405\n",
            "step 87500: train loss = 0.8760, val loss = 1.6254\n",
            "step 87510: train loss = 0.8385, val loss = 1.6995\n",
            "step 87520: train loss = 0.4203, val loss = 1.4603\n",
            "step 87530: train loss = 0.9264, val loss = 1.9453\n",
            "step 87540: train loss = 0.6944, val loss = 1.7029\n",
            "step 87550: train loss = 0.9360, val loss = 1.2782\n",
            "step 87560: train loss = 0.8705, val loss = 1.9846\n",
            "step 87570: train loss = 0.8861, val loss = 1.7487\n",
            "step 87580: train loss = 0.6723, val loss = 1.7157\n",
            "step 87590: train loss = 0.7075, val loss = 1.5046\n",
            "step 87600: train loss = 1.1456, val loss = 1.0522\n",
            "step 87610: train loss = 0.6699, val loss = 1.4906\n",
            "step 87620: train loss = 0.8385, val loss = 2.1876\n",
            "step 87630: train loss = 0.6564, val loss = 1.4409\n",
            "step 87640: train loss = 0.4811, val loss = 1.4256\n",
            "step 87650: train loss = 0.5297, val loss = 1.5500\n",
            "step 87660: train loss = 0.5636, val loss = 1.3621\n",
            "step 87670: train loss = 0.9388, val loss = 1.6609\n",
            "step 87680: train loss = 0.6616, val loss = 1.9177\n",
            "step 87690: train loss = 0.9133, val loss = 1.0843\n",
            "step 87700: train loss = 0.6907, val loss = 1.5490\n",
            "step 87710: train loss = 0.5930, val loss = 1.6327\n",
            "step 87720: train loss = 0.9734, val loss = 1.5619\n",
            "step 87730: train loss = 0.6050, val loss = 2.1311\n",
            "step 87740: train loss = 1.0496, val loss = 1.7705\n",
            "step 87750: train loss = 1.0273, val loss = 1.3224\n",
            "step 87760: train loss = 0.8934, val loss = 1.8088\n",
            "step 87770: train loss = 1.0420, val loss = 1.9630\n",
            "step 87780: train loss = 0.8988, val loss = 1.0092\n",
            "step 87790: train loss = 0.6673, val loss = 1.2142\n",
            "step 87800: train loss = 0.8170, val loss = 2.3441\n",
            "step 87810: train loss = 0.7250, val loss = 1.6119\n",
            "step 87820: train loss = 0.5951, val loss = 1.7310\n",
            "step 87830: train loss = 0.6437, val loss = 1.9585\n",
            "step 87840: train loss = 0.7770, val loss = 1.8945\n",
            "step 87850: train loss = 1.5155, val loss = 2.3310\n",
            "step 87860: train loss = 0.9021, val loss = 1.8888\n",
            "step 87870: train loss = 0.5748, val loss = 1.6107\n",
            "step 87880: train loss = 0.6883, val loss = 1.4346\n",
            "step 87890: train loss = 0.6174, val loss = 1.7328\n",
            "step 87900: train loss = 0.7193, val loss = 1.4396\n",
            "step 87910: train loss = 1.0944, val loss = 1.3441\n",
            "step 87920: train loss = 0.7196, val loss = 1.4229\n",
            "step 87930: train loss = 0.7894, val loss = 1.6012\n",
            "step 87940: train loss = 0.5598, val loss = 1.6982\n",
            "step 87950: train loss = 0.8760, val loss = 1.4738\n",
            "step 87960: train loss = 0.8311, val loss = 1.5298\n",
            "step 87970: train loss = 1.0198, val loss = 1.7222\n",
            "step 87980: train loss = 1.1315, val loss = 1.9038\n",
            "step 87990: train loss = 0.5517, val loss = 1.5271\n",
            "step 88000: train loss = 0.7153, val loss = 1.0688\n",
            "step 88010: train loss = 0.4261, val loss = 1.1440\n",
            "step 88020: train loss = 1.0400, val loss = 1.8236\n",
            "step 88030: train loss = 0.7256, val loss = 1.8603\n",
            "step 88040: train loss = 0.4669, val loss = 1.1226\n",
            "step 88050: train loss = 0.8514, val loss = 2.4187\n",
            "step 88060: train loss = 0.7994, val loss = 1.8035\n",
            "step 88070: train loss = 0.7299, val loss = 1.4578\n",
            "step 88080: train loss = 0.8434, val loss = 2.0170\n",
            "step 88090: train loss = 0.5830, val loss = 1.2352\n",
            "step 88100: train loss = 0.6851, val loss = 1.5155\n",
            "step 88110: train loss = 1.0500, val loss = 1.7415\n",
            "step 88120: train loss = 0.9865, val loss = 2.0616\n",
            "step 88130: train loss = 0.6153, val loss = 1.7047\n",
            "step 88140: train loss = 0.5978, val loss = 1.6754\n",
            "step 88150: train loss = 0.7912, val loss = 1.9632\n",
            "step 88160: train loss = 0.4995, val loss = 1.9923\n",
            "step 88170: train loss = 0.6488, val loss = 1.7541\n",
            "step 88180: train loss = 0.6591, val loss = 2.0941\n",
            "step 88190: train loss = 0.5617, val loss = 1.6309\n",
            "step 88200: train loss = 0.9515, val loss = 1.3471\n",
            "step 88210: train loss = 1.0588, val loss = 1.7679\n",
            "step 88220: train loss = 0.7837, val loss = 2.3152\n",
            "step 88230: train loss = 1.0078, val loss = 1.5804\n",
            "step 88240: train loss = 0.9501, val loss = 2.2811\n",
            "step 88250: train loss = 0.7886, val loss = 1.8745\n",
            "step 88260: train loss = 0.8733, val loss = 1.3445\n",
            "step 88270: train loss = 0.4387, val loss = 1.6174\n",
            "step 88280: train loss = 1.0223, val loss = 1.5595\n",
            "step 88290: train loss = 0.7278, val loss = 2.3724\n",
            "step 88300: train loss = 0.7492, val loss = 1.5721\n",
            "step 88310: train loss = 0.7205, val loss = 1.7981\n",
            "step 88320: train loss = 0.7114, val loss = 2.5287\n",
            "step 88330: train loss = 0.7230, val loss = 1.4700\n",
            "step 88340: train loss = 0.7346, val loss = 1.5080\n",
            "step 88350: train loss = 0.6786, val loss = 1.9440\n",
            "step 88360: train loss = 0.9283, val loss = 1.6298\n",
            "step 88370: train loss = 0.9177, val loss = 1.6737\n",
            "step 88380: train loss = 0.8012, val loss = 1.6018\n",
            "step 88390: train loss = 0.5994, val loss = 1.7499\n",
            "step 88400: train loss = 0.7638, val loss = 1.9006\n",
            "step 88410: train loss = 0.7584, val loss = 1.5295\n",
            "step 88420: train loss = 0.6411, val loss = 1.8129\n",
            "step 88430: train loss = 0.9073, val loss = 1.4668\n",
            "step 88440: train loss = 0.7202, val loss = 1.3164\n",
            "step 88450: train loss = 0.6775, val loss = 1.2551\n",
            "step 88460: train loss = 0.9599, val loss = 1.9705\n",
            "step 88470: train loss = 0.7812, val loss = 1.2101\n",
            "step 88480: train loss = 0.9383, val loss = 1.7386\n",
            "step 88490: train loss = 0.4059, val loss = 1.5854\n",
            "step 88500: train loss = 0.7443, val loss = 2.1068\n",
            "step 88510: train loss = 0.8224, val loss = 1.9209\n",
            "step 88520: train loss = 0.6839, val loss = 1.9601\n",
            "step 88530: train loss = 0.6884, val loss = 1.5448\n",
            "step 88540: train loss = 0.7258, val loss = 1.9670\n",
            "step 88550: train loss = 0.7861, val loss = 2.4815\n",
            "step 88560: train loss = 0.7728, val loss = 1.7396\n",
            "step 88570: train loss = 0.7619, val loss = 1.8017\n",
            "step 88580: train loss = 0.7121, val loss = 1.5290\n",
            "step 88590: train loss = 0.8157, val loss = 1.8418\n",
            "step 88600: train loss = 0.8836, val loss = 2.1390\n",
            "step 88610: train loss = 0.6540, val loss = 2.1198\n",
            "step 88620: train loss = 0.9422, val loss = 2.3242\n",
            "step 88630: train loss = 0.8835, val loss = 1.1688\n",
            "step 88640: train loss = 0.6360, val loss = 1.9178\n",
            "step 88650: train loss = 0.7900, val loss = 1.3977\n",
            "step 88660: train loss = 0.4815, val loss = 1.1761\n",
            "step 88670: train loss = 0.7494, val loss = 1.8925\n",
            "step 88680: train loss = 0.8491, val loss = 1.3175\n",
            "step 88690: train loss = 0.8075, val loss = 1.5251\n",
            "step 88700: train loss = 0.7839, val loss = 1.3409\n",
            "step 88710: train loss = 0.7385, val loss = 1.5268\n",
            "step 88720: train loss = 0.7854, val loss = 1.7490\n",
            "step 88730: train loss = 0.5661, val loss = 1.8904\n",
            "step 88740: train loss = 0.9766, val loss = 1.6489\n",
            "step 88750: train loss = 0.6844, val loss = 1.3195\n",
            "step 88760: train loss = 0.8278, val loss = 1.9327\n",
            "step 88770: train loss = 0.5682, val loss = 1.7173\n",
            "step 88780: train loss = 0.6043, val loss = 1.3128\n",
            "step 88790: train loss = 0.8607, val loss = 1.8590\n",
            "step 88800: train loss = 0.7396, val loss = 1.9918\n",
            "step 88810: train loss = 0.7968, val loss = 1.9824\n",
            "step 88820: train loss = 0.7626, val loss = 1.7427\n",
            "step 88830: train loss = 0.6565, val loss = 1.1926\n",
            "step 88840: train loss = 1.0144, val loss = 1.8421\n",
            "step 88850: train loss = 1.0263, val loss = 1.7651\n",
            "step 88860: train loss = 0.8601, val loss = 1.7818\n",
            "step 88870: train loss = 1.0257, val loss = 1.2225\n",
            "step 88880: train loss = 0.6861, val loss = 2.1337\n",
            "step 88890: train loss = 0.6214, val loss = 1.3638\n",
            "step 88900: train loss = 0.7156, val loss = 1.3115\n",
            "step 88910: train loss = 0.7554, val loss = 2.3718\n",
            "step 88920: train loss = 0.7937, val loss = 1.4109\n",
            "step 88930: train loss = 0.7839, val loss = 1.7205\n",
            "step 88940: train loss = 0.6595, val loss = 1.3194\n",
            "step 88950: train loss = 0.6762, val loss = 1.2718\n",
            "step 88960: train loss = 0.8753, val loss = 1.6245\n",
            "step 88970: train loss = 0.7587, val loss = 2.0654\n",
            "step 88980: train loss = 0.9109, val loss = 1.7753\n",
            "step 88990: train loss = 0.8167, val loss = 1.7278\n",
            "step 89000: train loss = 0.8484, val loss = 2.1241\n",
            "step 89010: train loss = 0.9643, val loss = 2.0184\n",
            "step 89020: train loss = 0.9802, val loss = 1.7214\n",
            "step 89030: train loss = 0.8673, val loss = 1.4945\n",
            "step 89040: train loss = 0.6737, val loss = 1.4400\n",
            "step 89050: train loss = 0.7931, val loss = 2.2297\n",
            "step 89060: train loss = 0.7269, val loss = 1.4326\n",
            "step 89070: train loss = 0.5708, val loss = 1.1542\n",
            "step 89080: train loss = 0.8264, val loss = 1.8616\n",
            "step 89090: train loss = 1.1617, val loss = 2.1439\n",
            "step 89100: train loss = 0.7134, val loss = 1.5944\n",
            "step 89110: train loss = 0.7855, val loss = 1.8145\n",
            "step 89120: train loss = 0.7412, val loss = 1.7888\n",
            "step 89130: train loss = 0.7769, val loss = 1.2509\n",
            "step 89140: train loss = 0.7367, val loss = 1.7317\n",
            "step 89150: train loss = 0.7898, val loss = 1.4996\n",
            "step 89160: train loss = 0.7118, val loss = 1.8303\n",
            "step 89170: train loss = 0.6809, val loss = 2.1091\n",
            "step 89180: train loss = 0.7023, val loss = 2.2134\n",
            "step 89190: train loss = 0.7888, val loss = 1.1118\n",
            "step 89200: train loss = 0.6669, val loss = 1.9013\n",
            "step 89210: train loss = 0.8874, val loss = 1.9001\n",
            "step 89220: train loss = 0.4353, val loss = 1.4491\n",
            "step 89230: train loss = 0.8740, val loss = 1.5668\n",
            "step 89240: train loss = 0.9641, val loss = 1.6063\n",
            "step 89250: train loss = 0.9935, val loss = 2.0436\n",
            "step 89260: train loss = 0.7978, val loss = 1.9586\n",
            "step 89270: train loss = 0.5342, val loss = 2.0303\n",
            "step 89280: train loss = 0.7325, val loss = 1.7359\n",
            "step 89290: train loss = 0.5640, val loss = 1.4523\n",
            "step 89300: train loss = 0.8522, val loss = 1.1786\n",
            "step 89310: train loss = 0.9574, val loss = 1.3828\n",
            "step 89320: train loss = 0.6107, val loss = 1.4234\n",
            "step 89330: train loss = 1.0161, val loss = 1.8652\n",
            "step 89340: train loss = 1.0237, val loss = 2.3141\n",
            "step 89350: train loss = 0.5972, val loss = 1.9742\n",
            "step 89360: train loss = 0.7869, val loss = 2.4479\n",
            "step 89370: train loss = 0.6367, val loss = 1.4509\n",
            "step 89380: train loss = 0.7187, val loss = 1.5157\n",
            "step 89390: train loss = 0.7706, val loss = 2.0739\n",
            "step 89400: train loss = 0.7348, val loss = 2.0900\n",
            "step 89410: train loss = 0.6984, val loss = 1.3360\n",
            "step 89420: train loss = 0.7659, val loss = 1.1710\n",
            "step 89430: train loss = 0.6056, val loss = 2.4317\n",
            "step 89440: train loss = 0.7932, val loss = 1.7864\n",
            "step 89450: train loss = 0.7501, val loss = 2.2176\n",
            "step 89460: train loss = 0.7197, val loss = 1.8070\n",
            "step 89470: train loss = 1.0990, val loss = 2.1777\n",
            "step 89480: train loss = 0.5092, val loss = 1.2840\n",
            "step 89490: train loss = 0.8555, val loss = 1.6794\n",
            "step 89500: train loss = 0.6009, val loss = 1.5738\n",
            "step 89510: train loss = 0.7949, val loss = 1.4944\n",
            "step 89520: train loss = 0.4913, val loss = 1.4903\n",
            "step 89530: train loss = 0.7519, val loss = 1.4849\n",
            "step 89540: train loss = 0.5531, val loss = 1.6538\n",
            "step 89550: train loss = 0.6961, val loss = 1.5918\n",
            "step 89560: train loss = 1.0116, val loss = 1.2335\n",
            "step 89570: train loss = 0.8439, val loss = 1.0826\n",
            "step 89580: train loss = 0.6277, val loss = 2.1634\n",
            "step 89590: train loss = 0.9575, val loss = 1.6294\n",
            "step 89600: train loss = 0.7864, val loss = 1.5419\n",
            "step 89610: train loss = 0.8913, val loss = 1.6436\n",
            "step 89620: train loss = 0.7750, val loss = 1.8047\n",
            "step 89630: train loss = 1.0103, val loss = 1.2895\n",
            "step 89640: train loss = 0.8972, val loss = 2.0925\n",
            "step 89650: train loss = 0.5927, val loss = 1.3438\n",
            "step 89660: train loss = 0.5895, val loss = 1.9945\n",
            "step 89670: train loss = 0.5605, val loss = 1.5138\n",
            "step 89680: train loss = 0.5844, val loss = 1.9057\n",
            "step 89690: train loss = 1.0500, val loss = 2.1843\n",
            "step 89700: train loss = 0.8848, val loss = 1.8765\n",
            "step 89710: train loss = 0.9184, val loss = 1.5783\n",
            "step 89720: train loss = 0.9418, val loss = 1.5394\n",
            "step 89730: train loss = 0.9645, val loss = 1.9274\n",
            "step 89740: train loss = 1.0064, val loss = 2.1178\n",
            "step 89750: train loss = 0.9457, val loss = 2.1832\n",
            "step 89760: train loss = 0.8887, val loss = 2.3235\n",
            "step 89770: train loss = 1.1065, val loss = 1.8587\n",
            "step 89780: train loss = 0.9729, val loss = 1.6150\n",
            "step 89790: train loss = 0.5145, val loss = 1.4377\n",
            "step 89800: train loss = 0.7526, val loss = 1.9248\n",
            "step 89810: train loss = 0.6946, val loss = 1.6163\n",
            "step 89820: train loss = 1.0220, val loss = 1.7651\n",
            "step 89830: train loss = 1.2224, val loss = 1.8681\n",
            "step 89840: train loss = 0.6575, val loss = 1.2697\n",
            "step 89850: train loss = 0.7220, val loss = 1.6191\n",
            "step 89860: train loss = 1.1312, val loss = 1.5588\n",
            "step 89870: train loss = 0.8328, val loss = 1.4517\n",
            "step 89880: train loss = 0.9732, val loss = 1.7860\n",
            "step 89890: train loss = 0.7217, val loss = 2.3789\n",
            "step 89900: train loss = 0.7051, val loss = 1.0856\n",
            "step 89910: train loss = 0.9185, val loss = 1.7753\n",
            "step 89920: train loss = 0.7702, val loss = 1.7768\n",
            "step 89930: train loss = 0.6234, val loss = 1.7271\n",
            "step 89940: train loss = 0.6771, val loss = 2.4087\n",
            "step 89950: train loss = 0.4354, val loss = 1.5668\n",
            "step 89960: train loss = 0.6699, val loss = 2.0585\n",
            "step 89970: train loss = 1.0378, val loss = 1.5144\n",
            "step 89980: train loss = 0.9419, val loss = 1.1340\n",
            "step 89990: train loss = 0.6185, val loss = 1.4586\n",
            "step 90000: train loss = 0.8337, val loss = 2.0245\n",
            "step 90010: train loss = 0.6418, val loss = 2.2322\n",
            "step 90020: train loss = 0.5602, val loss = 1.8424\n",
            "step 90030: train loss = 0.7190, val loss = 1.7807\n",
            "step 90040: train loss = 0.4641, val loss = 1.9002\n",
            "step 90050: train loss = 0.5655, val loss = 1.5198\n",
            "step 90060: train loss = 0.6955, val loss = 2.1198\n",
            "step 90070: train loss = 0.8966, val loss = 1.5856\n",
            "step 90080: train loss = 0.9090, val loss = 1.4704\n",
            "step 90090: train loss = 0.6264, val loss = 1.6058\n",
            "step 90100: train loss = 0.9986, val loss = 1.5151\n",
            "step 90110: train loss = 0.6722, val loss = 1.2158\n",
            "step 90120: train loss = 0.6920, val loss = 1.4144\n",
            "step 90130: train loss = 0.7415, val loss = 1.0650\n",
            "step 90140: train loss = 0.5633, val loss = 1.6145\n",
            "step 90150: train loss = 0.8855, val loss = 1.6060\n",
            "step 90160: train loss = 0.6786, val loss = 1.5202\n",
            "step 90170: train loss = 0.9973, val loss = 1.5094\n",
            "step 90180: train loss = 0.5325, val loss = 2.3573\n",
            "step 90190: train loss = 0.9602, val loss = 1.9521\n",
            "step 90200: train loss = 0.6709, val loss = 1.2363\n",
            "step 90210: train loss = 0.9534, val loss = 1.4581\n",
            "step 90220: train loss = 0.9243, val loss = 2.1879\n",
            "step 90230: train loss = 0.7892, val loss = 1.8428\n",
            "step 90240: train loss = 0.5542, val loss = 1.1295\n",
            "step 90250: train loss = 0.8617, val loss = 1.8457\n",
            "step 90260: train loss = 0.9544, val loss = 1.4627\n",
            "step 90270: train loss = 0.6993, val loss = 1.7937\n",
            "step 90280: train loss = 0.3757, val loss = 2.4377\n",
            "step 90290: train loss = 0.6237, val loss = 1.9718\n",
            "step 90300: train loss = 0.8848, val loss = 2.1084\n",
            "step 90310: train loss = 0.7193, val loss = 1.3641\n",
            "step 90320: train loss = 1.0849, val loss = 2.5551\n",
            "step 90330: train loss = 0.8067, val loss = 1.5643\n",
            "step 90340: train loss = 0.7732, val loss = 1.9665\n",
            "step 90350: train loss = 0.4269, val loss = 1.5814\n",
            "step 90360: train loss = 0.6630, val loss = 1.6193\n",
            "step 90370: train loss = 0.7544, val loss = 1.9379\n",
            "step 90380: train loss = 0.8960, val loss = 1.6270\n",
            "step 90390: train loss = 0.5315, val loss = 2.8891\n",
            "step 90400: train loss = 1.0166, val loss = 1.6269\n",
            "step 90410: train loss = 0.7313, val loss = 1.9643\n",
            "step 90420: train loss = 0.8229, val loss = 1.9713\n",
            "step 90430: train loss = 1.1911, val loss = 1.8439\n",
            "step 90440: train loss = 0.6846, val loss = 1.3958\n",
            "step 90450: train loss = 0.7742, val loss = 1.5062\n",
            "step 90460: train loss = 1.1861, val loss = 1.5901\n",
            "step 90470: train loss = 0.6855, val loss = 1.7259\n",
            "step 90480: train loss = 0.5661, val loss = 1.9224\n",
            "step 90490: train loss = 0.7427, val loss = 1.6386\n",
            "step 90500: train loss = 0.6455, val loss = 1.6516\n",
            "step 90510: train loss = 0.7267, val loss = 1.7895\n",
            "step 90520: train loss = 0.7807, val loss = 1.7550\n",
            "step 90530: train loss = 0.7679, val loss = 1.6634\n",
            "step 90540: train loss = 0.5701, val loss = 1.4361\n",
            "step 90550: train loss = 0.6560, val loss = 1.5110\n",
            "step 90560: train loss = 0.9834, val loss = 2.1099\n",
            "step 90570: train loss = 0.7682, val loss = 2.1476\n",
            "step 90580: train loss = 1.2172, val loss = 1.9187\n",
            "step 90590: train loss = 0.5426, val loss = 2.0145\n",
            "step 90600: train loss = 1.0727, val loss = 1.1045\n",
            "step 90610: train loss = 0.9593, val loss = 1.4837\n",
            "step 90620: train loss = 0.7060, val loss = 1.6118\n",
            "step 90630: train loss = 0.7231, val loss = 2.1378\n",
            "step 90640: train loss = 0.6568, val loss = 1.7458\n",
            "step 90650: train loss = 0.9836, val loss = 1.7058\n",
            "step 90660: train loss = 0.7217, val loss = 2.0695\n",
            "step 90670: train loss = 0.9049, val loss = 1.3922\n",
            "step 90680: train loss = 0.7046, val loss = 1.9312\n",
            "step 90690: train loss = 0.5985, val loss = 1.2409\n",
            "step 90700: train loss = 0.7998, val loss = 1.6751\n",
            "step 90710: train loss = 0.6363, val loss = 1.8012\n",
            "step 90720: train loss = 0.7956, val loss = 2.0265\n",
            "step 90730: train loss = 1.1198, val loss = 1.7413\n",
            "step 90740: train loss = 0.7074, val loss = 1.4116\n",
            "step 90750: train loss = 0.5215, val loss = 1.7888\n",
            "step 90760: train loss = 0.8175, val loss = 1.7526\n",
            "step 90770: train loss = 0.8883, val loss = 1.6713\n",
            "step 90780: train loss = 1.0028, val loss = 1.7127\n",
            "step 90790: train loss = 0.6742, val loss = 2.0203\n",
            "step 90800: train loss = 0.4148, val loss = 1.2981\n",
            "step 90810: train loss = 0.9373, val loss = 1.8282\n",
            "step 90820: train loss = 0.7398, val loss = 1.5958\n",
            "step 90830: train loss = 0.8733, val loss = 1.5576\n",
            "step 90840: train loss = 0.6380, val loss = 1.8372\n",
            "step 90850: train loss = 0.9123, val loss = 2.2241\n",
            "step 90860: train loss = 1.0015, val loss = 1.7824\n",
            "step 90870: train loss = 0.5960, val loss = 1.6665\n",
            "step 90880: train loss = 0.8508, val loss = 1.2133\n",
            "step 90890: train loss = 0.7237, val loss = 1.6043\n",
            "step 90900: train loss = 0.8542, val loss = 2.2088\n",
            "step 90910: train loss = 0.6943, val loss = 1.5229\n",
            "step 90920: train loss = 0.6530, val loss = 1.4318\n",
            "step 90930: train loss = 0.6040, val loss = 1.6577\n",
            "step 90940: train loss = 1.0090, val loss = 2.2255\n",
            "step 90950: train loss = 0.8699, val loss = 1.4240\n",
            "step 90960: train loss = 0.7252, val loss = 1.5047\n",
            "step 90970: train loss = 0.7488, val loss = 2.0189\n",
            "step 90980: train loss = 0.5220, val loss = 2.0824\n",
            "step 90990: train loss = 0.7229, val loss = 1.6928\n",
            "step 91000: train loss = 1.0078, val loss = 1.6942\n",
            "step 91010: train loss = 0.6764, val loss = 1.3517\n",
            "step 91020: train loss = 0.7304, val loss = 2.0478\n",
            "step 91030: train loss = 0.7312, val loss = 1.4888\n",
            "step 91040: train loss = 0.7566, val loss = 1.6785\n",
            "step 91050: train loss = 0.8756, val loss = 1.6734\n",
            "step 91060: train loss = 0.7587, val loss = 1.2180\n",
            "step 91070: train loss = 0.8075, val loss = 2.1604\n",
            "step 91080: train loss = 0.4742, val loss = 2.5838\n",
            "step 91090: train loss = 0.4014, val loss = 1.4372\n",
            "step 91100: train loss = 0.8585, val loss = 1.6248\n",
            "step 91110: train loss = 1.0285, val loss = 1.7485\n",
            "step 91120: train loss = 0.8663, val loss = 1.9827\n",
            "step 91130: train loss = 1.0113, val loss = 1.7838\n",
            "step 91140: train loss = 0.6085, val loss = 1.4578\n",
            "step 91150: train loss = 0.9899, val loss = 2.1609\n",
            "step 91160: train loss = 1.0663, val loss = 1.8116\n",
            "step 91170: train loss = 0.6452, val loss = 1.5838\n",
            "step 91180: train loss = 0.4674, val loss = 1.7154\n",
            "step 91190: train loss = 0.7445, val loss = 1.6414\n",
            "step 91200: train loss = 0.9775, val loss = 1.5977\n",
            "step 91210: train loss = 0.6214, val loss = 1.7721\n",
            "step 91220: train loss = 0.9052, val loss = 1.5763\n",
            "step 91230: train loss = 0.6924, val loss = 1.8695\n",
            "step 91240: train loss = 0.8128, val loss = 2.4104\n",
            "step 91250: train loss = 0.7497, val loss = 1.6058\n",
            "step 91260: train loss = 0.8252, val loss = 1.4807\n",
            "step 91270: train loss = 0.6702, val loss = 1.6174\n",
            "step 91280: train loss = 0.8429, val loss = 1.7481\n",
            "step 91290: train loss = 0.5239, val loss = 1.9161\n",
            "step 91300: train loss = 0.9980, val loss = 1.5950\n",
            "step 91310: train loss = 0.6479, val loss = 0.9217\n",
            "step 91320: train loss = 0.6593, val loss = 1.1770\n",
            "step 91330: train loss = 0.8529, val loss = 1.2500\n",
            "step 91340: train loss = 0.5790, val loss = 1.8630\n",
            "step 91350: train loss = 0.4460, val loss = 1.6233\n",
            "step 91360: train loss = 0.2675, val loss = 1.7659\n",
            "step 91370: train loss = 1.0061, val loss = 2.4401\n",
            "step 91380: train loss = 0.4174, val loss = 2.3820\n",
            "step 91390: train loss = 0.6074, val loss = 1.1343\n",
            "step 91400: train loss = 0.6194, val loss = 1.6086\n",
            "step 91410: train loss = 0.8570, val loss = 1.9263\n",
            "step 91420: train loss = 0.8322, val loss = 2.1807\n",
            "step 91430: train loss = 0.9564, val loss = 1.8383\n",
            "step 91440: train loss = 0.8314, val loss = 1.7379\n",
            "step 91450: train loss = 0.8328, val loss = 1.7820\n",
            "step 91460: train loss = 0.6117, val loss = 1.7735\n",
            "step 91470: train loss = 1.3858, val loss = 2.1808\n",
            "step 91480: train loss = 0.7017, val loss = 1.3874\n",
            "step 91490: train loss = 0.8767, val loss = 1.9893\n",
            "step 91500: train loss = 0.5240, val loss = 1.6369\n",
            "step 91510: train loss = 0.7607, val loss = 2.6783\n",
            "step 91520: train loss = 0.4871, val loss = 1.7139\n",
            "step 91530: train loss = 0.7112, val loss = 1.8396\n",
            "step 91540: train loss = 0.4397, val loss = 1.8675\n",
            "step 91550: train loss = 0.6105, val loss = 2.3272\n",
            "step 91560: train loss = 0.9333, val loss = 1.5182\n",
            "step 91570: train loss = 0.5926, val loss = 1.8695\n",
            "step 91580: train loss = 0.5737, val loss = 1.5839\n",
            "step 91590: train loss = 0.5730, val loss = 1.6631\n",
            "step 91600: train loss = 0.7437, val loss = 2.2009\n",
            "step 91610: train loss = 0.5221, val loss = 1.6997\n",
            "step 91620: train loss = 0.5384, val loss = 1.6812\n",
            "step 91630: train loss = 0.7028, val loss = 1.5355\n",
            "step 91640: train loss = 0.5685, val loss = 1.5761\n",
            "step 91650: train loss = 0.5614, val loss = 1.2075\n",
            "step 91660: train loss = 0.8615, val loss = 1.8253\n",
            "step 91670: train loss = 0.6998, val loss = 1.3832\n",
            "step 91680: train loss = 0.8520, val loss = 1.3837\n",
            "step 91690: train loss = 0.9656, val loss = 1.3950\n",
            "step 91700: train loss = 0.6067, val loss = 1.4412\n",
            "step 91710: train loss = 0.5589, val loss = 1.8974\n",
            "step 91720: train loss = 0.7491, val loss = 2.0811\n",
            "step 91730: train loss = 0.5315, val loss = 2.4101\n",
            "step 91740: train loss = 0.9086, val loss = 1.7895\n",
            "step 91750: train loss = 1.0519, val loss = 1.3034\n",
            "step 91760: train loss = 0.7109, val loss = 2.0752\n",
            "step 91770: train loss = 0.9049, val loss = 1.1569\n",
            "step 91780: train loss = 1.0508, val loss = 1.9562\n",
            "step 91790: train loss = 0.7867, val loss = 2.0567\n",
            "step 91800: train loss = 0.4965, val loss = 1.8330\n",
            "step 91810: train loss = 0.8846, val loss = 1.4394\n",
            "step 91820: train loss = 0.8219, val loss = 2.1391\n",
            "step 91830: train loss = 0.6828, val loss = 2.5170\n",
            "step 91840: train loss = 0.7162, val loss = 2.9477\n",
            "step 91850: train loss = 0.8554, val loss = 1.9360\n",
            "step 91860: train loss = 0.5032, val loss = 1.7116\n",
            "step 91870: train loss = 0.8047, val loss = 2.2609\n",
            "step 91880: train loss = 0.9191, val loss = 1.6487\n",
            "step 91890: train loss = 0.8807, val loss = 1.6582\n",
            "step 91900: train loss = 0.9962, val loss = 1.4705\n",
            "step 91910: train loss = 0.8488, val loss = 1.6411\n",
            "step 91920: train loss = 0.4571, val loss = 2.1670\n",
            "step 91930: train loss = 0.5755, val loss = 1.5116\n",
            "step 91940: train loss = 0.7874, val loss = 1.2304\n",
            "step 91950: train loss = 1.1830, val loss = 1.5365\n",
            "step 91960: train loss = 0.9599, val loss = 1.4916\n",
            "step 91970: train loss = 0.8088, val loss = 2.6085\n",
            "step 91980: train loss = 1.4513, val loss = 1.5158\n",
            "step 91990: train loss = 0.6588, val loss = 2.1426\n",
            "step 92000: train loss = 1.1402, val loss = 2.0212\n",
            "step 92010: train loss = 0.8306, val loss = 1.6724\n",
            "step 92020: train loss = 0.7078, val loss = 2.3535\n",
            "step 92030: train loss = 0.6532, val loss = 2.0659\n",
            "step 92040: train loss = 0.6582, val loss = 1.7074\n",
            "step 92050: train loss = 0.5892, val loss = 1.8288\n",
            "step 92060: train loss = 0.9552, val loss = 1.8356\n",
            "step 92070: train loss = 0.5084, val loss = 1.8249\n",
            "step 92080: train loss = 0.9212, val loss = 2.0787\n",
            "step 92090: train loss = 0.9905, val loss = 1.2876\n",
            "step 92100: train loss = 0.6985, val loss = 1.1496\n",
            "step 92110: train loss = 0.7906, val loss = 1.4599\n",
            "step 92120: train loss = 0.6871, val loss = 1.5470\n",
            "step 92130: train loss = 0.7746, val loss = 1.9267\n",
            "step 92140: train loss = 0.8915, val loss = 1.6409\n",
            "step 92150: train loss = 0.5731, val loss = 1.4601\n",
            "step 92160: train loss = 0.6502, val loss = 1.6390\n",
            "step 92170: train loss = 0.5609, val loss = 1.6411\n",
            "step 92180: train loss = 0.9059, val loss = 2.2049\n",
            "step 92190: train loss = 0.5767, val loss = 2.0596\n",
            "step 92200: train loss = 0.9184, val loss = 1.7731\n",
            "step 92210: train loss = 1.0941, val loss = 1.4775\n",
            "step 92220: train loss = 1.0274, val loss = 1.7083\n",
            "step 92230: train loss = 1.0151, val loss = 1.4931\n",
            "step 92240: train loss = 0.9261, val loss = 1.2409\n",
            "step 92250: train loss = 0.6575, val loss = 1.4117\n",
            "step 92260: train loss = 0.5877, val loss = 1.2432\n",
            "step 92270: train loss = 0.8342, val loss = 1.6177\n",
            "step 92280: train loss = 0.5225, val loss = 2.0425\n",
            "step 92290: train loss = 0.4956, val loss = 1.9129\n",
            "step 92300: train loss = 1.0243, val loss = 1.2358\n",
            "step 92310: train loss = 0.7109, val loss = 2.0489\n",
            "step 92320: train loss = 1.2746, val loss = 1.9040\n",
            "step 92330: train loss = 0.4992, val loss = 1.3189\n",
            "step 92340: train loss = 0.6643, val loss = 2.1841\n",
            "step 92350: train loss = 0.8718, val loss = 1.5667\n",
            "step 92360: train loss = 0.5633, val loss = 1.6198\n",
            "step 92370: train loss = 0.6838, val loss = 1.5579\n",
            "step 92380: train loss = 0.5412, val loss = 1.7906\n",
            "step 92390: train loss = 0.6411, val loss = 1.3283\n",
            "step 92400: train loss = 0.6768, val loss = 1.9131\n",
            "step 92410: train loss = 0.5422, val loss = 1.6333\n",
            "step 92420: train loss = 0.7182, val loss = 1.9030\n",
            "step 92430: train loss = 0.4962, val loss = 1.4714\n",
            "step 92440: train loss = 0.7204, val loss = 1.7836\n",
            "step 92450: train loss = 0.5481, val loss = 1.3072\n",
            "step 92460: train loss = 0.4100, val loss = 1.7530\n",
            "step 92470: train loss = 0.6053, val loss = 1.5823\n",
            "step 92480: train loss = 0.7043, val loss = 1.3319\n",
            "step 92490: train loss = 0.6099, val loss = 1.5438\n",
            "step 92500: train loss = 0.6707, val loss = 0.9414\n",
            "step 92510: train loss = 0.7436, val loss = 1.8959\n",
            "step 92520: train loss = 1.0119, val loss = 1.9311\n",
            "step 92530: train loss = 1.0242, val loss = 1.7227\n",
            "step 92540: train loss = 0.6136, val loss = 1.7802\n",
            "step 92550: train loss = 0.6767, val loss = 1.7661\n",
            "step 92560: train loss = 0.4407, val loss = 1.9900\n",
            "step 92570: train loss = 0.7772, val loss = 1.7793\n",
            "step 92580: train loss = 0.8845, val loss = 2.1105\n",
            "step 92590: train loss = 0.5260, val loss = 2.1975\n",
            "step 92600: train loss = 0.6667, val loss = 1.3501\n",
            "step 92610: train loss = 0.3957, val loss = 2.4473\n",
            "step 92620: train loss = 0.3844, val loss = 2.3034\n",
            "step 92630: train loss = 0.5817, val loss = 2.0156\n",
            "step 92640: train loss = 0.8398, val loss = 1.8732\n",
            "step 92650: train loss = 0.4939, val loss = 2.1782\n",
            "step 92660: train loss = 0.3162, val loss = 1.7980\n",
            "step 92670: train loss = 0.5524, val loss = 1.3651\n",
            "step 92680: train loss = 0.5916, val loss = 1.3523\n",
            "step 92690: train loss = 0.5877, val loss = 1.9490\n",
            "step 92700: train loss = 0.7533, val loss = 1.6857\n",
            "step 92710: train loss = 0.5473, val loss = 1.9235\n",
            "step 92720: train loss = 0.7425, val loss = 1.9114\n",
            "step 92730: train loss = 0.7721, val loss = 1.7293\n",
            "step 92740: train loss = 0.8427, val loss = 1.6572\n",
            "step 92750: train loss = 0.7755, val loss = 2.1388\n",
            "step 92760: train loss = 0.8637, val loss = 1.7647\n",
            "step 92770: train loss = 0.5913, val loss = 1.9863\n",
            "step 92780: train loss = 0.6275, val loss = 2.5309\n",
            "step 92790: train loss = 0.6038, val loss = 1.6250\n",
            "step 92800: train loss = 0.5229, val loss = 1.8572\n",
            "step 92810: train loss = 0.6154, val loss = 1.7838\n",
            "step 92820: train loss = 0.9218, val loss = 1.3000\n",
            "step 92830: train loss = 0.6972, val loss = 1.4306\n",
            "step 92840: train loss = 0.8017, val loss = 1.4939\n",
            "step 92850: train loss = 0.7677, val loss = 1.8507\n",
            "step 92860: train loss = 0.6408, val loss = 1.6456\n",
            "step 92870: train loss = 0.6048, val loss = 1.7283\n",
            "step 92880: train loss = 0.6535, val loss = 2.7105\n",
            "step 92890: train loss = 0.7642, val loss = 1.9763\n",
            "step 92900: train loss = 0.4641, val loss = 1.2531\n",
            "step 92910: train loss = 0.4486, val loss = 2.1929\n",
            "step 92920: train loss = 0.5901, val loss = 1.7420\n",
            "step 92930: train loss = 0.6842, val loss = 1.7861\n",
            "step 92940: train loss = 0.6980, val loss = 1.4979\n",
            "step 92950: train loss = 0.6650, val loss = 1.8844\n",
            "step 92960: train loss = 0.6078, val loss = 1.7112\n",
            "step 92970: train loss = 0.6258, val loss = 1.8502\n",
            "step 92980: train loss = 0.8779, val loss = 1.9388\n",
            "step 92990: train loss = 0.8531, val loss = 1.4241\n",
            "step 93000: train loss = 0.6922, val loss = 1.3312\n",
            "step 93010: train loss = 0.5436, val loss = 1.9981\n",
            "step 93020: train loss = 0.5575, val loss = 1.4757\n",
            "step 93030: train loss = 0.7517, val loss = 1.7437\n",
            "step 93040: train loss = 0.5387, val loss = 1.8050\n",
            "step 93050: train loss = 0.7474, val loss = 2.3160\n",
            "step 93060: train loss = 1.2380, val loss = 1.9331\n",
            "step 93070: train loss = 0.7468, val loss = 1.6600\n",
            "step 93080: train loss = 0.6801, val loss = 2.0974\n",
            "step 93090: train loss = 0.4142, val loss = 1.9135\n",
            "step 93100: train loss = 0.5953, val loss = 2.0624\n",
            "step 93110: train loss = 0.5887, val loss = 1.7226\n",
            "step 93120: train loss = 0.6995, val loss = 1.7236\n",
            "step 93130: train loss = 0.6182, val loss = 1.7999\n",
            "step 93140: train loss = 0.7360, val loss = 1.2439\n",
            "step 93150: train loss = 0.8187, val loss = 1.9440\n",
            "step 93160: train loss = 0.7143, val loss = 1.2272\n",
            "step 93170: train loss = 0.6043, val loss = 1.6391\n",
            "step 93180: train loss = 0.7237, val loss = 2.0674\n",
            "step 93190: train loss = 0.9023, val loss = 2.0322\n",
            "step 93200: train loss = 0.8048, val loss = 1.7578\n",
            "step 93210: train loss = 0.6640, val loss = 1.9363\n",
            "step 93220: train loss = 0.7704, val loss = 2.1613\n",
            "step 93230: train loss = 0.7214, val loss = 1.6155\n",
            "step 93240: train loss = 0.4028, val loss = 2.0653\n",
            "step 93250: train loss = 0.5292, val loss = 2.2884\n",
            "step 93260: train loss = 0.4123, val loss = 1.5466\n",
            "step 93270: train loss = 0.8979, val loss = 1.5276\n",
            "step 93280: train loss = 0.9028, val loss = 1.1290\n",
            "step 93290: train loss = 0.7675, val loss = 1.9295\n",
            "step 93300: train loss = 0.7362, val loss = 1.6011\n",
            "step 93310: train loss = 0.8614, val loss = 1.9059\n",
            "step 93320: train loss = 0.6468, val loss = 1.7404\n",
            "step 93330: train loss = 0.7223, val loss = 1.1338\n",
            "step 93340: train loss = 0.7118, val loss = 1.3104\n",
            "step 93350: train loss = 0.7347, val loss = 1.8475\n",
            "step 93360: train loss = 0.4867, val loss = 1.3752\n",
            "step 93370: train loss = 0.6413, val loss = 1.6736\n",
            "step 93380: train loss = 0.8252, val loss = 1.3803\n",
            "step 93390: train loss = 0.5702, val loss = 1.3368\n",
            "step 93400: train loss = 0.4766, val loss = 1.7959\n",
            "step 93410: train loss = 1.0397, val loss = 1.1333\n",
            "step 93420: train loss = 0.6886, val loss = 1.6202\n",
            "step 93430: train loss = 0.7224, val loss = 2.1003\n",
            "step 93440: train loss = 0.6804, val loss = 1.5790\n",
            "step 93450: train loss = 0.4192, val loss = 1.2542\n",
            "step 93460: train loss = 0.9220, val loss = 2.1521\n",
            "step 93470: train loss = 0.8284, val loss = 1.9536\n",
            "step 93480: train loss = 0.4443, val loss = 1.8345\n",
            "step 93490: train loss = 0.7340, val loss = 1.3458\n",
            "step 93500: train loss = 0.7263, val loss = 1.7227\n",
            "step 93510: train loss = 1.0376, val loss = 1.4141\n",
            "step 93520: train loss = 0.8746, val loss = 1.5582\n",
            "step 93530: train loss = 1.1663, val loss = 2.1433\n",
            "step 93540: train loss = 0.6787, val loss = 1.9341\n",
            "step 93550: train loss = 0.6929, val loss = 1.4918\n",
            "step 93560: train loss = 0.6675, val loss = 1.8687\n",
            "step 93570: train loss = 0.6552, val loss = 2.1589\n",
            "step 93580: train loss = 0.6166, val loss = 1.5957\n",
            "step 93590: train loss = 0.8104, val loss = 2.2992\n",
            "step 93600: train loss = 0.8826, val loss = 1.3267\n",
            "step 93610: train loss = 0.5414, val loss = 1.8137\n",
            "step 93620: train loss = 0.4207, val loss = 1.2056\n",
            "step 93630: train loss = 0.6854, val loss = 2.1997\n",
            "step 93640: train loss = 0.6085, val loss = 1.5298\n",
            "step 93650: train loss = 0.9098, val loss = 1.8769\n",
            "step 93660: train loss = 0.7269, val loss = 1.7701\n",
            "step 93670: train loss = 0.6387, val loss = 1.5375\n",
            "step 93680: train loss = 0.4969, val loss = 1.5504\n",
            "step 93690: train loss = 0.4994, val loss = 2.6637\n",
            "step 93700: train loss = 0.5384, val loss = 1.9280\n",
            "step 93710: train loss = 0.4728, val loss = 1.6505\n",
            "step 93720: train loss = 1.1144, val loss = 1.0724\n",
            "step 93730: train loss = 0.7349, val loss = 1.4264\n",
            "step 93740: train loss = 0.9364, val loss = 1.7241\n",
            "step 93750: train loss = 0.6237, val loss = 1.6153\n",
            "step 93760: train loss = 0.1988, val loss = 1.5843\n",
            "step 93770: train loss = 0.6213, val loss = 2.3280\n",
            "step 93780: train loss = 0.7578, val loss = 2.1922\n",
            "step 93790: train loss = 0.5698, val loss = 1.3619\n",
            "step 93800: train loss = 0.7887, val loss = 2.1539\n",
            "step 93810: train loss = 0.6582, val loss = 1.5337\n",
            "step 93820: train loss = 0.9038, val loss = 1.4637\n",
            "step 93830: train loss = 0.6829, val loss = 1.8898\n",
            "step 93840: train loss = 0.9085, val loss = 1.5779\n",
            "step 93850: train loss = 0.6235, val loss = 1.5144\n",
            "step 93860: train loss = 0.8737, val loss = 2.2225\n",
            "step 93870: train loss = 0.4125, val loss = 2.6817\n",
            "step 93880: train loss = 0.7249, val loss = 1.3107\n",
            "step 93890: train loss = 0.5004, val loss = 1.9494\n",
            "step 93900: train loss = 0.9084, val loss = 1.4281\n",
            "step 93910: train loss = 0.8803, val loss = 1.4145\n",
            "step 93920: train loss = 0.8065, val loss = 1.9421\n",
            "step 93930: train loss = 0.9166, val loss = 1.7672\n",
            "step 93940: train loss = 0.8663, val loss = 1.6688\n",
            "step 93950: train loss = 0.4301, val loss = 1.4551\n",
            "step 93960: train loss = 0.5995, val loss = 1.7777\n",
            "step 93970: train loss = 0.6610, val loss = 1.6038\n",
            "step 93980: train loss = 0.7289, val loss = 2.0949\n",
            "step 93990: train loss = 0.5563, val loss = 1.5921\n",
            "step 94000: train loss = 0.8620, val loss = 1.7133\n",
            "step 94010: train loss = 0.6763, val loss = 2.2077\n",
            "step 94020: train loss = 0.8992, val loss = 1.2988\n",
            "step 94030: train loss = 0.6496, val loss = 2.0296\n",
            "step 94040: train loss = 1.0123, val loss = 1.3478\n",
            "step 94050: train loss = 0.5113, val loss = 1.2386\n",
            "step 94060: train loss = 0.6820, val loss = 1.1352\n",
            "step 94070: train loss = 0.5163, val loss = 2.3263\n",
            "step 94080: train loss = 0.8859, val loss = 1.4459\n",
            "step 94090: train loss = 0.5386, val loss = 1.5767\n",
            "step 94100: train loss = 0.6483, val loss = 1.7581\n",
            "step 94110: train loss = 0.6633, val loss = 2.0363\n",
            "step 94120: train loss = 0.6885, val loss = 1.8715\n",
            "step 94130: train loss = 1.0953, val loss = 1.7632\n",
            "step 94140: train loss = 0.6791, val loss = 1.2187\n",
            "step 94150: train loss = 0.3495, val loss = 1.9537\n",
            "step 94160: train loss = 0.7654, val loss = 2.0941\n",
            "step 94170: train loss = 0.7810, val loss = 2.3663\n",
            "step 94180: train loss = 0.7442, val loss = 1.7229\n",
            "step 94190: train loss = 0.8092, val loss = 2.1019\n",
            "step 94200: train loss = 0.6351, val loss = 2.2367\n",
            "step 94210: train loss = 0.6888, val loss = 1.8890\n",
            "step 94220: train loss = 0.7603, val loss = 1.6674\n",
            "step 94230: train loss = 0.4142, val loss = 2.0473\n",
            "step 94240: train loss = 0.6210, val loss = 1.5902\n",
            "step 94250: train loss = 0.9090, val loss = 1.7167\n",
            "step 94260: train loss = 0.8320, val loss = 1.6025\n",
            "step 94270: train loss = 0.9270, val loss = 1.5807\n",
            "step 94280: train loss = 0.9129, val loss = 1.6443\n",
            "step 94290: train loss = 0.7224, val loss = 1.4598\n",
            "step 94300: train loss = 0.8424, val loss = 2.1135\n",
            "step 94310: train loss = 0.5065, val loss = 1.8270\n",
            "step 94320: train loss = 0.6979, val loss = 2.2470\n",
            "step 94330: train loss = 0.4954, val loss = 1.7152\n",
            "step 94340: train loss = 0.6474, val loss = 1.8527\n",
            "step 94350: train loss = 0.6012, val loss = 1.3122\n",
            "step 94360: train loss = 1.0194, val loss = 1.4987\n",
            "step 94370: train loss = 0.6975, val loss = 1.1575\n",
            "step 94380: train loss = 0.9994, val loss = 2.0957\n",
            "step 94390: train loss = 0.5976, val loss = 1.8860\n",
            "step 94400: train loss = 0.9543, val loss = 1.5692\n",
            "step 94410: train loss = 0.5414, val loss = 1.8159\n",
            "step 94420: train loss = 0.6212, val loss = 1.3023\n",
            "step 94430: train loss = 0.7202, val loss = 2.0035\n",
            "step 94440: train loss = 1.0607, val loss = 1.5653\n",
            "step 94450: train loss = 0.3712, val loss = 1.7854\n",
            "step 94460: train loss = 0.5285, val loss = 1.7429\n",
            "step 94470: train loss = 0.9811, val loss = 1.4047\n",
            "step 94480: train loss = 0.5180, val loss = 1.4229\n",
            "step 94490: train loss = 0.5576, val loss = 1.8898\n",
            "step 94500: train loss = 0.5720, val loss = 1.6461\n",
            "step 94510: train loss = 0.5610, val loss = 1.9829\n",
            "step 94520: train loss = 0.5797, val loss = 1.7647\n",
            "step 94530: train loss = 0.4378, val loss = 2.3982\n",
            "step 94540: train loss = 0.8177, val loss = 1.9752\n",
            "step 94550: train loss = 0.4466, val loss = 1.8793\n",
            "step 94560: train loss = 0.7710, val loss = 1.3194\n",
            "step 94570: train loss = 0.8815, val loss = 1.1210\n",
            "step 94580: train loss = 0.4013, val loss = 1.7482\n",
            "step 94590: train loss = 0.7812, val loss = 1.1824\n",
            "step 94600: train loss = 0.6443, val loss = 2.1870\n",
            "step 94610: train loss = 0.8907, val loss = 1.6708\n",
            "step 94620: train loss = 0.5539, val loss = 1.7255\n",
            "step 94630: train loss = 0.5039, val loss = 2.1959\n",
            "step 94640: train loss = 0.8953, val loss = 1.9325\n",
            "step 94650: train loss = 0.7699, val loss = 1.8383\n",
            "step 94660: train loss = 0.6296, val loss = 1.8428\n",
            "step 94670: train loss = 0.6215, val loss = 2.1839\n",
            "step 94680: train loss = 0.6012, val loss = 0.8198\n",
            "step 94690: train loss = 0.4555, val loss = 1.7885\n",
            "step 94700: train loss = 0.7331, val loss = 1.2451\n",
            "step 94710: train loss = 0.6669, val loss = 2.2392\n",
            "step 94720: train loss = 0.6167, val loss = 1.9184\n",
            "step 94730: train loss = 0.4361, val loss = 1.9493\n",
            "step 94740: train loss = 0.7308, val loss = 1.6958\n",
            "step 94750: train loss = 0.7722, val loss = 1.4306\n",
            "step 94760: train loss = 0.4789, val loss = 1.9599\n",
            "step 94770: train loss = 0.7068, val loss = 1.2239\n",
            "step 94780: train loss = 0.4323, val loss = 1.3568\n",
            "step 94790: train loss = 0.9362, val loss = 1.8566\n",
            "step 94800: train loss = 0.7836, val loss = 1.9573\n",
            "step 94810: train loss = 0.7082, val loss = 2.3670\n",
            "step 94820: train loss = 0.6537, val loss = 2.3800\n",
            "step 94830: train loss = 0.6247, val loss = 1.9928\n",
            "step 94840: train loss = 0.5663, val loss = 1.9713\n",
            "step 94850: train loss = 0.6670, val loss = 2.6030\n",
            "step 94860: train loss = 0.5147, val loss = 1.7494\n",
            "step 94870: train loss = 0.6980, val loss = 2.7553\n",
            "step 94880: train loss = 0.9211, val loss = 1.5576\n",
            "step 94890: train loss = 0.5347, val loss = 1.4122\n",
            "step 94900: train loss = 0.9163, val loss = 1.4677\n",
            "step 94910: train loss = 0.6488, val loss = 1.4751\n",
            "step 94920: train loss = 0.5868, val loss = 2.0043\n",
            "step 94930: train loss = 0.6823, val loss = 1.4384\n",
            "step 94940: train loss = 0.6327, val loss = 1.8821\n",
            "step 94950: train loss = 0.7660, val loss = 1.8668\n",
            "step 94960: train loss = 0.7702, val loss = 1.4534\n",
            "step 94970: train loss = 0.9200, val loss = 1.6104\n",
            "step 94980: train loss = 0.5388, val loss = 1.6920\n",
            "step 94990: train loss = 0.7517, val loss = 1.7242\n",
            "step 95000: train loss = 0.6032, val loss = 1.6520\n",
            "step 95010: train loss = 0.8953, val loss = 2.7808\n",
            "step 95020: train loss = 0.6710, val loss = 1.3686\n",
            "step 95030: train loss = 0.6113, val loss = 1.1150\n",
            "step 95040: train loss = 0.4977, val loss = 1.5671\n",
            "step 95050: train loss = 0.6970, val loss = 1.8364\n",
            "step 95060: train loss = 0.8549, val loss = 1.3365\n",
            "step 95070: train loss = 0.5202, val loss = 0.9976\n",
            "step 95080: train loss = 0.8481, val loss = 1.7105\n",
            "step 95090: train loss = 0.9197, val loss = 1.8253\n",
            "step 95100: train loss = 1.0605, val loss = 1.4307\n",
            "step 95110: train loss = 0.5468, val loss = 2.0001\n",
            "step 95120: train loss = 0.7963, val loss = 2.0242\n",
            "step 95130: train loss = 0.8216, val loss = 2.4589\n",
            "step 95140: train loss = 0.7404, val loss = 1.8416\n",
            "step 95150: train loss = 0.7132, val loss = 1.3818\n",
            "step 95160: train loss = 0.7291, val loss = 2.1372\n",
            "step 95170: train loss = 0.8399, val loss = 1.9470\n",
            "step 95180: train loss = 0.7890, val loss = 1.6393\n",
            "step 95190: train loss = 0.5883, val loss = 2.1241\n",
            "step 95200: train loss = 1.0039, val loss = 1.7323\n",
            "step 95210: train loss = 0.7605, val loss = 2.0226\n",
            "step 95220: train loss = 0.9949, val loss = 1.9502\n",
            "step 95230: train loss = 0.6947, val loss = 1.5360\n",
            "step 95240: train loss = 0.8076, val loss = 1.7616\n",
            "step 95250: train loss = 0.8034, val loss = 1.8164\n",
            "step 95260: train loss = 0.7480, val loss = 1.2703\n",
            "step 95270: train loss = 0.6695, val loss = 2.1688\n",
            "step 95280: train loss = 0.7220, val loss = 1.6154\n",
            "step 95290: train loss = 0.7052, val loss = 1.8552\n",
            "step 95300: train loss = 0.7271, val loss = 1.6135\n",
            "step 95310: train loss = 0.5027, val loss = 2.1915\n",
            "step 95320: train loss = 0.7686, val loss = 1.8593\n",
            "step 95330: train loss = 0.7696, val loss = 1.5024\n",
            "step 95340: train loss = 0.6053, val loss = 1.9553\n",
            "step 95350: train loss = 0.6312, val loss = 1.6643\n",
            "step 95360: train loss = 0.5264, val loss = 1.4614\n",
            "step 95370: train loss = 0.9179, val loss = 1.5658\n",
            "step 95380: train loss = 0.4389, val loss = 1.6305\n",
            "step 95390: train loss = 0.3981, val loss = 1.4138\n",
            "step 95400: train loss = 0.5169, val loss = 1.8109\n",
            "step 95410: train loss = 0.6045, val loss = 1.6216\n",
            "step 95420: train loss = 0.8033, val loss = 1.9442\n",
            "step 95430: train loss = 0.4593, val loss = 1.2144\n",
            "step 95440: train loss = 0.5124, val loss = 2.2151\n",
            "step 95450: train loss = 0.5784, val loss = 1.7032\n",
            "step 95460: train loss = 0.6869, val loss = 1.3831\n",
            "step 95470: train loss = 0.7181, val loss = 1.0512\n",
            "step 95480: train loss = 0.7442, val loss = 1.7527\n",
            "step 95490: train loss = 1.1396, val loss = 1.5157\n",
            "step 95500: train loss = 0.8320, val loss = 2.1118\n",
            "step 95510: train loss = 0.6647, val loss = 1.8573\n",
            "step 95520: train loss = 0.9042, val loss = 1.4543\n",
            "step 95530: train loss = 0.7521, val loss = 1.4588\n",
            "step 95540: train loss = 0.6853, val loss = 2.3307\n",
            "step 95550: train loss = 0.7025, val loss = 2.3212\n",
            "step 95560: train loss = 0.7390, val loss = 1.4231\n",
            "step 95570: train loss = 0.7613, val loss = 2.0172\n",
            "step 95580: train loss = 0.7363, val loss = 1.6504\n",
            "step 95590: train loss = 0.9155, val loss = 1.8881\n",
            "step 95600: train loss = 0.7917, val loss = 1.1755\n",
            "step 95610: train loss = 0.4749, val loss = 1.4480\n",
            "step 95620: train loss = 0.8170, val loss = 3.1534\n",
            "step 95630: train loss = 0.6329, val loss = 2.2688\n",
            "step 95640: train loss = 0.9687, val loss = 1.3282\n",
            "step 95650: train loss = 0.6319, val loss = 1.9089\n",
            "step 95660: train loss = 0.6952, val loss = 1.9825\n",
            "step 95670: train loss = 0.6454, val loss = 1.9970\n",
            "step 95680: train loss = 0.6779, val loss = 2.1017\n",
            "step 95690: train loss = 0.6597, val loss = 2.1079\n",
            "step 95700: train loss = 0.4670, val loss = 1.7766\n",
            "step 95710: train loss = 0.6967, val loss = 1.3499\n",
            "step 95720: train loss = 0.6449, val loss = 2.0558\n",
            "step 95730: train loss = 0.7137, val loss = 1.7932\n",
            "step 95740: train loss = 0.8320, val loss = 1.8617\n",
            "step 95750: train loss = 0.3977, val loss = 2.0489\n",
            "step 95760: train loss = 0.6983, val loss = 1.8397\n",
            "step 95770: train loss = 0.4312, val loss = 1.7052\n",
            "step 95780: train loss = 0.7405, val loss = 1.8222\n",
            "step 95790: train loss = 0.8761, val loss = 1.2651\n",
            "step 95800: train loss = 0.6238, val loss = 2.4457\n",
            "step 95810: train loss = 0.5429, val loss = 1.8585\n",
            "step 95820: train loss = 0.8743, val loss = 1.8311\n",
            "step 95830: train loss = 0.4696, val loss = 2.2365\n",
            "step 95840: train loss = 0.6087, val loss = 1.6100\n",
            "step 95850: train loss = 0.6058, val loss = 1.3755\n",
            "step 95860: train loss = 0.6207, val loss = 1.8217\n",
            "step 95870: train loss = 0.4659, val loss = 2.5142\n",
            "step 95880: train loss = 0.6340, val loss = 2.4370\n",
            "step 95890: train loss = 0.5376, val loss = 2.0148\n",
            "step 95900: train loss = 0.5867, val loss = 1.8849\n",
            "step 95910: train loss = 0.7018, val loss = 2.1804\n",
            "step 95920: train loss = 0.4042, val loss = 1.8130\n",
            "step 95930: train loss = 0.6505, val loss = 1.3528\n",
            "step 95940: train loss = 0.8620, val loss = 2.2259\n",
            "step 95950: train loss = 0.8924, val loss = 1.3025\n",
            "step 95960: train loss = 0.6352, val loss = 1.6342\n",
            "step 95970: train loss = 0.6133, val loss = 1.6298\n",
            "step 95980: train loss = 0.7886, val loss = 2.0853\n",
            "step 95990: train loss = 0.3183, val loss = 1.4362\n",
            "step 96000: train loss = 1.0842, val loss = 1.8634\n",
            "step 96010: train loss = 0.6000, val loss = 1.9678\n",
            "step 96020: train loss = 0.6002, val loss = 1.5020\n",
            "step 96030: train loss = 0.4910, val loss = 2.6515\n",
            "step 96040: train loss = 0.6533, val loss = 1.4004\n",
            "step 96050: train loss = 0.6766, val loss = 2.2138\n",
            "step 96060: train loss = 0.5876, val loss = 1.6018\n",
            "step 96070: train loss = 0.7420, val loss = 0.8823\n",
            "step 96080: train loss = 0.7496, val loss = 1.8318\n",
            "step 96090: train loss = 0.5198, val loss = 1.8835\n",
            "step 96100: train loss = 0.5827, val loss = 2.7211\n",
            "step 96110: train loss = 0.6656, val loss = 1.9319\n",
            "step 96120: train loss = 0.5520, val loss = 1.5736\n",
            "step 96130: train loss = 0.5225, val loss = 1.5226\n",
            "step 96140: train loss = 0.5698, val loss = 2.0314\n",
            "step 96150: train loss = 0.6526, val loss = 2.0511\n",
            "step 96160: train loss = 0.7408, val loss = 1.2823\n",
            "step 96170: train loss = 0.7388, val loss = 1.7950\n",
            "step 96180: train loss = 0.4015, val loss = 2.0747\n",
            "step 96190: train loss = 0.6339, val loss = 2.5535\n",
            "step 96200: train loss = 0.6815, val loss = 1.6385\n",
            "step 96210: train loss = 0.3539, val loss = 1.6811\n",
            "step 96220: train loss = 0.6025, val loss = 1.8036\n",
            "step 96230: train loss = 0.8855, val loss = 1.9030\n",
            "step 96240: train loss = 0.4864, val loss = 2.3625\n",
            "step 96250: train loss = 0.7340, val loss = 1.7748\n",
            "step 96260: train loss = 0.8861, val loss = 1.5060\n",
            "step 96270: train loss = 0.7537, val loss = 1.3525\n",
            "step 96280: train loss = 0.9903, val loss = 1.6489\n",
            "step 96290: train loss = 0.5846, val loss = 2.0197\n",
            "step 96300: train loss = 0.6752, val loss = 1.8394\n",
            "step 96310: train loss = 0.6723, val loss = 1.8154\n",
            "step 96320: train loss = 0.7134, val loss = 1.5392\n",
            "step 96330: train loss = 1.0043, val loss = 1.4017\n",
            "step 96340: train loss = 0.6799, val loss = 1.9801\n",
            "step 96350: train loss = 0.5094, val loss = 1.4490\n",
            "step 96360: train loss = 1.1300, val loss = 1.5002\n",
            "step 96370: train loss = 0.6297, val loss = 1.6872\n",
            "step 96380: train loss = 0.7531, val loss = 1.6452\n",
            "step 96390: train loss = 0.4460, val loss = 2.3248\n",
            "step 96400: train loss = 0.5841, val loss = 1.6575\n",
            "step 96410: train loss = 1.0562, val loss = 2.1414\n",
            "step 96420: train loss = 0.5148, val loss = 1.7462\n",
            "step 96430: train loss = 0.7155, val loss = 1.9765\n",
            "step 96440: train loss = 0.7442, val loss = 1.5561\n",
            "step 96450: train loss = 0.5019, val loss = 2.4424\n",
            "step 96460: train loss = 0.9420, val loss = 1.8677\n",
            "step 96470: train loss = 0.5079, val loss = 2.3019\n",
            "step 96480: train loss = 0.5864, val loss = 1.9183\n",
            "step 96490: train loss = 0.5270, val loss = 1.8295\n",
            "step 96500: train loss = 0.3884, val loss = 1.9628\n",
            "step 96510: train loss = 0.8163, val loss = 1.5094\n",
            "step 96520: train loss = 0.5688, val loss = 1.4838\n",
            "step 96530: train loss = 0.4989, val loss = 1.9164\n",
            "step 96540: train loss = 0.6943, val loss = 1.8381\n",
            "step 96550: train loss = 0.6704, val loss = 1.4239\n",
            "step 96560: train loss = 0.5387, val loss = 2.3497\n",
            "step 96570: train loss = 0.5760, val loss = 1.8806\n",
            "step 96580: train loss = 0.8018, val loss = 1.1429\n",
            "step 96590: train loss = 0.8203, val loss = 1.4210\n",
            "step 96600: train loss = 0.5128, val loss = 1.2114\n",
            "step 96610: train loss = 0.4950, val loss = 1.2039\n",
            "step 96620: train loss = 0.5339, val loss = 1.2710\n",
            "step 96630: train loss = 0.5585, val loss = 2.3337\n",
            "step 96640: train loss = 0.8601, val loss = 1.4707\n",
            "step 96650: train loss = 0.6123, val loss = 1.9172\n",
            "step 96660: train loss = 0.7972, val loss = 2.1629\n",
            "step 96670: train loss = 0.4499, val loss = 2.2162\n",
            "step 96680: train loss = 0.7023, val loss = 1.9539\n",
            "step 96690: train loss = 0.9143, val loss = 1.7198\n",
            "step 96700: train loss = 1.0924, val loss = 1.3173\n",
            "step 96710: train loss = 0.6500, val loss = 2.0684\n",
            "step 96720: train loss = 0.5042, val loss = 1.7096\n",
            "step 96730: train loss = 0.8599, val loss = 1.9149\n",
            "step 96740: train loss = 0.4166, val loss = 1.8298\n",
            "step 96750: train loss = 0.8674, val loss = 1.9007\n",
            "step 96760: train loss = 0.6485, val loss = 1.6511\n",
            "step 96770: train loss = 0.9025, val loss = 1.7114\n",
            "step 96780: train loss = 0.7441, val loss = 1.8073\n",
            "step 96790: train loss = 0.7369, val loss = 2.1994\n",
            "step 96800: train loss = 1.0952, val loss = 1.9364\n",
            "step 96810: train loss = 0.5700, val loss = 1.9785\n",
            "step 96820: train loss = 0.5728, val loss = 1.9989\n",
            "step 96830: train loss = 0.4510, val loss = 2.0903\n",
            "step 96840: train loss = 0.6824, val loss = 1.9583\n",
            "step 96850: train loss = 0.5423, val loss = 1.7600\n",
            "step 96860: train loss = 0.7668, val loss = 2.3262\n",
            "step 96870: train loss = 0.8387, val loss = 1.5087\n",
            "step 96880: train loss = 0.5454, val loss = 2.1256\n",
            "step 96890: train loss = 0.7215, val loss = 2.5068\n",
            "step 96900: train loss = 0.3968, val loss = 1.8519\n",
            "step 96910: train loss = 0.5356, val loss = 1.7516\n",
            "step 96920: train loss = 0.8121, val loss = 1.8422\n",
            "step 96930: train loss = 0.4612, val loss = 1.9498\n",
            "step 96940: train loss = 0.5996, val loss = 1.5130\n",
            "step 96950: train loss = 0.5454, val loss = 1.7090\n",
            "step 96960: train loss = 0.6129, val loss = 1.6999\n",
            "step 96970: train loss = 0.7263, val loss = 1.8655\n",
            "step 96980: train loss = 0.3944, val loss = 1.7588\n",
            "step 96990: train loss = 0.5749, val loss = 1.3735\n",
            "step 97000: train loss = 0.8136, val loss = 2.0521\n",
            "step 97010: train loss = 0.6513, val loss = 1.4490\n",
            "step 97020: train loss = 0.6829, val loss = 2.1924\n",
            "step 97030: train loss = 0.6338, val loss = 1.8700\n",
            "step 97040: train loss = 0.6292, val loss = 1.5453\n",
            "step 97050: train loss = 0.8824, val loss = 1.6908\n",
            "step 97060: train loss = 0.3676, val loss = 2.6991\n",
            "step 97070: train loss = 0.5621, val loss = 2.4806\n",
            "step 97080: train loss = 0.8109, val loss = 1.6592\n",
            "step 97090: train loss = 0.7535, val loss = 2.3256\n",
            "step 97100: train loss = 0.5468, val loss = 1.1270\n",
            "step 97110: train loss = 0.6739, val loss = 1.9195\n",
            "step 97120: train loss = 0.5026, val loss = 1.2024\n",
            "step 97130: train loss = 0.8566, val loss = 2.5102\n",
            "step 97140: train loss = 0.8238, val loss = 1.7381\n",
            "step 97150: train loss = 0.7485, val loss = 1.7346\n",
            "step 97160: train loss = 0.6801, val loss = 1.3783\n",
            "step 97170: train loss = 0.7819, val loss = 2.0779\n",
            "step 97180: train loss = 0.4371, val loss = 2.0218\n",
            "step 97190: train loss = 0.6633, val loss = 1.1133\n",
            "step 97200: train loss = 0.7882, val loss = 1.2418\n",
            "step 97210: train loss = 0.7645, val loss = 1.9738\n",
            "step 97220: train loss = 0.8674, val loss = 2.3502\n",
            "step 97230: train loss = 0.6917, val loss = 1.5789\n",
            "step 97240: train loss = 0.5211, val loss = 1.8334\n",
            "step 97250: train loss = 0.6802, val loss = 1.1282\n",
            "step 97260: train loss = 0.3899, val loss = 2.0327\n",
            "step 97270: train loss = 0.4831, val loss = 2.5495\n",
            "step 97280: train loss = 0.6164, val loss = 1.6611\n",
            "step 97290: train loss = 0.5563, val loss = 1.3607\n",
            "step 97300: train loss = 0.4249, val loss = 1.7653\n",
            "step 97310: train loss = 0.5354, val loss = 1.7735\n",
            "step 97320: train loss = 0.6151, val loss = 2.0633\n",
            "step 97330: train loss = 0.6282, val loss = 1.8821\n",
            "step 97340: train loss = 0.5822, val loss = 1.6325\n",
            "step 97350: train loss = 0.7214, val loss = 2.2277\n",
            "step 97360: train loss = 0.8001, val loss = 1.5654\n",
            "step 97370: train loss = 0.7095, val loss = 2.0771\n",
            "step 97380: train loss = 0.5598, val loss = 1.0883\n",
            "step 97390: train loss = 0.5249, val loss = 3.3355\n",
            "step 97400: train loss = 0.7519, val loss = 1.8561\n",
            "step 97410: train loss = 0.4190, val loss = 1.5459\n",
            "step 97420: train loss = 0.6554, val loss = 2.0081\n",
            "step 97430: train loss = 0.7522, val loss = 1.3953\n",
            "step 97440: train loss = 0.9438, val loss = 1.7255\n",
            "step 97450: train loss = 0.5216, val loss = 1.9015\n",
            "step 97460: train loss = 0.8919, val loss = 1.5642\n",
            "step 97470: train loss = 0.7688, val loss = 1.9159\n",
            "step 97480: train loss = 0.7333, val loss = 1.7746\n",
            "step 97490: train loss = 0.5496, val loss = 2.6289\n",
            "step 97500: train loss = 0.8167, val loss = 1.8690\n",
            "step 97510: train loss = 0.4535, val loss = 1.6763\n",
            "step 97520: train loss = 0.6821, val loss = 1.8373\n",
            "step 97530: train loss = 0.8034, val loss = 2.0930\n",
            "step 97540: train loss = 0.4951, val loss = 1.8210\n",
            "step 97550: train loss = 0.7669, val loss = 2.0093\n",
            "step 97560: train loss = 0.4786, val loss = 2.1525\n",
            "step 97570: train loss = 0.8947, val loss = 1.8870\n",
            "step 97580: train loss = 0.5353, val loss = 1.9080\n",
            "step 97590: train loss = 1.0379, val loss = 1.5060\n",
            "step 97600: train loss = 0.8178, val loss = 1.9915\n",
            "step 97610: train loss = 0.7752, val loss = 1.5103\n",
            "step 97620: train loss = 1.0783, val loss = 1.7157\n",
            "step 97630: train loss = 0.8517, val loss = 1.6646\n",
            "step 97640: train loss = 0.4541, val loss = 1.0995\n",
            "step 97650: train loss = 0.7536, val loss = 2.2676\n",
            "step 97660: train loss = 0.4170, val loss = 1.7752\n",
            "step 97670: train loss = 0.5427, val loss = 1.6810\n",
            "step 97680: train loss = 0.9189, val loss = 1.6501\n",
            "step 97690: train loss = 0.6368, val loss = 1.5302\n",
            "step 97700: train loss = 0.6990, val loss = 1.2982\n",
            "step 97710: train loss = 0.5369, val loss = 1.5153\n",
            "step 97720: train loss = 0.7768, val loss = 2.4500\n",
            "step 97730: train loss = 0.7694, val loss = 1.7358\n",
            "step 97740: train loss = 0.4620, val loss = 1.4980\n",
            "step 97750: train loss = 0.6524, val loss = 1.8669\n",
            "step 97760: train loss = 0.5663, val loss = 1.4718\n",
            "step 97770: train loss = 0.5951, val loss = 1.5182\n",
            "step 97780: train loss = 0.4589, val loss = 1.7618\n",
            "step 97790: train loss = 0.5381, val loss = 2.6238\n",
            "step 97800: train loss = 1.0647, val loss = 1.8945\n",
            "step 97810: train loss = 0.8777, val loss = 2.1163\n",
            "step 97820: train loss = 0.7843, val loss = 2.2847\n",
            "step 97830: train loss = 0.7220, val loss = 2.0275\n",
            "step 97840: train loss = 0.5970, val loss = 1.6331\n",
            "step 97850: train loss = 1.2228, val loss = 1.0485\n",
            "step 97860: train loss = 0.7506, val loss = 1.7048\n",
            "step 97870: train loss = 0.9875, val loss = 1.6296\n",
            "step 97880: train loss = 0.7445, val loss = 1.6650\n",
            "step 97890: train loss = 0.4819, val loss = 1.8984\n",
            "step 97900: train loss = 0.5469, val loss = 2.0457\n",
            "step 97910: train loss = 0.7188, val loss = 2.1585\n",
            "step 97920: train loss = 0.4262, val loss = 1.7360\n",
            "step 97930: train loss = 0.5732, val loss = 1.5618\n",
            "step 97940: train loss = 0.4441, val loss = 1.4318\n",
            "step 97950: train loss = 0.9581, val loss = 1.5704\n",
            "step 97960: train loss = 0.6723, val loss = 1.3152\n",
            "step 97970: train loss = 0.5421, val loss = 2.4685\n",
            "step 97980: train loss = 0.6099, val loss = 2.1797\n",
            "step 97990: train loss = 0.6223, val loss = 1.3492\n",
            "step 98000: train loss = 0.5577, val loss = 1.6341\n",
            "step 98010: train loss = 0.5896, val loss = 1.8979\n",
            "step 98020: train loss = 0.5855, val loss = 2.0597\n",
            "step 98030: train loss = 0.7454, val loss = 1.7477\n",
            "step 98040: train loss = 0.6968, val loss = 2.0205\n",
            "step 98050: train loss = 0.7961, val loss = 1.8853\n",
            "step 98060: train loss = 0.4578, val loss = 1.5639\n",
            "step 98070: train loss = 0.5651, val loss = 1.3449\n",
            "step 98080: train loss = 0.4708, val loss = 1.8349\n",
            "step 98090: train loss = 0.7777, val loss = 1.8164\n",
            "step 98100: train loss = 0.6095, val loss = 1.6338\n",
            "step 98110: train loss = 0.6112, val loss = 2.2109\n",
            "step 98120: train loss = 0.7122, val loss = 1.7243\n",
            "step 98130: train loss = 0.5585, val loss = 1.8746\n",
            "step 98140: train loss = 0.9829, val loss = 1.4917\n",
            "step 98150: train loss = 0.7506, val loss = 1.8446\n",
            "step 98160: train loss = 0.4868, val loss = 1.5326\n",
            "step 98170: train loss = 0.6573, val loss = 2.3182\n",
            "step 98180: train loss = 0.7716, val loss = 2.1660\n",
            "step 98190: train loss = 0.5983, val loss = 1.7455\n",
            "step 98200: train loss = 0.6500, val loss = 1.5921\n",
            "step 98210: train loss = 0.6364, val loss = 1.4080\n",
            "step 98220: train loss = 0.9101, val loss = 1.8255\n",
            "step 98230: train loss = 0.8275, val loss = 1.7112\n",
            "step 98240: train loss = 0.9004, val loss = 1.8074\n",
            "step 98250: train loss = 0.7478, val loss = 2.6178\n",
            "step 98260: train loss = 0.6854, val loss = 2.4698\n",
            "step 98270: train loss = 0.7678, val loss = 2.2425\n",
            "step 98280: train loss = 0.5585, val loss = 1.5684\n",
            "step 98290: train loss = 0.6675, val loss = 1.8959\n",
            "step 98300: train loss = 0.4010, val loss = 1.5539\n",
            "step 98310: train loss = 0.7863, val loss = 1.9608\n",
            "step 98320: train loss = 0.3442, val loss = 2.0091\n",
            "step 98330: train loss = 0.4856, val loss = 1.6775\n",
            "step 98340: train loss = 0.6028, val loss = 2.0951\n",
            "step 98350: train loss = 0.5584, val loss = 1.7885\n",
            "step 98360: train loss = 0.7232, val loss = 2.4298\n",
            "step 98370: train loss = 0.8608, val loss = 1.3886\n",
            "step 98380: train loss = 0.5618, val loss = 2.0524\n",
            "step 98390: train loss = 0.5025, val loss = 2.1091\n",
            "step 98400: train loss = 0.6897, val loss = 1.8441\n",
            "step 98410: train loss = 0.5521, val loss = 1.8715\n",
            "step 98420: train loss = 0.3400, val loss = 1.6490\n",
            "step 98430: train loss = 0.7401, val loss = 1.7385\n",
            "step 98440: train loss = 0.5035, val loss = 1.5152\n",
            "step 98450: train loss = 0.4540, val loss = 1.4183\n",
            "step 98460: train loss = 0.2564, val loss = 1.5646\n",
            "step 98470: train loss = 0.6954, val loss = 1.2798\n",
            "step 98480: train loss = 0.6344, val loss = 1.4738\n",
            "step 98490: train loss = 0.4473, val loss = 2.4736\n",
            "step 98500: train loss = 0.8107, val loss = 1.8639\n",
            "step 98510: train loss = 0.7107, val loss = 1.6307\n",
            "step 98520: train loss = 0.5199, val loss = 1.2319\n",
            "step 98530: train loss = 0.6005, val loss = 1.7413\n",
            "step 98540: train loss = 0.5058, val loss = 1.7380\n",
            "step 98550: train loss = 0.6282, val loss = 1.5995\n",
            "step 98560: train loss = 0.5563, val loss = 1.7663\n",
            "step 98570: train loss = 0.5675, val loss = 2.4416\n",
            "step 98580: train loss = 0.8971, val loss = 2.2562\n",
            "step 98590: train loss = 0.4473, val loss = 1.8672\n",
            "step 98600: train loss = 0.8938, val loss = 2.2573\n",
            "step 98610: train loss = 0.7187, val loss = 2.2245\n",
            "step 98620: train loss = 0.6968, val loss = 1.7905\n",
            "step 98630: train loss = 0.5954, val loss = 1.7505\n",
            "step 98640: train loss = 0.5249, val loss = 1.6318\n",
            "step 98650: train loss = 0.6546, val loss = 1.9891\n",
            "step 98660: train loss = 0.8957, val loss = 1.6200\n",
            "step 98670: train loss = 0.5816, val loss = 1.4144\n",
            "step 98680: train loss = 0.5962, val loss = 2.2310\n",
            "step 98690: train loss = 0.9835, val loss = 1.7565\n",
            "step 98700: train loss = 0.4926, val loss = 1.7811\n",
            "step 98710: train loss = 0.7526, val loss = 1.8546\n",
            "step 98720: train loss = 0.3918, val loss = 2.1095\n",
            "step 98730: train loss = 0.4704, val loss = 1.2572\n",
            "step 98740: train loss = 0.6403, val loss = 2.3002\n",
            "step 98750: train loss = 0.8545, val loss = 1.1959\n",
            "step 98760: train loss = 0.5854, val loss = 1.9335\n",
            "step 98770: train loss = 0.6668, val loss = 1.9301\n",
            "step 98780: train loss = 0.8799, val loss = 1.7400\n",
            "step 98790: train loss = 0.6846, val loss = 1.1114\n",
            "step 98800: train loss = 0.4413, val loss = 1.6613\n",
            "step 98810: train loss = 0.7107, val loss = 1.8686\n",
            "step 98820: train loss = 1.0057, val loss = 2.2710\n",
            "step 98830: train loss = 0.6456, val loss = 1.4748\n",
            "step 98840: train loss = 0.6514, val loss = 1.5427\n",
            "step 98850: train loss = 0.2700, val loss = 2.4158\n",
            "step 98860: train loss = 0.3551, val loss = 2.6684\n",
            "step 98870: train loss = 0.7030, val loss = 1.4885\n",
            "step 98880: train loss = 0.3231, val loss = 2.0697\n",
            "step 98890: train loss = 0.6483, val loss = 1.7476\n",
            "step 98900: train loss = 0.6749, val loss = 1.6728\n",
            "step 98910: train loss = 0.5727, val loss = 1.6367\n",
            "step 98920: train loss = 0.5748, val loss = 1.5698\n",
            "step 98930: train loss = 0.6073, val loss = 1.9882\n",
            "step 98940: train loss = 0.8888, val loss = 1.3337\n",
            "step 98950: train loss = 0.7525, val loss = 1.9837\n",
            "step 98960: train loss = 0.6461, val loss = 1.8536\n",
            "step 98970: train loss = 0.4384, val loss = 2.5271\n",
            "step 98980: train loss = 0.5830, val loss = 1.8071\n",
            "step 98990: train loss = 0.6114, val loss = 1.5620\n",
            "step 99000: train loss = 0.6551, val loss = 1.6045\n",
            "step 99010: train loss = 0.7512, val loss = 2.3763\n",
            "step 99020: train loss = 0.5465, val loss = 1.9243\n",
            "step 99030: train loss = 0.9783, val loss = 1.6632\n",
            "step 99040: train loss = 1.1296, val loss = 1.8944\n",
            "step 99050: train loss = 1.0767, val loss = 1.7526\n",
            "step 99060: train loss = 0.7987, val loss = 2.1518\n",
            "step 99070: train loss = 0.8089, val loss = 1.5649\n",
            "step 99080: train loss = 0.6249, val loss = 2.3330\n",
            "step 99090: train loss = 0.4314, val loss = 1.2476\n",
            "step 99100: train loss = 0.5536, val loss = 1.5147\n",
            "step 99110: train loss = 0.5179, val loss = 1.4160\n",
            "step 99120: train loss = 0.4523, val loss = 1.7696\n",
            "step 99130: train loss = 0.7366, val loss = 1.9485\n",
            "step 99140: train loss = 0.5825, val loss = 1.7632\n",
            "step 99150: train loss = 0.5194, val loss = 2.0482\n",
            "step 99160: train loss = 0.7633, val loss = 2.0480\n",
            "step 99170: train loss = 0.7252, val loss = 1.8742\n",
            "step 99180: train loss = 0.4691, val loss = 1.8359\n",
            "step 99190: train loss = 0.7489, val loss = 2.1537\n",
            "step 99200: train loss = 0.8754, val loss = 1.6809\n",
            "step 99210: train loss = 0.5753, val loss = 1.6094\n",
            "step 99220: train loss = 0.5572, val loss = 1.4563\n",
            "step 99230: train loss = 0.8966, val loss = 2.1334\n",
            "step 99240: train loss = 0.7148, val loss = 1.3897\n",
            "step 99250: train loss = 0.3057, val loss = 1.7902\n",
            "step 99260: train loss = 0.5190, val loss = 1.3402\n",
            "step 99270: train loss = 0.4739, val loss = 2.1132\n",
            "step 99280: train loss = 0.5319, val loss = 1.4825\n",
            "step 99290: train loss = 0.4492, val loss = 1.7967\n",
            "step 99300: train loss = 0.4702, val loss = 2.8696\n",
            "step 99310: train loss = 0.5639, val loss = 1.7899\n",
            "step 99320: train loss = 0.8841, val loss = 1.3018\n",
            "step 99330: train loss = 0.3239, val loss = 1.9929\n",
            "step 99340: train loss = 0.5150, val loss = 2.3323\n",
            "step 99350: train loss = 0.5440, val loss = 1.4289\n",
            "step 99360: train loss = 0.7484, val loss = 2.2584\n",
            "step 99370: train loss = 0.8308, val loss = 1.9176\n",
            "step 99380: train loss = 0.4528, val loss = 1.6901\n",
            "step 99390: train loss = 0.6691, val loss = 2.0267\n",
            "step 99400: train loss = 0.2721, val loss = 1.5601\n",
            "step 99410: train loss = 0.3503, val loss = 1.9452\n",
            "step 99420: train loss = 0.6161, val loss = 1.9113\n",
            "step 99430: train loss = 0.6510, val loss = 1.8979\n",
            "step 99440: train loss = 0.7725, val loss = 1.2382\n",
            "step 99450: train loss = 0.7369, val loss = 1.9079\n",
            "step 99460: train loss = 0.4019, val loss = 2.2810\n",
            "step 99470: train loss = 0.7155, val loss = 2.1891\n",
            "step 99480: train loss = 0.7475, val loss = 1.3816\n",
            "step 99490: train loss = 0.7035, val loss = 1.4958\n",
            "step 99500: train loss = 0.7563, val loss = 1.9395\n",
            "step 99510: train loss = 0.6522, val loss = 2.2863\n",
            "step 99520: train loss = 0.3655, val loss = 1.1526\n",
            "step 99530: train loss = 0.5557, val loss = 1.6773\n",
            "step 99540: train loss = 0.5741, val loss = 1.8819\n",
            "step 99550: train loss = 0.4776, val loss = 1.7248\n",
            "step 99560: train loss = 0.7304, val loss = 1.7700\n",
            "step 99570: train loss = 0.8685, val loss = 2.4660\n",
            "step 99580: train loss = 0.5756, val loss = 2.2166\n",
            "step 99590: train loss = 0.7708, val loss = 2.1121\n",
            "step 99600: train loss = 0.5853, val loss = 1.6392\n",
            "step 99610: train loss = 0.5865, val loss = 2.4007\n",
            "step 99620: train loss = 0.6051, val loss = 1.3672\n",
            "step 99630: train loss = 0.5322, val loss = 1.6525\n",
            "step 99640: train loss = 0.3351, val loss = 1.6529\n",
            "step 99650: train loss = 0.5773, val loss = 2.2997\n",
            "step 99660: train loss = 0.4644, val loss = 1.6982\n",
            "step 99670: train loss = 0.9468, val loss = 2.3056\n",
            "step 99680: train loss = 0.7793, val loss = 1.3316\n",
            "step 99690: train loss = 0.8787, val loss = 1.6703\n",
            "step 99700: train loss = 0.7184, val loss = 1.8869\n",
            "step 99710: train loss = 0.4703, val loss = 2.0143\n",
            "step 99720: train loss = 0.4804, val loss = 1.7790\n",
            "step 99730: train loss = 0.5892, val loss = 1.2847\n",
            "step 99740: train loss = 0.4110, val loss = 1.4299\n",
            "step 99750: train loss = 0.5351, val loss = 2.2180\n",
            "step 99760: train loss = 0.5942, val loss = 1.7570\n",
            "step 99770: train loss = 0.7465, val loss = 1.6699\n",
            "step 99780: train loss = 0.5108, val loss = 1.5222\n",
            "step 99790: train loss = 0.6980, val loss = 1.3399\n",
            "step 99800: train loss = 0.9381, val loss = 1.7190\n",
            "step 99810: train loss = 0.5002, val loss = 1.3186\n",
            "step 99820: train loss = 0.7218, val loss = 1.6499\n",
            "step 99830: train loss = 0.6172, val loss = 1.8242\n",
            "step 99840: train loss = 0.5959, val loss = 2.1778\n",
            "step 99850: train loss = 0.6534, val loss = 3.0829\n",
            "step 99860: train loss = 0.7066, val loss = 1.2420\n",
            "step 99870: train loss = 0.4598, val loss = 1.5392\n",
            "step 99880: train loss = 0.8649, val loss = 2.2595\n",
            "step 99890: train loss = 0.8804, val loss = 1.9940\n",
            "step 99900: train loss = 0.4456, val loss = 1.4603\n",
            "step 99910: train loss = 0.5242, val loss = 2.1447\n",
            "step 99920: train loss = 0.5910, val loss = 2.1178\n",
            "step 99930: train loss = 0.6517, val loss = 1.5974\n",
            "step 99940: train loss = 0.4270, val loss = 2.1444\n",
            "step 99950: train loss = 0.7139, val loss = 1.7859\n",
            "step 99960: train loss = 0.4919, val loss = 2.2394\n",
            "step 99970: train loss = 0.6359, val loss = 1.1765\n",
            "step 99980: train loss = 0.7171, val loss = 2.2135\n",
            "step 99990: train loss = 0.9906, val loss = 1.5897\n",
            "step 99999: train loss = 0.7244, val loss = 1.8407\n"
          ]
        }
      ],
      "source": [
        "# Write a small test script with a smaller model to train on the cifar10 dataset\n",
        "import torch.nn.init as init\n",
        "\n",
        "# construct model\n",
        "model = ViT(img_size = 32, \n",
        "                    patch_size = 16, \n",
        "                    in_chans=3, \n",
        "                    n_classes = 10, \n",
        "                    embed_dim = 512, \n",
        "                    depth = 12,\n",
        "                    n_heads = 8,\n",
        "                    mlp_ratio = 4.0,\n",
        "                    qkv_bias = True,\n",
        "                    attn_d = 0.2,\n",
        "                    proj_d = 0.2)\n",
        "\n",
        "print(f'Total params: {sum(p.numel() for p in model.parameters())}')\n",
        "\n",
        "# Initialize the weights\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.normal_(m.weight, std = 0.02)\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        init.ones_(m.weight)\n",
        "        init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight, mode = 'fan_in', nonlinearity = 'leaky_relu')\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "\n",
        "# load data and convert to torch tensors\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.FloatTensor(y_test).long()\n",
        "\n",
        "print(f'Training set:: input: {x_train.shape}, output: {y_train.shape}')\n",
        "print(f'Test set:: input: {x_test.shape}, output: {y_test.shape}')\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4) # use Adam optimizer with a small learning rate\n",
        "\n",
        "# Send all to device\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# define a function to get mini batches of data\n",
        "def get_batch(mode):\n",
        "    if mode == 'train':\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        xb = x_train[idx]\n",
        "        yb = y_train[idx]\n",
        "    elif mode == 'test':\n",
        "        idx = np.random.randint(0, x_test.shape[0], batch_size)\n",
        "        xb = x_test[idx]\n",
        "        yb = y_test[idx]\n",
        "    return xb,yb\n",
        "\n",
        "# Write a function to evaluate the loss on the training and test set\n",
        "def estimate_loss():\n",
        "    losses = {}\n",
        "    for mode in ['train', 'val']:\n",
        "        if mode == 'train':\n",
        "            xb,yb = get_batch('train')\n",
        "        elif mode == 'val':\n",
        "            xb,yb = get_batch('test')\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        losses[mode] = loss.item()\n",
        "    return losses\n",
        "\n",
        "# train the model\n",
        "max_iters = 100000\n",
        "batch_size = 32\n",
        "eval_interval = 10\n",
        "lossi_train = []\n",
        "lossi_test = []\n",
        "for i in range(max_iters):\n",
        "    # print training and testing loss\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n",
        "    # sample from the batch\n",
        "    xb,yb = get_batch('train')\n",
        "    # forward pass\n",
        "    logits = model(xb)\n",
        "    loss = criterion(logits, yb)\n",
        "    lossi = estimate_loss()\n",
        "    lossi_train.append(lossi['train'])\n",
        "    lossi_test.append(lossi['val'])\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "loss_fin = estimate_loss()\n",
        "print(f\"step {i}: train loss = {loss_fin['train']:.4f}, val loss = {loss_fin['val']:.4f}\")\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Tn3NqO4-j8hr",
        "outputId": "f2431c47-75f6-449a-e5ca-bec55fcdd408"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'cross entropy loss')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot the loss function\n",
        "lossi_train_mean = torch.Tensor(lossi_train).view(-1,100).mean(1)\n",
        "lossi_test_mean = torch.Tensor(lossi_test).view(-1,100).mean(1)\n",
        "plt.plot(lossi_train_mean, label = 'train')\n",
        "plt.plot(lossi_test_mean, label = 'test')\n",
        "plt.legend()\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cross entropy loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "CDFOnTlNj8hs",
        "outputId": "73cf7b2e-ec39-45b4-f7f6-eba99fd9b94e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Show an example of a picture and its prediction on the side, also plot the attention weights\n",
        "import seaborn as sns\n",
        "\n",
        "## Extract the attention weights from your model\n",
        "#layer_number = ... # Choose a specific layer number\n",
        "#head_number = ... # Choose a specific head number\n",
        "#weights = model.transformer.layer[layer_number].attention.self.weights[head_number].detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "def pred2label(pred):\n",
        "    return class_names[pred]\n",
        "\n",
        "\n",
        "def show_example():\n",
        "    xb,yb = get_batch('test')\n",
        "    logits = model(xb)\n",
        "    pred = torch.argmax(logits, dim = 1)\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(xb[0].cpu().numpy().transpose(1,2,0).astype('uint8'))\n",
        "    # Plot the attention weights as a heatmap\n",
        "    #sns.heatmap(weights, cmap=\"YlGnBu\")\n",
        "    plt.title(f'Prediction: {class_names[pred[0].item()]}, Actual: {class_names[yb[0].item()]}')\n",
        "    plt.show()\n",
        "    \n",
        "show_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7jfIzCrj8hs",
        "outputId": "be23be2e-7f08-4b4a-c10c-7e6b77046f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=10, out_features=20, bias=True)\n",
            "torch.Size([40, 20])\n",
            "torch.Size([40, 30, 20])\n",
            "torch.Size([40, 30, 40, 20])\n"
          ]
        }
      ],
      "source": [
        "# How does the linear layer behave when you have more than 1 dimension\n",
        "module = torch.nn.Linear(10,20)\n",
        "print(module)\n",
        "n_samples = 40\n",
        "input_2d = torch.Tensor(n_samples, 10)\n",
        "print(module(input_2d).shape) # now you have a n_samples X out_features tensor\n",
        "input_3d = torch.Tensor(n_samples,30, 10)\n",
        "print(module(input_3d).shape) # the middle dimension is maintained\n",
        "input_xd = torch.Tensor(n_samples, 30, 40, 10)\n",
        "print(module(input_xd).shape) # the middle dimensions are maintained\n",
        "# So the linear layer only works on the first dimension and the last dimension, the middle dimensions are maintained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyI8XMszj8ht",
        "outputId": "1dcf7722-4569-4f02-90bc-fe74d0f14ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]]) \n",
            " tensor([[0., 0., 2., 2., 2.],\n",
            "        [0., 2., 0., 2., 0.],\n",
            "        [0., 2., 2., 0., 2.]])\n",
            "tensor(15.) tensor(22.)\n"
          ]
        }
      ],
      "source": [
        "# What is dropout?\n",
        "\n",
        "p = 0.5\n",
        "module = torch.nn.Dropout(p)\n",
        "test_mat = torch.ones(3,5)\n",
        "print(test_mat, '\\n' ,module(test_mat))\n",
        "print(test_mat.sum(),module(test_mat).sum())\n",
        "# We see here that the dropout layer has now been multiplied with\n",
        "# 1/(1-p) = 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "textscan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d98cbb787251fac8b09d58c88d44135973ceca7df75589457d3db0159f5eed1c"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffdcda2bc7624974a5f95fd7d0c89c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_777f138cc3974772b62998557bb198c4",
              "IPY_MODEL_40ee4c3b73e54d828c8e9f2b101aef1f",
              "IPY_MODEL_3e195770ea71400092ca73069d4b22d5"
            ],
            "layout": "IPY_MODEL_90e39af36dd142d7a21dd51a48a9f4a6"
          }
        },
        "777f138cc3974772b62998557bb198c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c044a8d88434f09ade08433c1e4bf26",
            "placeholder": "",
            "style": "IPY_MODEL_1d621a7347b6471f94dad09f175d9667",
            "value": "100%"
          }
        },
        "40ee4c3b73e54d828c8e9f2b101aef1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2c54bf4c21d4f40ac631701c183c3ff",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50d42aa123ac46268a9af780b1b0a240",
            "value": 170498071
          }
        },
        "3e195770ea71400092ca73069d4b22d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ac9bbea99f4432b047d3dd02be99e4",
            "placeholder": "",
            "style": "IPY_MODEL_9d67a662bdcb4641ace444cda376d387",
            "value": " 170498071/170498071 [00:13&lt;00:00, 14237539.95it/s]"
          }
        },
        "90e39af36dd142d7a21dd51a48a9f4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c044a8d88434f09ade08433c1e4bf26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d621a7347b6471f94dad09f175d9667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2c54bf4c21d4f40ac631701c183c3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d42aa123ac46268a9af780b1b0a240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70ac9bbea99f4432b047d3dd02be99e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d67a662bdcb4641ace444cda376d387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}