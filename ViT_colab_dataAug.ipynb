{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyczHVHpcN8ytiH6VSQL2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d365627f5d234a749035e6bcbbb1a56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc390107f80b433ebbb319fec56c2f64",
              "IPY_MODEL_aaea76abcb1646cb8256138a2f5ffe72",
              "IPY_MODEL_f6bd1d25effa4decb615f053d881e008"
            ],
            "layout": "IPY_MODEL_794390395cca458aa243fc359c16345f"
          }
        },
        "bc390107f80b433ebbb319fec56c2f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b28e8b480d8442996b8cb37267cbc6c",
            "placeholder": "​",
            "style": "IPY_MODEL_958da589d065488bb7508d46a7667b1c",
            "value": "100%"
          }
        },
        "aaea76abcb1646cb8256138a2f5ffe72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db128b7b90b246208c376eeae26ae9bd",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99b43b16c22a4730b3ffb6f8fb20d29b",
            "value": 170498071
          }
        },
        "f6bd1d25effa4decb615f053d881e008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_971cf37e28ee47dd80529b04ef454c67",
            "placeholder": "​",
            "style": "IPY_MODEL_b32cef38f52342b4a0b2d21e8971885c",
            "value": " 170498071/170498071 [00:01&lt;00:00, 100918536.21it/s]"
          }
        },
        "794390395cca458aa243fc359c16345f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b28e8b480d8442996b8cb37267cbc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958da589d065488bb7508d46a7667b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db128b7b90b246208c376eeae26ae9bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b43b16c22a4730b3ffb6f8fb20d29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "971cf37e28ee47dd80529b04ef454c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32cef38f52342b4a0b2d21e8971885c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarajnanda2021/ViT_original/blob/main/ViT_colab_dataAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEbbicMjWw8N"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init as init\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the CIFAR-10 dataset\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "d365627f5d234a749035e6bcbbb1a56f",
            "bc390107f80b433ebbb319fec56c2f64",
            "aaea76abcb1646cb8256138a2f5ffe72",
            "f6bd1d25effa4decb615f053d881e008",
            "794390395cca458aa243fc359c16345f",
            "8b28e8b480d8442996b8cb37267cbc6c",
            "958da589d065488bb7508d46a7667b1c",
            "db128b7b90b246208c376eeae26ae9bd",
            "99b43b16c22a4730b3ffb6f8fb20d29b",
            "971cf37e28ee47dd80529b04ef454c67",
            "b32cef38f52342b4a0b2d21e8971885c"
          ]
        },
        "id": "LZG3TriHXAKP",
        "outputId": "b647f8b1-6b56-4e1e-f446-5a2d9d0efb69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d365627f5d234a749035e6bcbbb1a56f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle \n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "%matplotlib inline\n",
        "def load_batch(fpath, label_key='labels'):\n",
        "    with open(fpath, 'rb') as f:\n",
        "        if sys.version_info < (3,):\n",
        "            d = pickle.load(f)\n",
        "        else:\n",
        "            d = pickle.load(f, encoding='bytes')\n",
        "            # decode utf8\n",
        "            d_decoded = {}\n",
        "            for k, v in d.items():\n",
        "                d_decoded[k.decode('utf8')] = v\n",
        "            d = d_decoded\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "def load_cifar10():\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for i in range(1, 6):\n",
        "        fpath = './data/cifar-10-batches-py/data_batch_' + str(i)\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train.append(data)\n",
        "        y_train.append(labels)\n",
        "\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    x_test, y_test = load_batch(\"./data/cifar-10-batches-py/test_batch\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DyDWb8wYXgBC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "\n",
        "# We will now write a script for getting image patches\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "\n",
        "    '''\n",
        "    Params:\n",
        "    -------\n",
        "    img_size (int)      : size of the image (assumed to be square) \n",
        "    patch_size (int)    : size of the patch (assumed to be square)\n",
        "    in_chans (int)      : number of channels in the image (assumed to be RGB typically)\n",
        "    embed_dim (int)     : embedding dimension (will be constant throughout the network)\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "    num_patches (int)   : number of patches in the image\n",
        "    proj (nn.Conv2d)    : convolutional layer to get the patches, will have same stride as patch_size\n",
        "    '''\n",
        "    def __init__(self, img_size, patch_size, in_chans=3,embed_dim=256):\n",
        "        super().__init__() # call the super class constructor which is used to inherit the properties of the parent class\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2 # assuming square image\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride = patch_size\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ''' Parameters: \n",
        "        x (torch.Tensor): input image of shape (n_samples or batches, number of channels, height, width)\n",
        "        Returns: \n",
        "        output = n_samplex X n_patches X embed_dim shape tensor\n",
        "        '''\n",
        "        x = self.proj(x) # n_samples X embed_dim X sqrt(n_patches) X sqrt(n_patches)\n",
        "        x = x.flatten(2) # n_sample X embed_dim X n_patches\n",
        "        x = x.transpose(1, 2) # n_samples X n_patches X embed_dim (dimensions are swapped)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Let us now write the attention module\n",
        "class Attention(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding dimension, \n",
        "    n_heads (int)       : number of attention heads\n",
        "    qkv_bias (bool)     : if True, we will include a bias in the query, key and value projections\n",
        "    attn_d (float)      : Probability of dropout added to q, k and v during the training\n",
        "    proj_d (float)      : Probability of dropout added to the projection layer\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    scale (float)               : Used for norrmalizing the dot product\n",
        "    qkv (nn.Linear)             : Linear projection, which are used for performing the attention\n",
        "    proj (nn.Linear)            : Takes in the concatenated output of all attention heads and maps it further\n",
        "    attn_d, proj_d (nn.Dropout) : Dropout layers\n",
        "\n",
        "    '''\n",
        "    def __init__(self,dim, n_heads=4, qkv_bias = False, attn_d = 0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5 # scaling added as per Vaswani paper for not feeding extremely large values to softmas\n",
        "        self.qkv = nn.Linear(dim,dim * 3, bias = qkv_bias) # can be written separately too\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_d = nn.Dropout(proj_d)\n",
        "        self.attn_d = nn.Dropout(attn_d)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        ''' \n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor) : has shape (n_samples/batch, n_patches+1, dim)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor (n_samples, n_patches+1, dim)\n",
        "\n",
        "        '''\n",
        "        n_samples, n_tokens, dim = x.shape # extract shapes, tokens and dimensions from the output of the embeddings\n",
        "        if dim != self.dim:\n",
        "            raise ValueError # raise an error if dim isn't equal to the dimension set in the attention layer\n",
        "        \n",
        "        qkv = self.qkv(x) # Perform the query, key, value projections. (n_samples/batches, n_patches+1, 3*dim), the middle dimension is maintained\n",
        "\n",
        "        # Let us now reshape the qkv tensor to separate the query, key and value\n",
        "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(2,0,3,1,4) # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # Now extract the query, key and value\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2] # (n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # perform the dot product and scale the dot product\n",
        "        dot_prod = (q @ k.transpose(-2,-1)) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        # apply a softmax\n",
        "        attention = dot_prod.softmax(dim = -1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        attention = self.attn_d(attention) # apply dropout for regularization during training\n",
        "        # weighted average\n",
        "        wei = (attention @ v).transpose(1,2) # (n_samples, n_patches+1, n_heads, head_dim)\n",
        "        # flatten\n",
        "        wei = wei.flatten(2) # (n_samples, n_patches+1, dim) as dim = n_heads * head_dim\n",
        "        # we now apply the projection\n",
        "        x = self.proj(wei) # (n_samples, n_patches+1, dim)\n",
        "        x = self.proj_d(x) # apply dropout for regularization during training\n",
        "        return x\n",
        "\n",
        "    # Let us now write the MLP module\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features (int)           : embedding dimension, \n",
        "    hidden_features(int)        : dimension of the hidden layer\n",
        "    out_features (int)          : dimension of the hidden layer\n",
        "    dropout (float)     : probability of dropout\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    fc1 (nn.Linear)     : Linear projection, which are used for performing the attention\n",
        "    fc2 (nn.Linear)     : Takes in the concatenated output of all attention heads and maps it further\n",
        "    dropout (nn.Dropout): Dropout layer\n",
        "\n",
        "    '''\n",
        "    def __init__(self, in_features,hidden_features,out_features, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features) # takes in the input and maps it to the hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features) # takes in the hidden layer and maps it to the output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.GELU() # we will the GELU activation function in the paper\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) # apply the first linear projection, (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.act(x) # apply the activation function (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.fc2(x) # apply the second linear projection (n_samples, n_patches+1, out_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, out_features)\n",
        "        return x\n",
        "\n",
        "# We have everything we need to write the ViT class\n",
        "\n",
        "class Block(nn.Module):\n",
        "    ''' Transformer with Vision Token\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding\n",
        "    n_heads (int)       : number of attention heads\n",
        "    mlp_ratio (float)   : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)     : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,          : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2        :  LayerNorm layers\n",
        "    attn                : Attention layer\n",
        "    mlp                 : MLP layer\n",
        "    '''\n",
        "    def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias=True, attn_d=0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6) # division by zero is prevented and we match the props of the pretrained model\n",
        "        self.attn = Attention(dim, n_heads, qkv_bias, attn_d, proj_d)\n",
        "        self.norm2 = nn.LayerNorm(dim, eps = 1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features = dim, hidden_features = hidden_features, out_features = dim, dropout = proj_d)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x)) # add to the residual highway after performing Layernorm and attention\n",
        "        x = x + self.mlp(self.norm2(x)) # add to the residual highway after performing Layernorm and MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "# now we can write the Vision Transformer class\n",
        "class ViT(nn.Module):\n",
        "    ''' Vision Transformer\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_size (int)            : size of the input image\n",
        "    patch_size (int)            : size of the patches to be extracted from the input image\n",
        "    in_channels (int)           : number of input channels\n",
        "    num_classes (int)           : number of classes\n",
        "    embed_dim (int              : embedding dimension\n",
        "    depth (int)                 : number of transformer blocks\n",
        "    n_heads (int)               : number of attention heads per block\n",
        "    mlp_ratio (float)           : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)             : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,             : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed (nn.Conv2d)     : Convolutional embedding layer\n",
        "    pos_embed (nn.Parameter)    : learnable positional embedding\n",
        "    cls_token (nn.Parameter)    : learnable class token\n",
        "    blocks (nn.ModuleList)      : list of transformer blocks\n",
        "    norm (nn.LayerNorm)         : final LayerNorm layer\n",
        "    head (nn.Linear)            : final linear projection layer\n",
        "    '''\n",
        "    # initialize\n",
        "    def __init__(self, \n",
        "                img_size = 384, \n",
        "                patch_size = 16, \n",
        "                in_chans=3, \n",
        "                n_classes = 1000, \n",
        "                embed_dim = 768, \n",
        "                depth = 12,\n",
        "                n_heads = 12,\n",
        "                mlp_ratio = 4.0,\n",
        "                qkv_bias = True,\n",
        "                attn_d = 0.,\n",
        "                proj_d = 0.):\n",
        "        super().__init__()\n",
        "        # we will use the same image size as the pretrained model\n",
        "        self.patch_embed = PatchEmbed(\n",
        "                                        img_size = img_size,\n",
        "                                        patch_size = patch_size,\n",
        "                                        in_chans = in_chans,\n",
        "                                        embed_dim = embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)) # learnable class token\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim)) # learnable positional embedding\n",
        "        self.pos_d     = nn.Dropout(p = proj_d) # dropout layer\n",
        "        self.blocks    = nn.ModuleList(\n",
        "                            [\n",
        "                                Block( \n",
        "                                    dim = embed_dim, \n",
        "                                    n_heads = n_heads, \n",
        "                                    mlp_ratio = mlp_ratio, \n",
        "                                    qkv_bias = qkv_bias, \n",
        "                                    attn_d = attn_d, \n",
        "                                    proj_d = proj_d) for _ in range(depth)] # iteratively create the transformer blocks with same parameters\n",
        "                                    )\n",
        "        self.norm       = nn.LayerNorm(embed_dim, eps = 1e-6) # final LayerNorm layer\n",
        "        self.head       = nn.Linear(embed_dim, n_classes) # final linear projection layer    \n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        ''' Forward pass\n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor)            : n_samples X in_chans X img_size X img_size\n",
        "        Returns\n",
        "        -------\n",
        "        logits (torch.Tensor)       : n_samples X n_classes\n",
        "        '''\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x) # extract patches from the input image and turn them into patch embeddings\n",
        "        cls_tokens = self.cls_token.expand(n_samples, -1, -1) # expand the class token to match the batch size\n",
        "        # pre-append the class token to the patch embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim = 1) # n_samples X (n_patches + 1) X embed_dim\n",
        "        x = x + self.pos_embed # add the positional embedding to the patch embeddings\n",
        "        x = self.pos_d(x) # apply dropout to the embeddings\n",
        "        for block in self.blocks: # apply transformer blocks\n",
        "            x = block(x)\n",
        "        x = self.norm(x) # apply LayerNorm to the final output\n",
        "        # the shape of x now is n_samples X (n_patches + 1) X embed_dim\n",
        "        # extract the class token from the output\n",
        "        cls_token = x[:, 0] # n_samples X embed_dim\n",
        "        x = self.head(cls_token) # n_samples X n_classes\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZiCj6Ce6X41D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a small test script with a smaller model to train on the cifar10 dataset\n",
        "\n",
        "# construct model\n",
        "model = ViT(img_size = 32, \n",
        "                    patch_size = 16, \n",
        "                    in_chans=3, \n",
        "                    n_classes = 10, \n",
        "                    embed_dim = 768, \n",
        "                    depth = 12,\n",
        "                    n_heads = 8,\n",
        "                    mlp_ratio = 4.0,\n",
        "                    qkv_bias = True,\n",
        "                    attn_d = 0.2,\n",
        "                    proj_d = 0.2)\n",
        "\n",
        "print(f'Total params: {sum(p.numel() for p in model.parameters())}')\n",
        "\n",
        "\n",
        "# Make some basic image transformations\n",
        "def augment_image(img):\n",
        "    # Color jitter\n",
        "    img = img + 0.1 * torch.randn_like(img)\n",
        "    \n",
        "    # Random rotation\n",
        "    angle = 30 * torch.randn(1).item()\n",
        "    angle = angle / 180 * 3.14\n",
        "    angle = torch.tensor(angle)\n",
        "    rotation_matrix = torch.tensor([\n",
        "        [torch.cos(angle), -torch.sin(angle), 0],\n",
        "        [torch.sin(angle), torch.cos(angle), 0]\n",
        "    ]).unsqueeze(0)\n",
        "    grid = F.affine_grid(rotation_matrix, torch.Size([1, 3, 32, 32]))\n",
        "    grid = grid.to(device)\n",
        "    img = F.grid_sample(img.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros').squeeze(0)\n",
        "    \n",
        "    # Channel shuffling\n",
        "    channel_shuffle_order = torch.randperm(3)\n",
        "    img = img[channel_shuffle_order, :, :]\n",
        "\n",
        "    # Random Flipping\n",
        "    flip_dir = torch.randint(2, (1,)).item()\n",
        "    if flip_dir == 0:\n",
        "        img = torch.flip(img, [2]) # flip horizontally\n",
        "    else:\n",
        "        img = torch.flip(img, [1]) # flip vertically\n",
        "    \n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the weights\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.normal_(m.weight, std = 0.02)\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        init.ones_(m.weight)\n",
        "        init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight, mode = 'fan_in', nonlinearity = 'leaky_relu')\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "\n",
        "# load data and convert to torch tensors\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.FloatTensor(y_test).long()\n",
        "\n",
        "print(f'Training set:: input: {x_train.shape}, output: {y_train.shape}')\n",
        "print(f'Test set:: input: {x_test.shape}, output: {y_test.shape}')\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4) # use Adam optimizer with a small learning rate\n",
        "\n",
        "# Send all to device\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# define a function to get mini batches of data\n",
        "def get_batch(mode):\n",
        "    if mode == 'train':\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        xb = x_train[idx]\n",
        "        yb = y_train[idx]\n",
        "    elif mode == 'test':\n",
        "        idx = np.random.randint(0, x_test.shape[0], batch_size)\n",
        "        xb = x_test[idx]\n",
        "        yb = y_test[idx]\n",
        "    return xb,yb\n",
        "\n",
        "# Write a function to evaluate the loss on the training and test set\n",
        "def estimate_loss():\n",
        "    losses = {}\n",
        "    for mode in ['train', 'val']:\n",
        "        if mode == 'train':\n",
        "            xb,yb = get_batch('train')\n",
        "        elif mode == 'val':\n",
        "            xb,yb = get_batch('test')\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        losses[mode] = loss.item()\n",
        "    return losses\n",
        "\n",
        "# train the model\n",
        "max_iters = 100000\n",
        "batch_size = 128\n",
        "eval_interval = 10\n",
        "lossi_train = []\n",
        "lossi_test = []\n",
        "for i in range(max_iters):\n",
        "    # print training and testing loss\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n",
        "    # sample from the batch\n",
        "    imgs,yb = get_batch('train')\n",
        "    # perform a randomized data augmentation of the inputs\n",
        "    imgs = [augment_image(img) for img in imgs]\n",
        "    xb = torch.stack(imgs)\n",
        "    # forward pass\n",
        "    logits = model(xb)\n",
        "    loss = criterion(logits, yb)\n",
        "    lossi = estimate_loss()\n",
        "    lossi_train.append(lossi['train'])\n",
        "    lossi_test.append(lossi['val'])\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "loss_fin = estimate_loss()\n",
        "print(f\"step {i}: train loss = {loss_fin['train']:.4f}, val loss = {loss_fin['val']:.4f}\")\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X49TPSWgX8aA",
        "outputId": "f02c270c-981f-4da7-af9d-97e468ac2ea0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 85658890\n",
            "Training set:: input: torch.Size([50000, 3, 32, 32]), output: torch.Size([50000])\n",
            "Test set:: input: torch.Size([10000, 3, 32, 32]), output: torch.Size([10000])\n",
            "step 0: train loss = 2.4186, val loss = 2.4855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step 1960: train loss = 2.1318, val loss = 1.9994\n",
            "step 1970: train loss = 2.1117, val loss = 2.0892\n",
            "step 1980: train loss = 2.1818, val loss = 2.0321\n",
            "step 1990: train loss = 2.0615, val loss = 2.0677\n",
            "step 2000: train loss = 2.0096, val loss = 2.0582\n",
            "step 2010: train loss = 2.0161, val loss = 1.9943\n",
            "step 2020: train loss = 2.1820, val loss = 2.1047\n",
            "step 2030: train loss = 2.1130, val loss = 2.1496\n",
            "step 2040: train loss = 2.0617, val loss = 2.0873\n",
            "step 2050: train loss = 2.0687, val loss = 2.0869\n",
            "step 2060: train loss = 2.0753, val loss = 2.1469\n",
            "step 2070: train loss = 2.1116, val loss = 2.0445\n",
            "step 2080: train loss = 2.1482, val loss = 2.1606\n",
            "step 2090: train loss = 2.1652, val loss = 2.1688\n",
            "step 2100: train loss = 2.0533, val loss = 2.0328\n",
            "step 2110: train loss = 2.0492, val loss = 2.1196\n",
            "step 2120: train loss = 2.1624, val loss = 2.1265\n",
            "step 2130: train loss = 2.0917, val loss = 2.0034\n",
            "step 2140: train loss = 2.0957, val loss = 2.1561\n",
            "step 2150: train loss = 2.0779, val loss = 2.0524\n",
            "step 2160: train loss = 2.1441, val loss = 2.0754\n",
            "step 2170: train loss = 2.0554, val loss = 2.0783\n",
            "step 2180: train loss = 2.0412, val loss = 2.0858\n",
            "step 2190: train loss = 2.1642, val loss = 2.0327\n",
            "step 2200: train loss = 2.1586, val loss = 2.0428\n",
            "step 2210: train loss = 1.9959, val loss = 2.1443\n",
            "step 2220: train loss = 2.1056, val loss = 2.0936\n",
            "step 2230: train loss = 2.0773, val loss = 2.1513\n",
            "step 2240: train loss = 2.0297, val loss = 2.0136\n",
            "step 2250: train loss = 2.1698, val loss = 2.0263\n",
            "step 2260: train loss = 2.0700, val loss = 2.0616\n",
            "step 2270: train loss = 2.0971, val loss = 2.1366\n",
            "step 2280: train loss = 2.0288, val loss = 2.0357\n",
            "step 2290: train loss = 2.1073, val loss = 2.2307\n",
            "step 2300: train loss = 2.1214, val loss = 2.0965\n",
            "step 2310: train loss = 1.9756, val loss = 2.0346\n",
            "step 2320: train loss = 2.2117, val loss = 2.1306\n",
            "step 2330: train loss = 2.0605, val loss = 2.0850\n",
            "step 2340: train loss = 2.0578, val loss = 2.0408\n",
            "step 2350: train loss = 2.0044, val loss = 2.0714\n",
            "step 2360: train loss = 2.0539, val loss = 2.0032\n",
            "step 2370: train loss = 2.0307, val loss = 2.0982\n",
            "step 2380: train loss = 2.3705, val loss = 2.1540\n",
            "step 2390: train loss = 2.0357, val loss = 2.1680\n",
            "step 2400: train loss = 2.0863, val loss = 2.0846\n",
            "step 2410: train loss = 2.0791, val loss = 2.1353\n",
            "step 2420: train loss = 2.0740, val loss = 2.1186\n",
            "step 2430: train loss = 2.0026, val loss = 2.0753\n",
            "step 2440: train loss = 1.9999, val loss = 2.0059\n",
            "step 2450: train loss = 2.1605, val loss = 2.1540\n",
            "step 2460: train loss = 2.0273, val loss = 2.1287\n",
            "step 2470: train loss = 2.0854, val loss = 2.1035\n",
            "step 2480: train loss = 2.0791, val loss = 2.1231\n",
            "step 2490: train loss = 1.9813, val loss = 2.0449\n",
            "step 2500: train loss = 2.1951, val loss = 2.0611\n",
            "step 2510: train loss = 2.0291, val loss = 2.0050\n",
            "step 2520: train loss = 2.2136, val loss = 2.0872\n",
            "step 2530: train loss = 2.0407, val loss = 2.1253\n",
            "step 2540: train loss = 2.0319, val loss = 1.9510\n",
            "step 2550: train loss = 2.0563, val loss = 2.0067\n",
            "step 2560: train loss = 2.2310, val loss = 2.0512\n",
            "step 2570: train loss = 2.1443, val loss = 1.9777\n",
            "step 2580: train loss = 2.2571, val loss = 2.1045\n",
            "step 2590: train loss = 2.1692, val loss = 1.9830\n",
            "step 2600: train loss = 2.1752, val loss = 2.0117\n",
            "step 2610: train loss = 1.9595, val loss = 2.0919\n",
            "step 2620: train loss = 2.0563, val loss = 2.0892\n",
            "step 2630: train loss = 2.0197, val loss = 2.0967\n",
            "step 2640: train loss = 2.0319, val loss = 2.0261\n",
            "step 2650: train loss = 1.9807, val loss = 2.0532\n",
            "step 2660: train loss = 2.0294, val loss = 2.0711\n",
            "step 2670: train loss = 2.1147, val loss = 2.0707\n",
            "step 2680: train loss = 2.0194, val loss = 2.1111\n",
            "step 2690: train loss = 2.1248, val loss = 1.9707\n",
            "step 2700: train loss = 2.0245, val loss = 2.0161\n",
            "step 2710: train loss = 1.9937, val loss = 2.1084\n",
            "step 2720: train loss = 2.1322, val loss = 1.9301\n",
            "step 2730: train loss = 2.0143, val loss = 1.9333\n",
            "step 2740: train loss = 1.9977, val loss = 2.1027\n",
            "step 2750: train loss = 2.1941, val loss = 2.0016\n",
            "step 2760: train loss = 2.0321, val loss = 1.9838\n",
            "step 2770: train loss = 2.0399, val loss = 2.0542\n",
            "step 2780: train loss = 2.0076, val loss = 1.9735\n",
            "step 2790: train loss = 2.0528, val loss = 2.0679\n",
            "step 2800: train loss = 1.9893, val loss = 2.0764\n",
            "step 2810: train loss = 1.9944, val loss = 2.1040\n",
            "step 2820: train loss = 2.1200, val loss = 2.0178\n",
            "step 2830: train loss = 2.1398, val loss = 1.9823\n",
            "step 2840: train loss = 2.1889, val loss = 2.0910\n",
            "step 2850: train loss = 2.0583, val loss = 2.0996\n",
            "step 2860: train loss = 2.0464, val loss = 2.0023\n",
            "step 2870: train loss = 2.2350, val loss = 2.0707\n",
            "step 2880: train loss = 1.8982, val loss = 2.0901\n",
            "step 2890: train loss = 1.9961, val loss = 2.0660\n",
            "step 2900: train loss = 2.0409, val loss = 2.0572\n",
            "step 2910: train loss = 2.0604, val loss = 2.0840\n",
            "step 2920: train loss = 2.0462, val loss = 2.0302\n",
            "step 2930: train loss = 2.0548, val loss = 2.1279\n",
            "step 2940: train loss = 1.9905, val loss = 2.1776\n",
            "step 2950: train loss = 2.1282, val loss = 2.0535\n",
            "step 2960: train loss = 2.0995, val loss = 1.9545\n",
            "step 2970: train loss = 2.1152, val loss = 2.0626\n",
            "step 2980: train loss = 2.0302, val loss = 2.0254\n",
            "step 2990: train loss = 2.0740, val loss = 2.0505\n",
            "step 3000: train loss = 2.1667, val loss = 2.1685\n",
            "step 3010: train loss = 1.9246, val loss = 2.0618\n",
            "step 3020: train loss = 2.0765, val loss = 2.0767\n",
            "step 3030: train loss = 1.9367, val loss = 2.0238\n",
            "step 3040: train loss = 2.0134, val loss = 2.0616\n",
            "step 3050: train loss = 2.0331, val loss = 2.0098\n",
            "step 3060: train loss = 2.1803, val loss = 2.0078\n",
            "step 3070: train loss = 2.1159, val loss = 2.0123\n",
            "step 3080: train loss = 1.9738, val loss = 2.1778\n",
            "step 3090: train loss = 2.0530, val loss = 2.0835\n",
            "step 3100: train loss = 2.0716, val loss = 2.1345\n",
            "step 3110: train loss = 1.9925, val loss = 1.9729\n",
            "step 3120: train loss = 2.1426, val loss = 2.0544\n",
            "step 3130: train loss = 2.0425, val loss = 2.1522\n",
            "step 3140: train loss = 2.0007, val loss = 1.9037\n",
            "step 3150: train loss = 2.0355, val loss = 1.9973\n",
            "step 3160: train loss = 2.0013, val loss = 1.9746\n",
            "step 3170: train loss = 2.0548, val loss = 1.9212\n",
            "step 3180: train loss = 2.0767, val loss = 2.1829\n",
            "step 3190: train loss = 2.0999, val loss = 2.1448\n",
            "step 3200: train loss = 2.0423, val loss = 2.0358\n",
            "step 3210: train loss = 2.1019, val loss = 2.0452\n",
            "step 3220: train loss = 2.0988, val loss = 2.0597\n",
            "step 3230: train loss = 2.2505, val loss = 2.0769\n",
            "step 3240: train loss = 2.0479, val loss = 2.0701\n",
            "step 3250: train loss = 1.9729, val loss = 2.0834\n",
            "step 3260: train loss = 1.9706, val loss = 2.0903\n",
            "step 3270: train loss = 2.0837, val loss = 2.0282\n",
            "step 3280: train loss = 2.0392, val loss = 2.0426\n",
            "step 3290: train loss = 2.0514, val loss = 2.1389\n",
            "step 3300: train loss = 1.9728, val loss = 2.0136\n",
            "step 3310: train loss = 2.0082, val loss = 2.0274\n",
            "step 3320: train loss = 2.0652, val loss = 2.0578\n",
            "step 3330: train loss = 1.8710, val loss = 2.0243\n",
            "step 3340: train loss = 1.9379, val loss = 2.0657\n",
            "step 3350: train loss = 2.0362, val loss = 2.0817\n",
            "step 3360: train loss = 2.0337, val loss = 2.0843\n",
            "step 3370: train loss = 2.0860, val loss = 2.1640\n",
            "step 3380: train loss = 2.0011, val loss = 2.0569\n",
            "step 3390: train loss = 1.9808, val loss = 2.0805\n",
            "step 3400: train loss = 2.1540, val loss = 2.1627\n",
            "step 3410: train loss = 2.0226, val loss = 1.9951\n",
            "step 3420: train loss = 2.0714, val loss = 2.1802\n",
            "step 3430: train loss = 1.9710, val loss = 2.0060\n",
            "step 3440: train loss = 2.0743, val loss = 2.0362\n",
            "step 3450: train loss = 2.0882, val loss = 2.0163\n",
            "step 3460: train loss = 2.0506, val loss = 1.9399\n",
            "step 3470: train loss = 2.0585, val loss = 1.8832\n",
            "step 3480: train loss = 1.9285, val loss = 1.8838\n",
            "step 3490: train loss = 1.9006, val loss = 2.0680\n",
            "step 3500: train loss = 2.0055, val loss = 2.1716\n",
            "step 3510: train loss = 1.9590, val loss = 2.0181\n",
            "step 3520: train loss = 2.1551, val loss = 2.2741\n",
            "step 3530: train loss = 2.0475, val loss = 2.1144\n",
            "step 3540: train loss = 2.1567, val loss = 1.9614\n",
            "step 3550: train loss = 1.9585, val loss = 1.9932\n",
            "step 3560: train loss = 2.0652, val loss = 2.0825\n",
            "step 3570: train loss = 1.9659, val loss = 2.0362\n",
            "step 3580: train loss = 2.1517, val loss = 2.0480\n",
            "step 3590: train loss = 1.9820, val loss = 1.9354\n",
            "step 3600: train loss = 2.2171, val loss = 2.1491\n",
            "step 3610: train loss = 1.9887, val loss = 2.1122\n",
            "step 3620: train loss = 1.9878, val loss = 2.1188\n",
            "step 3630: train loss = 2.0326, val loss = 2.0341\n",
            "step 3640: train loss = 2.1131, val loss = 2.0682\n",
            "step 3650: train loss = 1.9859, val loss = 2.1172\n",
            "step 3660: train loss = 2.0716, val loss = 2.1637\n",
            "step 3670: train loss = 2.0460, val loss = 1.9887\n",
            "step 3680: train loss = 2.1028, val loss = 1.9301\n",
            "step 3690: train loss = 1.9897, val loss = 2.0000\n",
            "step 3700: train loss = 2.0326, val loss = 2.0878\n",
            "step 3710: train loss = 2.0543, val loss = 2.0639\n",
            "step 3720: train loss = 1.9663, val loss = 2.0376\n",
            "step 3730: train loss = 1.9721, val loss = 2.0932\n",
            "step 3740: train loss = 1.9711, val loss = 1.9962\n",
            "step 3750: train loss = 2.0845, val loss = 1.9920\n",
            "step 3760: train loss = 2.2473, val loss = 2.0962\n",
            "step 3770: train loss = 1.9132, val loss = 2.0048\n",
            "step 3780: train loss = 1.9790, val loss = 1.8964\n",
            "step 3790: train loss = 2.0523, val loss = 1.9816\n",
            "step 3800: train loss = 2.0221, val loss = 2.1394\n",
            "step 3810: train loss = 2.2207, val loss = 2.1072\n",
            "step 3820: train loss = 2.1018, val loss = 2.0070\n",
            "step 3830: train loss = 2.1022, val loss = 2.0609\n",
            "step 3840: train loss = 2.0386, val loss = 1.9531\n",
            "step 3850: train loss = 1.9153, val loss = 2.0693\n",
            "step 3860: train loss = 1.9387, val loss = 1.9975\n",
            "step 3870: train loss = 1.9505, val loss = 1.9868\n",
            "step 3880: train loss = 2.0240, val loss = 1.9617\n",
            "step 3890: train loss = 1.9817, val loss = 2.0508\n",
            "step 3900: train loss = 2.0510, val loss = 2.0102\n",
            "step 3910: train loss = 1.9879, val loss = 2.1201\n",
            "step 3920: train loss = 2.1076, val loss = 1.9855\n",
            "step 3930: train loss = 2.0229, val loss = 2.1367\n",
            "step 3940: train loss = 2.0874, val loss = 2.1149\n",
            "step 3950: train loss = 2.0510, val loss = 2.1836\n",
            "step 3960: train loss = 2.0128, val loss = 2.1123\n",
            "step 3970: train loss = 1.9138, val loss = 2.0305\n",
            "step 3980: train loss = 2.1213, val loss = 1.9335\n",
            "step 3990: train loss = 2.0098, val loss = 1.9291\n",
            "step 4000: train loss = 2.0760, val loss = 1.9742\n",
            "step 4010: train loss = 1.8919, val loss = 2.1303\n",
            "step 4020: train loss = 1.9420, val loss = 2.2104\n",
            "step 4030: train loss = 2.0821, val loss = 1.9769\n",
            "step 4040: train loss = 2.1038, val loss = 2.0882\n",
            "step 4050: train loss = 2.0195, val loss = 1.9690\n",
            "step 4060: train loss = 1.9810, val loss = 1.8985\n",
            "step 4070: train loss = 2.0937, val loss = 2.0384\n",
            "step 4080: train loss = 1.9436, val loss = 2.0249\n",
            "step 4090: train loss = 2.0835, val loss = 1.9087\n",
            "step 4100: train loss = 2.0282, val loss = 2.1091\n",
            "step 4110: train loss = 2.1091, val loss = 2.1311\n",
            "step 4120: train loss = 2.0555, val loss = 2.0176\n",
            "step 4130: train loss = 2.0595, val loss = 1.9697\n",
            "step 4140: train loss = 2.2471, val loss = 2.1905\n",
            "step 4150: train loss = 2.0496, val loss = 1.9628\n",
            "step 4160: train loss = 2.0792, val loss = 2.1535\n",
            "step 4170: train loss = 2.0398, val loss = 1.9252\n",
            "step 4180: train loss = 2.1198, val loss = 2.1242\n",
            "step 4190: train loss = 1.9062, val loss = 2.0339\n",
            "step 4200: train loss = 2.0121, val loss = 2.0322\n",
            "step 4210: train loss = 2.0795, val loss = 2.0567\n",
            "step 4220: train loss = 2.1079, val loss = 2.0616\n",
            "step 4230: train loss = 2.0882, val loss = 2.0567\n",
            "step 4240: train loss = 2.0180, val loss = 2.0919\n",
            "step 4250: train loss = 2.0350, val loss = 2.0906\n",
            "step 4260: train loss = 1.8409, val loss = 2.0038\n",
            "step 4270: train loss = 2.0848, val loss = 2.0234\n",
            "step 4280: train loss = 1.9749, val loss = 1.9431\n",
            "step 4290: train loss = 1.9898, val loss = 2.0636\n",
            "step 4300: train loss = 2.1496, val loss = 2.1301\n",
            "step 4310: train loss = 1.9721, val loss = 2.1235\n",
            "step 4320: train loss = 2.1194, val loss = 1.9760\n",
            "step 4330: train loss = 2.1162, val loss = 2.1194\n",
            "step 4340: train loss = 2.1844, val loss = 1.9463\n",
            "step 4350: train loss = 1.9995, val loss = 1.9865\n",
            "step 4360: train loss = 1.9879, val loss = 2.1232\n",
            "step 4370: train loss = 2.0562, val loss = 1.9464\n",
            "step 4380: train loss = 2.1301, val loss = 2.0237\n",
            "step 4390: train loss = 2.0937, val loss = 1.9241\n",
            "step 4400: train loss = 2.0485, val loss = 1.9365\n",
            "step 4410: train loss = 2.0541, val loss = 1.9222\n",
            "step 4420: train loss = 2.0256, val loss = 2.0484\n",
            "step 4430: train loss = 1.9653, val loss = 2.1512\n",
            "step 4440: train loss = 2.0621, val loss = 1.9580\n",
            "step 4450: train loss = 1.9733, val loss = 2.0115\n",
            "step 4460: train loss = 1.9272, val loss = 2.1393\n",
            "step 4470: train loss = 1.9887, val loss = 2.1015\n",
            "step 4480: train loss = 2.0203, val loss = 2.0012\n",
            "step 4490: train loss = 1.9233, val loss = 1.9531\n",
            "step 4500: train loss = 1.9233, val loss = 1.9663\n",
            "step 4510: train loss = 2.0499, val loss = 2.1330\n",
            "step 4520: train loss = 2.1050, val loss = 2.0827\n",
            "step 4530: train loss = 2.0540, val loss = 2.0319\n",
            "step 4540: train loss = 1.9941, val loss = 1.9510\n",
            "step 4550: train loss = 1.9815, val loss = 1.9502\n",
            "step 4560: train loss = 2.0944, val loss = 2.2279\n",
            "step 4570: train loss = 2.0595, val loss = 2.0397\n",
            "step 4580: train loss = 1.9329, val loss = 2.0152\n",
            "step 4590: train loss = 2.0179, val loss = 1.9407\n",
            "step 4600: train loss = 1.9387, val loss = 2.0621\n",
            "step 4610: train loss = 1.9778, val loss = 1.9883\n",
            "step 4620: train loss = 2.0740, val loss = 2.0249\n",
            "step 4630: train loss = 2.0229, val loss = 2.1093\n",
            "step 4640: train loss = 2.1035, val loss = 2.0012\n",
            "step 4650: train loss = 2.1217, val loss = 2.0146\n",
            "step 4660: train loss = 2.1021, val loss = 2.0956\n",
            "step 4670: train loss = 2.0253, val loss = 2.0117\n",
            "step 4680: train loss = 2.0549, val loss = 1.9999\n",
            "step 4690: train loss = 2.0508, val loss = 2.0205\n",
            "step 4700: train loss = 1.9715, val loss = 2.1048\n",
            "step 4710: train loss = 2.0189, val loss = 1.9707\n",
            "step 4720: train loss = 2.0829, val loss = 2.0037\n",
            "step 4730: train loss = 1.9119, val loss = 2.0205\n",
            "step 4740: train loss = 1.9670, val loss = 1.9536\n",
            "step 4750: train loss = 1.9607, val loss = 1.9196\n",
            "step 4760: train loss = 2.0045, val loss = 1.9961\n",
            "step 4770: train loss = 2.0695, val loss = 2.1657\n",
            "step 4780: train loss = 2.0253, val loss = 2.0296\n",
            "step 4790: train loss = 2.1180, val loss = 2.0163\n",
            "step 4800: train loss = 2.0314, val loss = 2.0710\n",
            "step 4810: train loss = 2.0637, val loss = 2.0719\n",
            "step 4820: train loss = 2.0394, val loss = 2.0182\n",
            "step 4830: train loss = 2.0631, val loss = 2.0134\n",
            "step 4840: train loss = 2.0243, val loss = 2.0496\n",
            "step 4850: train loss = 2.0098, val loss = 2.1059\n",
            "step 4860: train loss = 1.9607, val loss = 2.0359\n",
            "step 4870: train loss = 2.0166, val loss = 1.9657\n",
            "step 4880: train loss = 1.9476, val loss = 2.1191\n",
            "step 4890: train loss = 1.9197, val loss = 1.9890\n",
            "step 4900: train loss = 2.1855, val loss = 2.0926\n",
            "step 4910: train loss = 2.0423, val loss = 1.9817\n",
            "step 4920: train loss = 2.0716, val loss = 2.0479\n",
            "step 4930: train loss = 1.9345, val loss = 2.0139\n",
            "step 4940: train loss = 2.1348, val loss = 2.1361\n",
            "step 4950: train loss = 2.1987, val loss = 2.0342\n",
            "step 4960: train loss = 2.0225, val loss = 2.0963\n",
            "step 4970: train loss = 1.9655, val loss = 2.0172\n",
            "step 4980: train loss = 2.0158, val loss = 2.0150\n",
            "step 4990: train loss = 1.9773, val loss = 1.9558\n",
            "step 5000: train loss = 1.9480, val loss = 2.0068\n",
            "step 5010: train loss = 2.1329, val loss = 2.1025\n",
            "step 5020: train loss = 2.0518, val loss = 2.0542\n",
            "step 5030: train loss = 1.9134, val loss = 2.0353\n",
            "step 5040: train loss = 2.1050, val loss = 2.0290\n",
            "step 5050: train loss = 1.9564, val loss = 1.8892\n",
            "step 5060: train loss = 1.9231, val loss = 2.0246\n",
            "step 5070: train loss = 1.9727, val loss = 2.0325\n",
            "step 5080: train loss = 2.0703, val loss = 2.1073\n",
            "step 5090: train loss = 2.0213, val loss = 1.9655\n",
            "step 5100: train loss = 2.0529, val loss = 2.1605\n",
            "step 5110: train loss = 1.9659, val loss = 1.9872\n",
            "step 5120: train loss = 2.0691, val loss = 1.9666\n",
            "step 5130: train loss = 1.9817, val loss = 2.0790\n",
            "step 5140: train loss = 1.9598, val loss = 2.0234\n",
            "step 5150: train loss = 2.0850, val loss = 1.9635\n",
            "step 5160: train loss = 2.0485, val loss = 2.0990\n",
            "step 5170: train loss = 2.0724, val loss = 2.0591\n",
            "step 5180: train loss = 2.0657, val loss = 1.9880\n",
            "step 5190: train loss = 2.0463, val loss = 2.0599\n",
            "step 5200: train loss = 1.9843, val loss = 2.0546\n",
            "step 5210: train loss = 1.8632, val loss = 1.9586\n",
            "step 5220: train loss = 2.0211, val loss = 2.0474\n",
            "step 5230: train loss = 2.0889, val loss = 1.9413\n",
            "step 5240: train loss = 2.0272, val loss = 1.9983\n",
            "step 5250: train loss = 2.1549, val loss = 2.0545\n",
            "step 5260: train loss = 1.9645, val loss = 1.8994\n",
            "step 5270: train loss = 1.9943, val loss = 2.0762\n",
            "step 5280: train loss = 1.9095, val loss = 1.9881\n",
            "step 5290: train loss = 2.1758, val loss = 2.0451\n",
            "step 5300: train loss = 2.0007, val loss = 2.0059\n",
            "step 5310: train loss = 1.9895, val loss = 1.9624\n",
            "step 5320: train loss = 2.0771, val loss = 2.0186\n",
            "step 5330: train loss = 2.0255, val loss = 2.1215\n",
            "step 5340: train loss = 1.9226, val loss = 1.9645\n",
            "step 5350: train loss = 1.9903, val loss = 1.9385\n",
            "step 5360: train loss = 1.9625, val loss = 2.0759\n",
            "step 5370: train loss = 1.9836, val loss = 2.0893\n",
            "step 5380: train loss = 2.0367, val loss = 2.0825\n",
            "step 5390: train loss = 2.1555, val loss = 1.9545\n",
            "step 5400: train loss = 2.0394, val loss = 2.0921\n",
            "step 5410: train loss = 2.0031, val loss = 2.0318\n",
            "step 5420: train loss = 2.0214, val loss = 1.9643\n",
            "step 5430: train loss = 1.9920, val loss = 2.0177\n",
            "step 5440: train loss = 2.0067, val loss = 2.0152\n",
            "step 5450: train loss = 2.0119, val loss = 1.9479\n",
            "step 5460: train loss = 1.9739, val loss = 1.9497\n",
            "step 5470: train loss = 1.9791, val loss = 2.0895\n",
            "step 5480: train loss = 1.9095, val loss = 1.9128\n",
            "step 5490: train loss = 2.0016, val loss = 1.9571\n",
            "step 5500: train loss = 2.0468, val loss = 1.9060\n",
            "step 5510: train loss = 1.9686, val loss = 2.1097\n",
            "step 5520: train loss = 1.8473, val loss = 1.9800\n",
            "step 5530: train loss = 2.0041, val loss = 2.1132\n",
            "step 5540: train loss = 1.9948, val loss = 1.9601\n",
            "step 5550: train loss = 1.9627, val loss = 2.0626\n",
            "step 5560: train loss = 1.9439, val loss = 2.0189\n",
            "step 5570: train loss = 2.0598, val loss = 2.0258\n",
            "step 5580: train loss = 2.0019, val loss = 1.9837\n",
            "step 5590: train loss = 2.0853, val loss = 2.0768\n",
            "step 5600: train loss = 2.0663, val loss = 2.1012\n",
            "step 5610: train loss = 1.9666, val loss = 2.0772\n",
            "step 5620: train loss = 2.2000, val loss = 2.0763\n",
            "step 5630: train loss = 2.0458, val loss = 1.9645\n",
            "step 5640: train loss = 2.0782, val loss = 2.0509\n",
            "step 5650: train loss = 1.9212, val loss = 1.9972\n",
            "step 5660: train loss = 2.0353, val loss = 2.0214\n",
            "step 5670: train loss = 2.0029, val loss = 1.9893\n",
            "step 5680: train loss = 2.1188, val loss = 2.0597\n",
            "step 5690: train loss = 1.9800, val loss = 2.0774\n",
            "step 5700: train loss = 2.1201, val loss = 1.9618\n",
            "step 5710: train loss = 2.0516, val loss = 1.9230\n",
            "step 5720: train loss = 2.1148, val loss = 2.1683\n",
            "step 5730: train loss = 1.9082, val loss = 1.9479\n",
            "step 5740: train loss = 2.0198, val loss = 1.9488\n",
            "step 5750: train loss = 2.0351, val loss = 2.0907\n",
            "step 5760: train loss = 2.0630, val loss = 1.9789\n",
            "step 5770: train loss = 1.9734, val loss = 2.2827\n",
            "step 5780: train loss = 2.1022, val loss = 2.1749\n",
            "step 5790: train loss = 2.0257, val loss = 2.0025\n",
            "step 5800: train loss = 1.9765, val loss = 2.0278\n",
            "step 5810: train loss = 1.9704, val loss = 2.0974\n",
            "step 5820: train loss = 1.9441, val loss = 1.9622\n",
            "step 5830: train loss = 2.1568, val loss = 2.0089\n",
            "step 5840: train loss = 2.0978, val loss = 1.9886\n",
            "step 5850: train loss = 2.0393, val loss = 2.0163\n",
            "step 5860: train loss = 1.9419, val loss = 1.8955\n",
            "step 5870: train loss = 2.0011, val loss = 2.1170\n",
            "step 5880: train loss = 1.9755, val loss = 2.1181\n",
            "step 5890: train loss = 2.1110, val loss = 1.9804\n",
            "step 5900: train loss = 1.8421, val loss = 1.9859\n",
            "step 5910: train loss = 2.1405, val loss = 2.0647\n",
            "step 5920: train loss = 1.9653, val loss = 1.8824\n",
            "step 5930: train loss = 1.9061, val loss = 1.9623\n",
            "step 5940: train loss = 1.8678, val loss = 2.0073\n",
            "step 5950: train loss = 2.0462, val loss = 2.0820\n",
            "step 5960: train loss = 2.0294, val loss = 1.9940\n",
            "step 5970: train loss = 2.0307, val loss = 2.0931\n",
            "step 5980: train loss = 1.9549, val loss = 2.0220\n",
            "step 5990: train loss = 1.9285, val loss = 2.0143\n",
            "step 6000: train loss = 2.0905, val loss = 2.1136\n",
            "step 6010: train loss = 1.9724, val loss = 2.0011\n",
            "step 6020: train loss = 1.9122, val loss = 1.9822\n",
            "step 6030: train loss = 2.0676, val loss = 2.0435\n",
            "step 6040: train loss = 2.0260, val loss = 1.9221\n",
            "step 6050: train loss = 2.0562, val loss = 2.0564\n",
            "step 6060: train loss = 2.0450, val loss = 1.9739\n",
            "step 6070: train loss = 2.0615, val loss = 1.9932\n",
            "step 6080: train loss = 2.0302, val loss = 1.9550\n",
            "step 6090: train loss = 2.0128, val loss = 1.9937\n",
            "step 6100: train loss = 1.9774, val loss = 2.0543\n",
            "step 6110: train loss = 2.1183, val loss = 1.9704\n",
            "step 6120: train loss = 2.0448, val loss = 2.0583\n",
            "step 6130: train loss = 2.0365, val loss = 1.9966\n",
            "step 6140: train loss = 2.0459, val loss = 1.9528\n",
            "step 6150: train loss = 2.0351, val loss = 1.9796\n",
            "step 6160: train loss = 1.9932, val loss = 1.9454\n",
            "step 6170: train loss = 2.1922, val loss = 2.0443\n",
            "step 6180: train loss = 1.9709, val loss = 2.0009\n",
            "step 6190: train loss = 1.9942, val loss = 2.0154\n",
            "step 6200: train loss = 2.0484, val loss = 2.0086\n",
            "step 6210: train loss = 1.8294, val loss = 2.0498\n",
            "step 6220: train loss = 1.9883, val loss = 2.0229\n",
            "step 6230: train loss = 2.0093, val loss = 1.9364\n",
            "step 6240: train loss = 2.1405, val loss = 1.9577\n",
            "step 6250: train loss = 2.0149, val loss = 1.9237\n",
            "step 6260: train loss = 1.9084, val loss = 2.0394\n",
            "step 6270: train loss = 1.9725, val loss = 1.9126\n",
            "step 6280: train loss = 2.0130, val loss = 2.0564\n",
            "step 6290: train loss = 2.0323, val loss = 1.9804\n",
            "step 6300: train loss = 1.9022, val loss = 2.0812\n",
            "step 6310: train loss = 1.9619, val loss = 1.9940\n",
            "step 6320: train loss = 1.9533, val loss = 2.0039\n",
            "step 6330: train loss = 2.0409, val loss = 1.9483\n",
            "step 6340: train loss = 2.0867, val loss = 1.9683\n",
            "step 6350: train loss = 2.0558, val loss = 2.0346\n",
            "step 6360: train loss = 2.0326, val loss = 2.0132\n",
            "step 6370: train loss = 2.1299, val loss = 2.0632\n",
            "step 6380: train loss = 2.2166, val loss = 2.0877\n",
            "step 6390: train loss = 2.0317, val loss = 2.0916\n",
            "step 6400: train loss = 1.9010, val loss = 1.9398\n",
            "step 6410: train loss = 1.9905, val loss = 1.9958\n",
            "step 6420: train loss = 2.0438, val loss = 1.8540\n",
            "step 6430: train loss = 2.0044, val loss = 1.9207\n",
            "step 6440: train loss = 2.0039, val loss = 2.0025\n",
            "step 6450: train loss = 2.0217, val loss = 2.0338\n",
            "step 6460: train loss = 2.0728, val loss = 2.0456\n",
            "step 6470: train loss = 1.9553, val loss = 2.0561\n",
            "step 6480: train loss = 2.0388, val loss = 2.0579\n",
            "step 6490: train loss = 1.9883, val loss = 1.9423\n",
            "step 6500: train loss = 1.8225, val loss = 1.8856\n",
            "step 6510: train loss = 1.9185, val loss = 2.0059\n",
            "step 6520: train loss = 2.0005, val loss = 1.8898\n",
            "step 6530: train loss = 1.9751, val loss = 2.0784\n",
            "step 6540: train loss = 2.0363, val loss = 1.9551\n",
            "step 6550: train loss = 1.9972, val loss = 2.0457\n",
            "step 6560: train loss = 1.9371, val loss = 2.1020\n",
            "step 6570: train loss = 2.0223, val loss = 2.0325\n",
            "step 6580: train loss = 1.9200, val loss = 1.9613\n",
            "step 6590: train loss = 2.0527, val loss = 1.9680\n",
            "step 6600: train loss = 1.8659, val loss = 2.0933\n",
            "step 6610: train loss = 2.0091, val loss = 2.0004\n",
            "step 6620: train loss = 2.0291, val loss = 2.1827\n",
            "step 6630: train loss = 1.9654, val loss = 2.0505\n",
            "step 6640: train loss = 2.0438, val loss = 2.0262\n",
            "step 6650: train loss = 2.1711, val loss = 1.9791\n",
            "step 6660: train loss = 2.0653, val loss = 1.9599\n",
            "step 6670: train loss = 1.9354, val loss = 1.9352\n",
            "step 6680: train loss = 2.0336, val loss = 2.0172\n",
            "step 6690: train loss = 1.9810, val loss = 1.9036\n",
            "step 6700: train loss = 2.0298, val loss = 1.9159\n",
            "step 6710: train loss = 1.9914, val loss = 1.9706\n",
            "step 6720: train loss = 2.0818, val loss = 2.0762\n",
            "step 6730: train loss = 1.9229, val loss = 1.9954\n",
            "step 6740: train loss = 1.9047, val loss = 2.0819\n",
            "step 6750: train loss = 1.9975, val loss = 2.0427\n",
            "step 6760: train loss = 1.8325, val loss = 1.9363\n",
            "step 6770: train loss = 2.0082, val loss = 1.9377\n",
            "step 6780: train loss = 2.0392, val loss = 2.1933\n",
            "step 6790: train loss = 1.9700, val loss = 2.0391\n",
            "step 6800: train loss = 1.9951, val loss = 1.9926\n",
            "step 6810: train loss = 1.9132, val loss = 1.9077\n",
            "step 6820: train loss = 2.0097, val loss = 2.0588\n",
            "step 6830: train loss = 2.0615, val loss = 2.0030\n",
            "step 6840: train loss = 2.0751, val loss = 2.0747\n",
            "step 6850: train loss = 2.0642, val loss = 1.9762\n",
            "step 6860: train loss = 1.8835, val loss = 2.0942\n",
            "step 6870: train loss = 1.9495, val loss = 1.9703\n",
            "step 6880: train loss = 2.0741, val loss = 1.9469\n",
            "step 6890: train loss = 1.9750, val loss = 1.7983\n",
            "step 6900: train loss = 1.9937, val loss = 1.9472\n",
            "step 6910: train loss = 1.9947, val loss = 1.9799\n",
            "step 6920: train loss = 1.9716, val loss = 1.9587\n",
            "step 6930: train loss = 1.9457, val loss = 1.8002\n",
            "step 6940: train loss = 1.9871, val loss = 1.9394\n",
            "step 6950: train loss = 1.8966, val loss = 2.1668\n",
            "step 6960: train loss = 2.0150, val loss = 1.9936\n",
            "step 6970: train loss = 2.0359, val loss = 1.9060\n",
            "step 6980: train loss = 2.0811, val loss = 2.0095\n",
            "step 6990: train loss = 1.8532, val loss = 1.8881\n",
            "step 7000: train loss = 1.8962, val loss = 1.9960\n",
            "step 7010: train loss = 1.9394, val loss = 2.0093\n",
            "step 7020: train loss = 1.9047, val loss = 1.9784\n",
            "step 7030: train loss = 1.9904, val loss = 2.0764\n",
            "step 7040: train loss = 2.0406, val loss = 1.9896\n",
            "step 7050: train loss = 2.0701, val loss = 2.0435\n",
            "step 7060: train loss = 1.9487, val loss = 2.0802\n",
            "step 7070: train loss = 2.0395, val loss = 1.9124\n",
            "step 7080: train loss = 1.9698, val loss = 1.9258\n",
            "step 7090: train loss = 2.0019, val loss = 2.1055\n",
            "step 7100: train loss = 2.1076, val loss = 2.0489\n",
            "step 7110: train loss = 2.0743, val loss = 1.9636\n",
            "step 7120: train loss = 2.0156, val loss = 2.0734\n",
            "step 7130: train loss = 1.9877, val loss = 2.0072\n",
            "step 7140: train loss = 1.9764, val loss = 2.0271\n",
            "step 7150: train loss = 2.1447, val loss = 2.0398\n",
            "step 7160: train loss = 2.0360, val loss = 2.0366\n",
            "step 7170: train loss = 2.0441, val loss = 1.9261\n",
            "step 7180: train loss = 1.9740, val loss = 1.9852\n",
            "step 7190: train loss = 1.9407, val loss = 1.9642\n",
            "step 7200: train loss = 2.0431, val loss = 2.0312\n",
            "step 7210: train loss = 1.9851, val loss = 2.0718\n",
            "step 7220: train loss = 2.1021, val loss = 2.1237\n",
            "step 7230: train loss = 2.1475, val loss = 1.9465\n",
            "step 7240: train loss = 2.0100, val loss = 2.0133\n",
            "step 7250: train loss = 1.9389, val loss = 1.9765\n",
            "step 7260: train loss = 2.1490, val loss = 2.0550\n",
            "step 7270: train loss = 1.9775, val loss = 2.0642\n",
            "step 7280: train loss = 2.0644, val loss = 2.0224\n",
            "step 7290: train loss = 1.9146, val loss = 1.9975\n",
            "step 7300: train loss = 1.9572, val loss = 2.0838\n",
            "step 7310: train loss = 1.9277, val loss = 2.0489\n",
            "step 7320: train loss = 1.9748, val loss = 2.0154\n",
            "step 7330: train loss = 2.0139, val loss = 2.0073\n",
            "step 7340: train loss = 2.0735, val loss = 2.0528\n",
            "step 7350: train loss = 2.0170, val loss = 2.0511\n",
            "step 7360: train loss = 2.0035, val loss = 2.2152\n",
            "step 7370: train loss = 2.0162, val loss = 1.9232\n",
            "step 7380: train loss = 1.8859, val loss = 1.9357\n",
            "step 7390: train loss = 2.0679, val loss = 2.0412\n",
            "step 7400: train loss = 2.0156, val loss = 2.0987\n",
            "step 7410: train loss = 1.9823, val loss = 2.0304\n",
            "step 7420: train loss = 2.0175, val loss = 2.0576\n",
            "step 7430: train loss = 1.8250, val loss = 1.9774\n",
            "step 7440: train loss = 1.9488, val loss = 2.0790\n",
            "step 7450: train loss = 1.8490, val loss = 1.9960\n",
            "step 7460: train loss = 2.0518, val loss = 1.9944\n",
            "step 7470: train loss = 1.9567, val loss = 1.9593\n",
            "step 7480: train loss = 2.0212, val loss = 1.9665\n",
            "step 7490: train loss = 2.0034, val loss = 2.0111\n",
            "step 7500: train loss = 2.0676, val loss = 1.9396\n",
            "step 7510: train loss = 1.9025, val loss = 2.0947\n",
            "step 7520: train loss = 1.9683, val loss = 1.9674\n",
            "step 7530: train loss = 1.9628, val loss = 2.0195\n",
            "step 7540: train loss = 1.9513, val loss = 2.0242\n",
            "step 7550: train loss = 2.0796, val loss = 1.9241\n",
            "step 7560: train loss = 2.0254, val loss = 1.9946\n",
            "step 7570: train loss = 2.0079, val loss = 1.9663\n",
            "step 7580: train loss = 1.8946, val loss = 1.9426\n",
            "step 7590: train loss = 2.0713, val loss = 1.9992\n",
            "step 7600: train loss = 2.0025, val loss = 1.9881\n",
            "step 7610: train loss = 1.9988, val loss = 2.0821\n",
            "step 7620: train loss = 2.0229, val loss = 2.0106\n",
            "step 7630: train loss = 2.0612, val loss = 2.1891\n",
            "step 7640: train loss = 2.1464, val loss = 1.9853\n",
            "step 7650: train loss = 1.9379, val loss = 1.9588\n",
            "step 7660: train loss = 1.8802, val loss = 1.8677\n",
            "step 7670: train loss = 1.9524, val loss = 2.0892\n",
            "step 7680: train loss = 1.9234, val loss = 2.0782\n",
            "step 7690: train loss = 2.1835, val loss = 2.1194\n",
            "step 7700: train loss = 1.9408, val loss = 2.0439\n",
            "step 7710: train loss = 2.0811, val loss = 2.0114\n",
            "step 7720: train loss = 2.0140, val loss = 2.0620\n",
            "step 7730: train loss = 2.0309, val loss = 2.1356\n",
            "step 7740: train loss = 2.0419, val loss = 1.9568\n",
            "step 7750: train loss = 1.8519, val loss = 2.0707\n",
            "step 7760: train loss = 1.9980, val loss = 2.0658\n",
            "step 7770: train loss = 2.0829, val loss = 1.9925\n",
            "step 7780: train loss = 1.9896, val loss = 1.9845\n",
            "step 7790: train loss = 2.0708, val loss = 2.0551\n",
            "step 7800: train loss = 1.9017, val loss = 1.9724\n",
            "step 7810: train loss = 1.9345, val loss = 1.9219\n",
            "step 7820: train loss = 1.9029, val loss = 2.1281\n",
            "step 7830: train loss = 1.9315, val loss = 2.0099\n",
            "step 7840: train loss = 2.0662, val loss = 2.1606\n",
            "step 7850: train loss = 2.1129, val loss = 1.9878\n",
            "step 7860: train loss = 2.0242, val loss = 1.9387\n",
            "step 7870: train loss = 1.9470, val loss = 2.0229\n",
            "step 7880: train loss = 1.9835, val loss = 2.0834\n",
            "step 7890: train loss = 2.1104, val loss = 1.9780\n",
            "step 7900: train loss = 1.9860, val loss = 1.7719\n",
            "step 7910: train loss = 2.0781, val loss = 1.8826\n",
            "step 7920: train loss = 1.9454, val loss = 1.9615\n",
            "step 7930: train loss = 1.9177, val loss = 2.0616\n",
            "step 7940: train loss = 1.9726, val loss = 2.0089\n",
            "step 7950: train loss = 2.0020, val loss = 1.8598\n",
            "step 7960: train loss = 2.0013, val loss = 2.0166\n",
            "step 7970: train loss = 1.9404, val loss = 2.0045\n",
            "step 7980: train loss = 2.1337, val loss = 1.8551\n",
            "step 7990: train loss = 2.0294, val loss = 2.1584\n",
            "step 8000: train loss = 1.8165, val loss = 1.9327\n",
            "step 8010: train loss = 2.1378, val loss = 2.0309\n",
            "step 8020: train loss = 1.8823, val loss = 1.9758\n",
            "step 8030: train loss = 2.0448, val loss = 2.0601\n",
            "step 8040: train loss = 2.1988, val loss = 2.0416\n",
            "step 8050: train loss = 2.0827, val loss = 1.9418\n",
            "step 8060: train loss = 1.8655, val loss = 1.9746\n",
            "step 8070: train loss = 2.1007, val loss = 2.0051\n",
            "step 8080: train loss = 2.1093, val loss = 2.0291\n",
            "step 8090: train loss = 1.9226, val loss = 1.9248\n",
            "step 8100: train loss = 1.9928, val loss = 2.0591\n",
            "step 8110: train loss = 1.8758, val loss = 1.9129\n",
            "step 8120: train loss = 1.9010, val loss = 1.8872\n",
            "step 8130: train loss = 2.0159, val loss = 2.0314\n",
            "step 8140: train loss = 1.9478, val loss = 1.9492\n",
            "step 8150: train loss = 2.0053, val loss = 1.9417\n",
            "step 8160: train loss = 2.1180, val loss = 1.9586\n",
            "step 8170: train loss = 2.0201, val loss = 1.9921\n",
            "step 8180: train loss = 2.0336, val loss = 1.9655\n",
            "step 8190: train loss = 1.9625, val loss = 2.0535\n",
            "step 8200: train loss = 1.9912, val loss = 2.0190\n",
            "step 8210: train loss = 2.0009, val loss = 2.1371\n",
            "step 8220: train loss = 1.8312, val loss = 2.0408\n",
            "step 8230: train loss = 2.0095, val loss = 1.9467\n",
            "step 8240: train loss = 1.9804, val loss = 1.9865\n",
            "step 8250: train loss = 1.9637, val loss = 1.8943\n",
            "step 8260: train loss = 2.0929, val loss = 2.1353\n",
            "step 8270: train loss = 1.9566, val loss = 2.1296\n",
            "step 8280: train loss = 2.0093, val loss = 2.1547\n",
            "step 8290: train loss = 1.9574, val loss = 1.9820\n",
            "step 8300: train loss = 1.9637, val loss = 1.8696\n",
            "step 8310: train loss = 1.9606, val loss = 2.2408\n",
            "step 8320: train loss = 1.8745, val loss = 1.9080\n",
            "step 8330: train loss = 1.9422, val loss = 1.8993\n",
            "step 8340: train loss = 1.9787, val loss = 2.0210\n",
            "step 8350: train loss = 2.0366, val loss = 1.9527\n",
            "step 8360: train loss = 1.9539, val loss = 1.9666\n",
            "step 8370: train loss = 1.8804, val loss = 1.8919\n",
            "step 8380: train loss = 2.0774, val loss = 1.9597\n",
            "step 8390: train loss = 1.9391, val loss = 2.0909\n",
            "step 8400: train loss = 1.7975, val loss = 1.9480\n",
            "step 8410: train loss = 1.9698, val loss = 1.9740\n",
            "step 8420: train loss = 1.9080, val loss = 1.9909\n",
            "step 8430: train loss = 1.8873, val loss = 2.0655\n",
            "step 8440: train loss = 2.0297, val loss = 1.9955\n",
            "step 8450: train loss = 1.9083, val loss = 1.9844\n",
            "step 8460: train loss = 2.0757, val loss = 1.8868\n",
            "step 8470: train loss = 2.1068, val loss = 1.9188\n",
            "step 8480: train loss = 1.8407, val loss = 2.0066\n",
            "step 8490: train loss = 2.1236, val loss = 1.9314\n",
            "step 8500: train loss = 2.0467, val loss = 2.0419\n",
            "step 8510: train loss = 1.8985, val loss = 1.8980\n",
            "step 8520: train loss = 1.9000, val loss = 1.8978\n",
            "step 8530: train loss = 1.8918, val loss = 1.8548\n",
            "step 8540: train loss = 1.9601, val loss = 1.9484\n",
            "step 8550: train loss = 1.9654, val loss = 1.8709\n",
            "step 8560: train loss = 2.0951, val loss = 1.9805\n",
            "step 8570: train loss = 1.9455, val loss = 1.9112\n",
            "step 8580: train loss = 1.8598, val loss = 1.9000\n",
            "step 8590: train loss = 2.0016, val loss = 1.9112\n",
            "step 8600: train loss = 1.9514, val loss = 1.9191\n",
            "step 8610: train loss = 1.8892, val loss = 1.8937\n",
            "step 8620: train loss = 2.0028, val loss = 1.8335\n",
            "step 8630: train loss = 1.9619, val loss = 1.9098\n",
            "step 8640: train loss = 1.8883, val loss = 1.9358\n",
            "step 8650: train loss = 1.9516, val loss = 2.0624\n",
            "step 8660: train loss = 2.0731, val loss = 1.9749\n",
            "step 8670: train loss = 1.7827, val loss = 1.8911\n",
            "step 8680: train loss = 1.9812, val loss = 1.8813\n",
            "step 8690: train loss = 1.9384, val loss = 2.0642\n",
            "step 8700: train loss = 2.0137, val loss = 2.0614\n",
            "step 8710: train loss = 2.1984, val loss = 2.0941\n",
            "step 8720: train loss = 2.0120, val loss = 1.9124\n",
            "step 8730: train loss = 1.8783, val loss = 1.9246\n",
            "step 8740: train loss = 2.1741, val loss = 2.0589\n",
            "step 8750: train loss = 2.0698, val loss = 1.9186\n",
            "step 8760: train loss = 1.9375, val loss = 1.9546\n",
            "step 8770: train loss = 1.8857, val loss = 1.8908\n",
            "step 8780: train loss = 1.9544, val loss = 1.9709\n",
            "step 8790: train loss = 1.8561, val loss = 2.0179\n",
            "step 8800: train loss = 1.9630, val loss = 1.9239\n",
            "step 8810: train loss = 1.9534, val loss = 1.8174\n",
            "step 8820: train loss = 1.9475, val loss = 2.0678\n",
            "step 8830: train loss = 1.9292, val loss = 1.9739\n",
            "step 8840: train loss = 1.9481, val loss = 2.1462\n",
            "step 8850: train loss = 1.9280, val loss = 2.0694\n",
            "step 8860: train loss = 1.8029, val loss = 1.9724\n",
            "step 8870: train loss = 1.9620, val loss = 1.8976\n",
            "step 8880: train loss = 1.9803, val loss = 2.0412\n",
            "step 8890: train loss = 2.0561, val loss = 2.0974\n",
            "step 8900: train loss = 2.0290, val loss = 1.9863\n",
            "step 8910: train loss = 1.9531, val loss = 2.0659\n",
            "step 8920: train loss = 1.9563, val loss = 1.9852\n",
            "step 8930: train loss = 2.0956, val loss = 1.8986\n",
            "step 8940: train loss = 1.9032, val loss = 1.9254\n",
            "step 8950: train loss = 2.0339, val loss = 1.9432\n",
            "step 8960: train loss = 1.8993, val loss = 1.9789\n",
            "step 8970: train loss = 2.0261, val loss = 2.0880\n",
            "step 8980: train loss = 2.0546, val loss = 1.8919\n",
            "step 8990: train loss = 1.9859, val loss = 2.0334\n",
            "step 9000: train loss = 1.9594, val loss = 1.9986\n",
            "step 9010: train loss = 2.0879, val loss = 2.0016\n",
            "step 9020: train loss = 1.9720, val loss = 2.0425\n",
            "step 9030: train loss = 1.7701, val loss = 1.8350\n",
            "step 9040: train loss = 1.9637, val loss = 1.9746\n",
            "step 9050: train loss = 1.9235, val loss = 1.9509\n",
            "step 9060: train loss = 2.0148, val loss = 1.8776\n",
            "step 9070: train loss = 1.9941, val loss = 2.0058\n",
            "step 9080: train loss = 1.8546, val loss = 1.9736\n",
            "step 9090: train loss = 1.9692, val loss = 1.9994\n",
            "step 9100: train loss = 1.8891, val loss = 1.9633\n",
            "step 9110: train loss = 1.8993, val loss = 1.9998\n",
            "step 9120: train loss = 1.9839, val loss = 1.9813\n",
            "step 9130: train loss = 1.9769, val loss = 2.0045\n",
            "step 9140: train loss = 2.0029, val loss = 1.8464\n",
            "step 9150: train loss = 1.9395, val loss = 2.0880\n",
            "step 9160: train loss = 2.0472, val loss = 1.8848\n",
            "step 9170: train loss = 1.9237, val loss = 1.9210\n",
            "step 9180: train loss = 2.0028, val loss = 2.0808\n",
            "step 9190: train loss = 2.0731, val loss = 2.0188\n",
            "step 9200: train loss = 2.0374, val loss = 1.9454\n",
            "step 9210: train loss = 1.8814, val loss = 1.9769\n",
            "step 9220: train loss = 1.9508, val loss = 1.9496\n",
            "step 9230: train loss = 1.8297, val loss = 1.9841\n",
            "step 9240: train loss = 1.8817, val loss = 2.1209\n",
            "step 9250: train loss = 2.0605, val loss = 2.0100\n",
            "step 9260: train loss = 1.9544, val loss = 1.8774\n",
            "step 9270: train loss = 1.9970, val loss = 1.9960\n",
            "step 9280: train loss = 1.9178, val loss = 1.9376\n",
            "step 9290: train loss = 1.9014, val loss = 1.9531\n",
            "step 9300: train loss = 1.9945, val loss = 1.9651\n",
            "step 9310: train loss = 1.8385, val loss = 1.9878\n",
            "step 9320: train loss = 2.0077, val loss = 1.9351\n",
            "step 9330: train loss = 1.9532, val loss = 1.9621\n",
            "step 9340: train loss = 1.9716, val loss = 2.0275\n",
            "step 9350: train loss = 1.9346, val loss = 1.9332\n",
            "step 9360: train loss = 1.9197, val loss = 2.0215\n",
            "step 9370: train loss = 1.9057, val loss = 1.8667\n",
            "step 9380: train loss = 1.9578, val loss = 2.0024\n",
            "step 9390: train loss = 2.0693, val loss = 1.8657\n",
            "step 9400: train loss = 1.9730, val loss = 1.9281\n",
            "step 9410: train loss = 2.0501, val loss = 1.8967\n",
            "step 9420: train loss = 2.0200, val loss = 2.0464\n",
            "step 9430: train loss = 1.9685, val loss = 2.0362\n",
            "step 9440: train loss = 1.9345, val loss = 2.0143\n",
            "step 9450: train loss = 1.9991, val loss = 1.9707\n",
            "step 9460: train loss = 1.9677, val loss = 1.9317\n",
            "step 9470: train loss = 1.8239, val loss = 2.0113\n",
            "step 9480: train loss = 1.9498, val loss = 1.9811\n",
            "step 9490: train loss = 2.0300, val loss = 2.0255\n",
            "step 9500: train loss = 1.8911, val loss = 2.0170\n",
            "step 9510: train loss = 1.9309, val loss = 2.0278\n",
            "step 9520: train loss = 2.0127, val loss = 1.9738\n",
            "step 9530: train loss = 2.0160, val loss = 1.8873\n",
            "step 9540: train loss = 1.7768, val loss = 1.9410\n",
            "step 9550: train loss = 1.9593, val loss = 2.0352\n",
            "step 9560: train loss = 2.0804, val loss = 2.0145\n",
            "step 9570: train loss = 2.0673, val loss = 1.9801\n",
            "step 9580: train loss = 2.0220, val loss = 2.0712\n",
            "step 9590: train loss = 2.0258, val loss = 1.9420\n",
            "step 9600: train loss = 1.9993, val loss = 1.9975\n",
            "step 9610: train loss = 2.1082, val loss = 2.2122\n",
            "step 9620: train loss = 1.9357, val loss = 1.9803\n",
            "step 9630: train loss = 2.0252, val loss = 1.8834\n",
            "step 9640: train loss = 1.9457, val loss = 2.1175\n",
            "step 9650: train loss = 1.9791, val loss = 2.0011\n",
            "step 9660: train loss = 1.8959, val loss = 1.9108\n",
            "step 9670: train loss = 1.9289, val loss = 2.0618\n",
            "step 9680: train loss = 1.9890, val loss = 1.9574\n",
            "step 9690: train loss = 2.0471, val loss = 1.9792\n",
            "step 9700: train loss = 1.9701, val loss = 1.9549\n",
            "step 9710: train loss = 1.9215, val loss = 2.0125\n",
            "step 9720: train loss = 1.9869, val loss = 1.9422\n",
            "step 9730: train loss = 1.9610, val loss = 1.9234\n",
            "step 9740: train loss = 2.0715, val loss = 1.9256\n",
            "step 9750: train loss = 1.9880, val loss = 1.9439\n",
            "step 9760: train loss = 2.0235, val loss = 2.1308\n",
            "step 9770: train loss = 1.8405, val loss = 1.9456\n",
            "step 9780: train loss = 2.0046, val loss = 1.9623\n",
            "step 9790: train loss = 2.0674, val loss = 1.9593\n",
            "step 9800: train loss = 1.9486, val loss = 1.8820\n",
            "step 9810: train loss = 1.9963, val loss = 1.8348\n",
            "step 9820: train loss = 2.0024, val loss = 2.0113\n",
            "step 9830: train loss = 2.0370, val loss = 1.8872\n",
            "step 9840: train loss = 1.9882, val loss = 1.8356\n",
            "step 9850: train loss = 2.0932, val loss = 1.9088\n",
            "step 9860: train loss = 1.9410, val loss = 1.8980\n",
            "step 9870: train loss = 1.9214, val loss = 1.8741\n",
            "step 9880: train loss = 2.0007, val loss = 2.1022\n",
            "step 9890: train loss = 1.8714, val loss = 2.0516\n",
            "step 9900: train loss = 1.9840, val loss = 1.9151\n",
            "step 9910: train loss = 2.0284, val loss = 2.1002\n",
            "step 9920: train loss = 2.0473, val loss = 2.0329\n",
            "step 9930: train loss = 1.9733, val loss = 2.0699\n",
            "step 9940: train loss = 1.9545, val loss = 1.9893\n",
            "step 9950: train loss = 1.8008, val loss = 1.9402\n",
            "step 9960: train loss = 1.9835, val loss = 1.9652\n",
            "step 9970: train loss = 1.9206, val loss = 2.0171\n",
            "step 9980: train loss = 1.8226, val loss = 2.0677\n",
            "step 9990: train loss = 1.9027, val loss = 1.8767\n",
            "step 10000: train loss = 2.0076, val loss = 1.9740\n",
            "step 10010: train loss = 2.1273, val loss = 1.9235\n",
            "step 10020: train loss = 1.9427, val loss = 1.9965\n",
            "step 10030: train loss = 2.0516, val loss = 1.8768\n",
            "step 10040: train loss = 1.9146, val loss = 1.9510\n",
            "step 10050: train loss = 1.9219, val loss = 2.0923\n",
            "step 10060: train loss = 1.9516, val loss = 1.9625\n",
            "step 10070: train loss = 1.9002, val loss = 1.8769\n",
            "step 10080: train loss = 1.9723, val loss = 2.1111\n",
            "step 10090: train loss = 1.9493, val loss = 1.9479\n",
            "step 10100: train loss = 1.9878, val loss = 2.0178\n",
            "step 10110: train loss = 1.8474, val loss = 1.8774\n",
            "step 10120: train loss = 2.0090, val loss = 2.0015\n",
            "step 10130: train loss = 1.9621, val loss = 1.9727\n",
            "step 10140: train loss = 2.0334, val loss = 2.0662\n",
            "step 10150: train loss = 1.9707, val loss = 2.0672\n",
            "step 10160: train loss = 1.8046, val loss = 1.9448\n",
            "step 10170: train loss = 2.0202, val loss = 2.0176\n",
            "step 10180: train loss = 2.0738, val loss = 1.9378\n",
            "step 10190: train loss = 1.8843, val loss = 2.0427\n",
            "step 10200: train loss = 1.8629, val loss = 1.9974\n",
            "step 10210: train loss = 1.9555, val loss = 1.9425\n",
            "step 10220: train loss = 1.8979, val loss = 1.9854\n",
            "step 10230: train loss = 2.0010, val loss = 2.0107\n",
            "step 10240: train loss = 1.9654, val loss = 1.8495\n",
            "step 10250: train loss = 1.8271, val loss = 2.0749\n",
            "step 10260: train loss = 1.9632, val loss = 2.0491\n",
            "step 10270: train loss = 1.8942, val loss = 1.9129\n",
            "step 10280: train loss = 1.9006, val loss = 1.8809\n",
            "step 10290: train loss = 2.0981, val loss = 1.9790\n",
            "step 10300: train loss = 1.8975, val loss = 1.8349\n",
            "step 10310: train loss = 1.9783, val loss = 1.9990\n",
            "step 10320: train loss = 1.9180, val loss = 1.9524\n",
            "step 10330: train loss = 1.9176, val loss = 1.8205\n",
            "step 10340: train loss = 2.0298, val loss = 1.9449\n",
            "step 10350: train loss = 1.8893, val loss = 1.8487\n",
            "step 10360: train loss = 1.8790, val loss = 1.9674\n",
            "step 10370: train loss = 1.9689, val loss = 1.9592\n",
            "step 10380: train loss = 1.8834, val loss = 1.8526\n",
            "step 10390: train loss = 1.9894, val loss = 1.9122\n",
            "step 10400: train loss = 1.9145, val loss = 1.8994\n",
            "step 10410: train loss = 1.8735, val loss = 1.9680\n",
            "step 10420: train loss = 1.9027, val loss = 1.9100\n",
            "step 10430: train loss = 1.9837, val loss = 1.9254\n",
            "step 10440: train loss = 1.8199, val loss = 1.9429\n",
            "step 10450: train loss = 1.9294, val loss = 1.9890\n",
            "step 10460: train loss = 1.8757, val loss = 2.0965\n",
            "step 10470: train loss = 1.9869, val loss = 1.9868\n",
            "step 10480: train loss = 1.9105, val loss = 1.9524\n",
            "step 10490: train loss = 1.9858, val loss = 1.9159\n",
            "step 10500: train loss = 1.9085, val loss = 2.1153\n",
            "step 10510: train loss = 1.9570, val loss = 1.9920\n",
            "step 10520: train loss = 1.9795, val loss = 1.9703\n",
            "step 10530: train loss = 1.9266, val loss = 2.0276\n",
            "step 10540: train loss = 1.9549, val loss = 2.0336\n",
            "step 10550: train loss = 1.8880, val loss = 1.9937\n",
            "step 10560: train loss = 1.9112, val loss = 1.8782\n",
            "step 10570: train loss = 1.9291, val loss = 2.1055\n",
            "step 10580: train loss = 2.0072, val loss = 1.8388\n",
            "step 10590: train loss = 1.9418, val loss = 1.8211\n",
            "step 10600: train loss = 1.9585, val loss = 1.8869\n",
            "step 10610: train loss = 1.8321, val loss = 1.9150\n",
            "step 10620: train loss = 1.9153, val loss = 2.0409\n",
            "step 10630: train loss = 1.8436, val loss = 1.8822\n",
            "step 10640: train loss = 1.9193, val loss = 1.9363\n",
            "step 10650: train loss = 1.9281, val loss = 1.9736\n",
            "step 10660: train loss = 1.9625, val loss = 1.9320\n",
            "step 10670: train loss = 1.9079, val loss = 1.9503\n",
            "step 10680: train loss = 1.9218, val loss = 1.9251\n",
            "step 10690: train loss = 1.9143, val loss = 1.9302\n",
            "step 10700: train loss = 2.0516, val loss = 1.8928\n",
            "step 10710: train loss = 1.9221, val loss = 1.9793\n",
            "step 10720: train loss = 1.9643, val loss = 1.9558\n",
            "step 10730: train loss = 1.8226, val loss = 2.1212\n",
            "step 10740: train loss = 2.0133, val loss = 1.9307\n",
            "step 10750: train loss = 1.9809, val loss = 1.9746\n",
            "step 10760: train loss = 1.8044, val loss = 1.9477\n",
            "step 10770: train loss = 2.0077, val loss = 1.8080\n",
            "step 10780: train loss = 2.0009, val loss = 1.8768\n",
            "step 10790: train loss = 2.0132, val loss = 1.8797\n",
            "step 10800: train loss = 2.0239, val loss = 1.9567\n",
            "step 10810: train loss = 1.9474, val loss = 1.8299\n",
            "step 10820: train loss = 1.9393, val loss = 1.9803\n",
            "step 10830: train loss = 2.0489, val loss = 1.9556\n",
            "step 10840: train loss = 2.0560, val loss = 2.0670\n",
            "step 10850: train loss = 1.9763, val loss = 1.9676\n",
            "step 10860: train loss = 2.0063, val loss = 1.9470\n",
            "step 10870: train loss = 1.9937, val loss = 1.9851\n",
            "step 10880: train loss = 1.9621, val loss = 1.8228\n",
            "step 10890: train loss = 2.0485, val loss = 1.9671\n",
            "step 10900: train loss = 2.0704, val loss = 1.9869\n",
            "step 10910: train loss = 1.9723, val loss = 1.9396\n",
            "step 10920: train loss = 2.0594, val loss = 1.9873\n",
            "step 10930: train loss = 1.8182, val loss = 2.0366\n",
            "step 10940: train loss = 2.0088, val loss = 1.9312\n",
            "step 10950: train loss = 1.9539, val loss = 1.9755\n",
            "step 10960: train loss = 2.0330, val loss = 1.9513\n",
            "step 10970: train loss = 1.9498, val loss = 1.9545\n",
            "step 10980: train loss = 1.8747, val loss = 2.0514\n",
            "step 10990: train loss = 1.8913, val loss = 2.0243\n",
            "step 11000: train loss = 2.1493, val loss = 1.9900\n",
            "step 11010: train loss = 1.8929, val loss = 2.0397\n",
            "step 11020: train loss = 1.9961, val loss = 1.8366\n",
            "step 11030: train loss = 1.8876, val loss = 1.8639\n",
            "step 11040: train loss = 1.8918, val loss = 2.1212\n",
            "step 11050: train loss = 2.0449, val loss = 1.9034\n",
            "step 11060: train loss = 2.0603, val loss = 1.7455\n",
            "step 11070: train loss = 2.0595, val loss = 1.9644\n",
            "step 11080: train loss = 1.9730, val loss = 1.8360\n",
            "step 11090: train loss = 1.8998, val loss = 1.9291\n",
            "step 11100: train loss = 1.8377, val loss = 1.9422\n",
            "step 11110: train loss = 2.0086, val loss = 1.9938\n",
            "step 11120: train loss = 2.0112, val loss = 1.9966\n",
            "step 11130: train loss = 2.0528, val loss = 2.0992\n",
            "step 11140: train loss = 2.0190, val loss = 1.9106\n",
            "step 11150: train loss = 1.9625, val loss = 1.9616\n",
            "step 11160: train loss = 1.9736, val loss = 2.0767\n",
            "step 11170: train loss = 1.9805, val loss = 2.0115\n",
            "step 11180: train loss = 2.1476, val loss = 1.8500\n",
            "step 11190: train loss = 1.9294, val loss = 1.9678\n",
            "step 11200: train loss = 1.8952, val loss = 1.9572\n",
            "step 11210: train loss = 1.9212, val loss = 2.1621\n",
            "step 11220: train loss = 1.9317, val loss = 1.9531\n",
            "step 11230: train loss = 1.8887, val loss = 2.0206\n",
            "step 11240: train loss = 1.9492, val loss = 1.8862\n",
            "step 11250: train loss = 1.8969, val loss = 2.0232\n",
            "step 11260: train loss = 1.9597, val loss = 1.9843\n",
            "step 11270: train loss = 1.9284, val loss = 1.9127\n",
            "step 11280: train loss = 1.9593, val loss = 1.8962\n",
            "step 11290: train loss = 1.9512, val loss = 1.9846\n",
            "step 11300: train loss = 2.0243, val loss = 2.0200\n",
            "step 11310: train loss = 1.9271, val loss = 1.9568\n",
            "step 11320: train loss = 2.0101, val loss = 1.8874\n",
            "step 11330: train loss = 1.9421, val loss = 1.8703\n",
            "step 11340: train loss = 1.9332, val loss = 1.8729\n",
            "step 11350: train loss = 1.9379, val loss = 1.8987\n",
            "step 11360: train loss = 2.0275, val loss = 1.9716\n",
            "step 11370: train loss = 2.0648, val loss = 2.0359\n",
            "step 11380: train loss = 1.8665, val loss = 2.0320\n",
            "step 11390: train loss = 1.8388, val loss = 1.8156\n",
            "step 11400: train loss = 1.9718, val loss = 1.8799\n",
            "step 11410: train loss = 1.9735, val loss = 1.9742\n",
            "step 11420: train loss = 1.8867, val loss = 1.9251\n",
            "step 11430: train loss = 1.8809, val loss = 1.9638\n",
            "step 11440: train loss = 1.9128, val loss = 1.8837\n",
            "step 11450: train loss = 1.9406, val loss = 1.8629\n",
            "step 11460: train loss = 1.9421, val loss = 2.0981\n",
            "step 11470: train loss = 2.0107, val loss = 1.9787\n",
            "step 11480: train loss = 1.8277, val loss = 2.0202\n",
            "step 11490: train loss = 2.0222, val loss = 1.9858\n",
            "step 11500: train loss = 1.9998, val loss = 1.9654\n",
            "step 11510: train loss = 1.8999, val loss = 1.8529\n",
            "step 11520: train loss = 1.8906, val loss = 1.9500\n",
            "step 11530: train loss = 2.0283, val loss = 1.8570\n",
            "step 11540: train loss = 1.9429, val loss = 1.8914\n",
            "step 11550: train loss = 1.9890, val loss = 2.0110\n",
            "step 11560: train loss = 1.9513, val loss = 1.9435\n",
            "step 11570: train loss = 1.9581, val loss = 1.9095\n",
            "step 11580: train loss = 1.9662, val loss = 1.9857\n",
            "step 11590: train loss = 2.1083, val loss = 2.0034\n",
            "step 11600: train loss = 2.0024, val loss = 1.9705\n",
            "step 11610: train loss = 1.9401, val loss = 2.0036\n",
            "step 11620: train loss = 1.8968, val loss = 1.8829\n",
            "step 11630: train loss = 1.8422, val loss = 2.0545\n",
            "step 11640: train loss = 1.9273, val loss = 2.0351\n",
            "step 11650: train loss = 1.8276, val loss = 1.9954\n",
            "step 11660: train loss = 1.9533, val loss = 1.9420\n",
            "step 11670: train loss = 1.9302, val loss = 1.9341\n",
            "step 11680: train loss = 1.8417, val loss = 1.9588\n",
            "step 11690: train loss = 1.9581, val loss = 1.8718\n",
            "step 11700: train loss = 2.0945, val loss = 2.1016\n",
            "step 11710: train loss = 1.8931, val loss = 2.0072\n",
            "step 11720: train loss = 1.9704, val loss = 1.9376\n",
            "step 11730: train loss = 1.9982, val loss = 1.9800\n",
            "step 11740: train loss = 1.9873, val loss = 1.8264\n",
            "step 11750: train loss = 1.9420, val loss = 1.9530\n",
            "step 11760: train loss = 1.9239, val loss = 2.1475\n",
            "step 11770: train loss = 2.0088, val loss = 1.9548\n",
            "step 11780: train loss = 2.0463, val loss = 2.0127\n",
            "step 11790: train loss = 1.8443, val loss = 1.8957\n",
            "step 11800: train loss = 1.9142, val loss = 2.0463\n",
            "step 11810: train loss = 1.9339, val loss = 2.0026\n",
            "step 11820: train loss = 1.9828, val loss = 1.9646\n",
            "step 11830: train loss = 1.9975, val loss = 1.9662\n",
            "step 11840: train loss = 1.8201, val loss = 1.9345\n",
            "step 11850: train loss = 1.8647, val loss = 2.0333\n",
            "step 11860: train loss = 1.8459, val loss = 1.9273\n",
            "step 11870: train loss = 1.8427, val loss = 1.9546\n",
            "step 11880: train loss = 2.0208, val loss = 1.9885\n",
            "step 11890: train loss = 1.9763, val loss = 1.9577\n",
            "step 11900: train loss = 1.9936, val loss = 1.9411\n",
            "step 11910: train loss = 1.9634, val loss = 1.9057\n",
            "step 11920: train loss = 2.0530, val loss = 2.0297\n",
            "step 11930: train loss = 1.9912, val loss = 1.9508\n",
            "step 11940: train loss = 1.9928, val loss = 2.0410\n",
            "step 11950: train loss = 2.0031, val loss = 1.7695\n",
            "step 11960: train loss = 1.9288, val loss = 1.8206\n",
            "step 11970: train loss = 1.9322, val loss = 1.7835\n",
            "step 11980: train loss = 1.9771, val loss = 1.8244\n",
            "step 11990: train loss = 1.9687, val loss = 1.8096\n",
            "step 12000: train loss = 2.0760, val loss = 1.8556\n",
            "step 12010: train loss = 1.8776, val loss = 2.1216\n",
            "step 12020: train loss = 2.0344, val loss = 2.0095\n",
            "step 12030: train loss = 2.0475, val loss = 1.8738\n",
            "step 12040: train loss = 1.9520, val loss = 2.0024\n",
            "step 12050: train loss = 2.0344, val loss = 1.9463\n",
            "step 12060: train loss = 1.9214, val loss = 1.8898\n",
            "step 12070: train loss = 1.9839, val loss = 1.9821\n",
            "step 12080: train loss = 1.9625, val loss = 1.8936\n",
            "step 12090: train loss = 1.8949, val loss = 2.0373\n",
            "step 12100: train loss = 1.7984, val loss = 1.9427\n",
            "step 12110: train loss = 2.0474, val loss = 1.8987\n",
            "step 12120: train loss = 1.9967, val loss = 1.9921\n",
            "step 12130: train loss = 1.9704, val loss = 1.8501\n",
            "step 12140: train loss = 1.9606, val loss = 2.0190\n",
            "step 12150: train loss = 1.8159, val loss = 1.9162\n",
            "step 12160: train loss = 2.0308, val loss = 1.8981\n",
            "step 12170: train loss = 1.9565, val loss = 1.7966\n",
            "step 12180: train loss = 1.8739, val loss = 1.9461\n",
            "step 12190: train loss = 1.9973, val loss = 2.0065\n",
            "step 12200: train loss = 2.0041, val loss = 1.9272\n",
            "step 12210: train loss = 1.9843, val loss = 1.9076\n",
            "step 12220: train loss = 2.0301, val loss = 2.0198\n",
            "step 12230: train loss = 1.9332, val loss = 1.8934\n",
            "step 12240: train loss = 1.9916, val loss = 1.9143\n",
            "step 12250: train loss = 1.9397, val loss = 1.8712\n",
            "step 12260: train loss = 2.0499, val loss = 1.9281\n",
            "step 12270: train loss = 2.0261, val loss = 1.8186\n",
            "step 12280: train loss = 2.0170, val loss = 1.9965\n",
            "step 12290: train loss = 2.0217, val loss = 1.9326\n",
            "step 12300: train loss = 1.9276, val loss = 1.8721\n",
            "step 12310: train loss = 1.8647, val loss = 1.8644\n",
            "step 12320: train loss = 2.0055, val loss = 1.9226\n",
            "step 12330: train loss = 1.9776, val loss = 1.9274\n",
            "step 12340: train loss = 1.9407, val loss = 1.8519\n",
            "step 12350: train loss = 1.9612, val loss = 1.8941\n",
            "step 12360: train loss = 1.9481, val loss = 1.7647\n",
            "step 12370: train loss = 1.9179, val loss = 1.8973\n",
            "step 12380: train loss = 2.0253, val loss = 1.8173\n",
            "step 12390: train loss = 2.1404, val loss = 1.8484\n",
            "step 12400: train loss = 1.9496, val loss = 2.0106\n",
            "step 12410: train loss = 1.9572, val loss = 2.0090\n",
            "step 12420: train loss = 2.0134, val loss = 2.0264\n",
            "step 12430: train loss = 2.0844, val loss = 1.8770\n",
            "step 12440: train loss = 1.8981, val loss = 1.9416\n",
            "step 12450: train loss = 1.9744, val loss = 2.0261\n",
            "step 12460: train loss = 1.9526, val loss = 2.0551\n",
            "step 12470: train loss = 2.0317, val loss = 2.0239\n",
            "step 12480: train loss = 1.9236, val loss = 1.9546\n",
            "step 12490: train loss = 1.9328, val loss = 1.8951\n",
            "step 12500: train loss = 2.0749, val loss = 1.8642\n",
            "step 12510: train loss = 2.0700, val loss = 2.0885\n",
            "step 12520: train loss = 1.9245, val loss = 1.8934\n",
            "step 12530: train loss = 1.9609, val loss = 2.0022\n",
            "step 12540: train loss = 2.0394, val loss = 1.9124\n",
            "step 12550: train loss = 2.0175, val loss = 1.9901\n",
            "step 12560: train loss = 2.0124, val loss = 1.8901\n",
            "step 12570: train loss = 1.9345, val loss = 1.9351\n",
            "step 12580: train loss = 1.9866, val loss = 1.9202\n",
            "step 12590: train loss = 1.9470, val loss = 1.9360\n",
            "step 12600: train loss = 1.8732, val loss = 1.7688\n",
            "step 12610: train loss = 1.9502, val loss = 1.8936\n",
            "step 12620: train loss = 1.9097, val loss = 2.0303\n",
            "step 12630: train loss = 2.0763, val loss = 2.0620\n",
            "step 12640: train loss = 2.0081, val loss = 1.9035\n",
            "step 12650: train loss = 1.9690, val loss = 1.9142\n",
            "step 12660: train loss = 2.0403, val loss = 1.8822\n",
            "step 12670: train loss = 1.9112, val loss = 2.0177\n",
            "step 12680: train loss = 2.0675, val loss = 2.1046\n",
            "step 12690: train loss = 1.9836, val loss = 2.0009\n",
            "step 12700: train loss = 1.9215, val loss = 1.9490\n",
            "step 12710: train loss = 1.8457, val loss = 1.9061\n",
            "step 12720: train loss = 1.9756, val loss = 1.9277\n",
            "step 12730: train loss = 1.9503, val loss = 1.9202\n",
            "step 12740: train loss = 1.9853, val loss = 1.9186\n",
            "step 12750: train loss = 2.0264, val loss = 1.8597\n",
            "step 12760: train loss = 1.9358, val loss = 2.1103\n",
            "step 12770: train loss = 2.0786, val loss = 1.9297\n",
            "step 12780: train loss = 1.9779, val loss = 1.8431\n",
            "step 12790: train loss = 1.9339, val loss = 1.9689\n",
            "step 12800: train loss = 1.8085, val loss = 1.8033\n",
            "step 12810: train loss = 1.9935, val loss = 2.0156\n",
            "step 12820: train loss = 1.9741, val loss = 1.9424\n",
            "step 12830: train loss = 2.0667, val loss = 1.9824\n",
            "step 12840: train loss = 1.9993, val loss = 1.8116\n",
            "step 12850: train loss = 1.9849, val loss = 1.9633\n",
            "step 12860: train loss = 1.9526, val loss = 1.9654\n",
            "step 12870: train loss = 1.8082, val loss = 1.7498\n",
            "step 12880: train loss = 1.8423, val loss = 2.0116\n",
            "step 12890: train loss = 1.8565, val loss = 1.9246\n",
            "step 12900: train loss = 1.8439, val loss = 1.9493\n",
            "step 12910: train loss = 1.9876, val loss = 1.9103\n",
            "step 12920: train loss = 1.9305, val loss = 1.8018\n",
            "step 12930: train loss = 1.9972, val loss = 1.8524\n",
            "step 12940: train loss = 1.9712, val loss = 2.0031\n",
            "step 12950: train loss = 1.8729, val loss = 1.9041\n",
            "step 12960: train loss = 1.9638, val loss = 1.8590\n",
            "step 12970: train loss = 1.9213, val loss = 1.9398\n",
            "step 12980: train loss = 2.0083, val loss = 2.0814\n",
            "step 12990: train loss = 1.8427, val loss = 1.9677\n",
            "step 13000: train loss = 1.8682, val loss = 1.9412\n",
            "step 13010: train loss = 1.9049, val loss = 2.0634\n",
            "step 13020: train loss = 1.9493, val loss = 1.9809\n",
            "step 13030: train loss = 1.7386, val loss = 1.9922\n",
            "step 13040: train loss = 2.0523, val loss = 2.0753\n",
            "step 13050: train loss = 2.0129, val loss = 1.9028\n",
            "step 13060: train loss = 2.0555, val loss = 1.9175\n",
            "step 13070: train loss = 1.9492, val loss = 2.0089\n",
            "step 13080: train loss = 2.0272, val loss = 1.9609\n",
            "step 13090: train loss = 2.0278, val loss = 1.8811\n",
            "step 13100: train loss = 1.9551, val loss = 1.9516\n",
            "step 13110: train loss = 2.0128, val loss = 1.8813\n",
            "step 13120: train loss = 2.0062, val loss = 1.8144\n",
            "step 13130: train loss = 2.0036, val loss = 1.9168\n",
            "step 13140: train loss = 1.9215, val loss = 1.9444\n",
            "step 13150: train loss = 1.9693, val loss = 1.9094\n",
            "step 13160: train loss = 2.0355, val loss = 2.0166\n",
            "step 13170: train loss = 1.8793, val loss = 1.9887\n",
            "step 13180: train loss = 1.8734, val loss = 2.0595\n",
            "step 13190: train loss = 1.8723, val loss = 1.9803\n",
            "step 13200: train loss = 1.9514, val loss = 2.0038\n",
            "step 13210: train loss = 1.9449, val loss = 2.0178\n",
            "step 13220: train loss = 1.8292, val loss = 1.9963\n",
            "step 13230: train loss = 1.9523, val loss = 1.8472\n",
            "step 13240: train loss = 1.8045, val loss = 1.9515\n",
            "step 13250: train loss = 1.9897, val loss = 1.9952\n",
            "step 13260: train loss = 1.7988, val loss = 2.0125\n",
            "step 13270: train loss = 1.9475, val loss = 1.9466\n",
            "step 13280: train loss = 1.9807, val loss = 2.0517\n",
            "step 13290: train loss = 1.8546, val loss = 1.9102\n",
            "step 13300: train loss = 2.0158, val loss = 1.9052\n",
            "step 13310: train loss = 1.9971, val loss = 1.9462\n",
            "step 13320: train loss = 2.0699, val loss = 2.0363\n",
            "step 13330: train loss = 1.9566, val loss = 2.0294\n",
            "step 13340: train loss = 1.8756, val loss = 1.8739\n",
            "step 13350: train loss = 2.0180, val loss = 1.9050\n",
            "step 13360: train loss = 1.8639, val loss = 1.8390\n",
            "step 13370: train loss = 1.8969, val loss = 1.8831\n",
            "step 13380: train loss = 1.8768, val loss = 1.9223\n",
            "step 13390: train loss = 1.8966, val loss = 1.8299\n",
            "step 13400: train loss = 1.8988, val loss = 1.9647\n",
            "step 13410: train loss = 1.9383, val loss = 2.0388\n",
            "step 13420: train loss = 1.9186, val loss = 1.8671\n",
            "step 13430: train loss = 1.9125, val loss = 1.9181\n",
            "step 13440: train loss = 2.0431, val loss = 1.8120\n",
            "step 13450: train loss = 1.9607, val loss = 2.0592\n",
            "step 13460: train loss = 1.8074, val loss = 1.8075\n",
            "step 13470: train loss = 1.9835, val loss = 1.9046\n",
            "step 13480: train loss = 1.9071, val loss = 1.9726\n",
            "step 13490: train loss = 1.9933, val loss = 1.9462\n",
            "step 13500: train loss = 2.1102, val loss = 1.9370\n",
            "step 13510: train loss = 1.9496, val loss = 1.9292\n",
            "step 13520: train loss = 1.9063, val loss = 1.8732\n",
            "step 13530: train loss = 1.9189, val loss = 1.9237\n",
            "step 13540: train loss = 2.1041, val loss = 1.9231\n",
            "step 13550: train loss = 1.9982, val loss = 2.1038\n",
            "step 13560: train loss = 1.8832, val loss = 1.9259\n",
            "step 13570: train loss = 1.8457, val loss = 1.8894\n",
            "step 13580: train loss = 1.9613, val loss = 1.9934\n",
            "step 13590: train loss = 1.9757, val loss = 2.0304\n",
            "step 13600: train loss = 1.7304, val loss = 1.9945\n",
            "step 13610: train loss = 1.9426, val loss = 1.9119\n",
            "step 13620: train loss = 1.9394, val loss = 1.8466\n",
            "step 13630: train loss = 1.9406, val loss = 1.9392\n",
            "step 13640: train loss = 1.9033, val loss = 1.8496\n",
            "step 13650: train loss = 1.8892, val loss = 1.9160\n",
            "step 13660: train loss = 1.8359, val loss = 1.8680\n",
            "step 13670: train loss = 1.8676, val loss = 1.8668\n",
            "step 13680: train loss = 1.9722, val loss = 1.8514\n",
            "step 13690: train loss = 1.9037, val loss = 1.9167\n",
            "step 13700: train loss = 1.9413, val loss = 1.9722\n",
            "step 13710: train loss = 2.0092, val loss = 1.8830\n",
            "step 13720: train loss = 1.8985, val loss = 1.9096\n",
            "step 13730: train loss = 1.9420, val loss = 1.9648\n",
            "step 13740: train loss = 1.8220, val loss = 1.8261\n",
            "step 13750: train loss = 1.9198, val loss = 1.9988\n",
            "step 13760: train loss = 1.9640, val loss = 1.9827\n",
            "step 13770: train loss = 1.9881, val loss = 1.9403\n",
            "step 13780: train loss = 1.8512, val loss = 1.8920\n",
            "step 13790: train loss = 2.0380, val loss = 1.9747\n",
            "step 13800: train loss = 2.0169, val loss = 1.9011\n",
            "step 13810: train loss = 2.0056, val loss = 1.8730\n",
            "step 13820: train loss = 1.9050, val loss = 1.8301\n",
            "step 13830: train loss = 2.0121, val loss = 1.9066\n",
            "step 13840: train loss = 1.9673, val loss = 1.9127\n",
            "step 13850: train loss = 1.9018, val loss = 2.0834\n",
            "step 13860: train loss = 2.0268, val loss = 1.9043\n",
            "step 13870: train loss = 2.0349, val loss = 2.0377\n",
            "step 13880: train loss = 1.9518, val loss = 2.0224\n",
            "step 13890: train loss = 1.9153, val loss = 1.9394\n",
            "step 13900: train loss = 1.9192, val loss = 1.9441\n",
            "step 13910: train loss = 1.9442, val loss = 1.9681\n",
            "step 13920: train loss = 1.9075, val loss = 2.1533\n",
            "step 13930: train loss = 2.0655, val loss = 2.0729\n",
            "step 13940: train loss = 1.9026, val loss = 2.1184\n",
            "step 13950: train loss = 1.9875, val loss = 1.8954\n",
            "step 13960: train loss = 2.0081, val loss = 1.9834\n",
            "step 13970: train loss = 1.8133, val loss = 1.9441\n",
            "step 13980: train loss = 1.8253, val loss = 2.0766\n",
            "step 13990: train loss = 2.0426, val loss = 1.9621\n",
            "step 14000: train loss = 2.0105, val loss = 1.8737\n",
            "step 14010: train loss = 1.9492, val loss = 1.8723\n",
            "step 14020: train loss = 1.9267, val loss = 1.9348\n",
            "step 14030: train loss = 1.9045, val loss = 1.8482\n",
            "step 14040: train loss = 1.9683, val loss = 2.0039\n",
            "step 14050: train loss = 2.0056, val loss = 1.8579\n",
            "step 14060: train loss = 1.9794, val loss = 1.9693\n",
            "step 14070: train loss = 2.0257, val loss = 2.0307\n",
            "step 14080: train loss = 2.0641, val loss = 1.9291\n",
            "step 14090: train loss = 1.9640, val loss = 1.9014\n",
            "step 14100: train loss = 2.0656, val loss = 2.0245\n",
            "step 14110: train loss = 2.0253, val loss = 1.9402\n",
            "step 14120: train loss = 1.9247, val loss = 1.9264\n",
            "step 14130: train loss = 2.0091, val loss = 1.9172\n",
            "step 14140: train loss = 1.9134, val loss = 1.7888\n",
            "step 14150: train loss = 2.0320, val loss = 1.9714\n",
            "step 14160: train loss = 1.9724, val loss = 1.8904\n",
            "step 14170: train loss = 2.0023, val loss = 1.8893\n",
            "step 14180: train loss = 1.8310, val loss = 1.9556\n",
            "step 14190: train loss = 1.8303, val loss = 1.9075\n",
            "step 14200: train loss = 1.8824, val loss = 1.9536\n",
            "step 14210: train loss = 2.0205, val loss = 1.8946\n",
            "step 14220: train loss = 1.7876, val loss = 1.8743\n",
            "step 14230: train loss = 1.8540, val loss = 2.0645\n",
            "step 14240: train loss = 2.0127, val loss = 1.8791\n",
            "step 14250: train loss = 1.9717, val loss = 2.0949\n",
            "step 14260: train loss = 1.9251, val loss = 1.9527\n",
            "step 14270: train loss = 1.9031, val loss = 1.8925\n",
            "step 14280: train loss = 2.0275, val loss = 1.8514\n",
            "step 14290: train loss = 1.9491, val loss = 2.0098\n",
            "step 14300: train loss = 1.9120, val loss = 1.7873\n",
            "step 14310: train loss = 1.9373, val loss = 1.9151\n",
            "step 14320: train loss = 1.9358, val loss = 1.8648\n",
            "step 14330: train loss = 1.8473, val loss = 1.8461\n",
            "step 14340: train loss = 1.9620, val loss = 2.0521\n",
            "step 14350: train loss = 1.8016, val loss = 2.0028\n",
            "step 14360: train loss = 1.8463, val loss = 1.9365\n",
            "step 14370: train loss = 1.7820, val loss = 1.8974\n",
            "step 14380: train loss = 1.9873, val loss = 1.9186\n",
            "step 14390: train loss = 1.8398, val loss = 1.9592\n",
            "step 14400: train loss = 1.9059, val loss = 1.9917\n",
            "step 14410: train loss = 1.8595, val loss = 1.8802\n",
            "step 14420: train loss = 2.0081, val loss = 1.9024\n",
            "step 14430: train loss = 1.9020, val loss = 1.9774\n",
            "step 14440: train loss = 1.9446, val loss = 1.9726\n",
            "step 14450: train loss = 1.9203, val loss = 1.9381\n",
            "step 14460: train loss = 1.9512, val loss = 1.9546\n",
            "step 14470: train loss = 1.9168, val loss = 1.9013\n",
            "step 14480: train loss = 1.9683, val loss = 1.9345\n",
            "step 14490: train loss = 1.9005, val loss = 2.0201\n",
            "step 14500: train loss = 1.9624, val loss = 1.9769\n",
            "step 14510: train loss = 1.8332, val loss = 2.0769\n",
            "step 14520: train loss = 1.9552, val loss = 1.9444\n",
            "step 14530: train loss = 1.9556, val loss = 1.8644\n",
            "step 14540: train loss = 1.8933, val loss = 1.8118\n",
            "step 14550: train loss = 1.9715, val loss = 1.8987\n",
            "step 14560: train loss = 1.9499, val loss = 2.0338\n",
            "step 14570: train loss = 1.8782, val loss = 1.9305\n",
            "step 14580: train loss = 2.1036, val loss = 1.8923\n",
            "step 14590: train loss = 1.8791, val loss = 1.8641\n",
            "step 14600: train loss = 1.9828, val loss = 1.8398\n",
            "step 14610: train loss = 1.8830, val loss = 1.8454\n",
            "step 14620: train loss = 1.7730, val loss = 1.8341\n",
            "step 14630: train loss = 1.8688, val loss = 1.9142\n",
            "step 14640: train loss = 1.9911, val loss = 1.9064\n",
            "step 14650: train loss = 1.8765, val loss = 1.8897\n",
            "step 14660: train loss = 1.8838, val loss = 1.9406\n",
            "step 14670: train loss = 1.9944, val loss = 1.9489\n",
            "step 14680: train loss = 1.8427, val loss = 2.0414\n",
            "step 14690: train loss = 1.8521, val loss = 1.9518\n",
            "step 14700: train loss = 1.9004, val loss = 2.0316\n",
            "step 14710: train loss = 1.9780, val loss = 1.9722\n",
            "step 14720: train loss = 1.9644, val loss = 2.1240\n",
            "step 14730: train loss = 1.9104, val loss = 1.9747\n",
            "step 14740: train loss = 1.9483, val loss = 1.9225\n",
            "step 14750: train loss = 2.0554, val loss = 1.9559\n",
            "step 14760: train loss = 1.8511, val loss = 1.9342\n",
            "step 14770: train loss = 1.8978, val loss = 2.0421\n",
            "step 14780: train loss = 1.9155, val loss = 1.8585\n",
            "step 14790: train loss = 1.8659, val loss = 1.9786\n",
            "step 14800: train loss = 1.9200, val loss = 1.9763\n",
            "step 14810: train loss = 2.0479, val loss = 2.0439\n",
            "step 14820: train loss = 1.9282, val loss = 1.9155\n",
            "step 14830: train loss = 1.9896, val loss = 1.9974\n",
            "step 14840: train loss = 1.8864, val loss = 1.9914\n",
            "step 14850: train loss = 1.9045, val loss = 1.9759\n",
            "step 14860: train loss = 1.8193, val loss = 1.9975\n",
            "step 14870: train loss = 1.9218, val loss = 1.8923\n",
            "step 14880: train loss = 1.9149, val loss = 2.0163\n",
            "step 14890: train loss = 1.9147, val loss = 1.9758\n",
            "step 14900: train loss = 2.0149, val loss = 1.8885\n",
            "step 14910: train loss = 1.9447, val loss = 1.8958\n",
            "step 14920: train loss = 1.9362, val loss = 1.8942\n",
            "step 14930: train loss = 1.8686, val loss = 1.9695\n",
            "step 14940: train loss = 2.0825, val loss = 1.9190\n",
            "step 14950: train loss = 1.8355, val loss = 1.9881\n",
            "step 14960: train loss = 1.8194, val loss = 1.9741\n",
            "step 14970: train loss = 1.8924, val loss = 1.8759\n",
            "step 14980: train loss = 1.9071, val loss = 1.8238\n",
            "step 14990: train loss = 2.0278, val loss = 1.8824\n",
            "step 15000: train loss = 1.8132, val loss = 1.8820\n",
            "step 15010: train loss = 1.9784, val loss = 1.9024\n",
            "step 15020: train loss = 2.0130, val loss = 2.1077\n",
            "step 15030: train loss = 1.9265, val loss = 1.8987\n",
            "step 15040: train loss = 2.0540, val loss = 1.8247\n",
            "step 15050: train loss = 1.9065, val loss = 1.9112\n",
            "step 15060: train loss = 2.0368, val loss = 1.8622\n",
            "step 15070: train loss = 1.8830, val loss = 1.9085\n",
            "step 15080: train loss = 2.0396, val loss = 2.0347\n",
            "step 15090: train loss = 1.7835, val loss = 1.8667\n",
            "step 15100: train loss = 1.7934, val loss = 1.9020\n",
            "step 15110: train loss = 1.9465, val loss = 1.9180\n",
            "step 15120: train loss = 1.8752, val loss = 1.9550\n",
            "step 15130: train loss = 1.9062, val loss = 1.8834\n",
            "step 15140: train loss = 1.9329, val loss = 1.9419\n",
            "step 15150: train loss = 1.9210, val loss = 1.9323\n",
            "step 15160: train loss = 2.0248, val loss = 1.8884\n",
            "step 15170: train loss = 1.9060, val loss = 2.0198\n",
            "step 15180: train loss = 1.8081, val loss = 1.8329\n",
            "step 15190: train loss = 1.9374, val loss = 1.9522\n",
            "step 15200: train loss = 1.8270, val loss = 1.9684\n",
            "step 15210: train loss = 1.9655, val loss = 2.0146\n",
            "step 15220: train loss = 1.8249, val loss = 1.8978\n",
            "step 15230: train loss = 1.7968, val loss = 1.8842\n",
            "step 15240: train loss = 1.9481, val loss = 1.8412\n",
            "step 15250: train loss = 2.1206, val loss = 1.8828\n",
            "step 15260: train loss = 1.9328, val loss = 1.9451\n",
            "step 15270: train loss = 1.9012, val loss = 1.8479\n",
            "step 15280: train loss = 1.9950, val loss = 1.9358\n",
            "step 15290: train loss = 1.9353, val loss = 1.9788\n",
            "step 15300: train loss = 1.9605, val loss = 1.8335\n",
            "step 15310: train loss = 2.0056, val loss = 1.8825\n",
            "step 15320: train loss = 1.9258, val loss = 2.0124\n",
            "step 15330: train loss = 1.9553, val loss = 1.9018\n",
            "step 15340: train loss = 1.9550, val loss = 1.8627\n",
            "step 15350: train loss = 1.8732, val loss = 2.0102\n",
            "step 15360: train loss = 1.9105, val loss = 1.9157\n",
            "step 15370: train loss = 1.9208, val loss = 1.9778\n",
            "step 15380: train loss = 1.9412, val loss = 1.8090\n",
            "step 15390: train loss = 1.9533, val loss = 1.9117\n",
            "step 15400: train loss = 1.8881, val loss = 1.9003\n",
            "step 15410: train loss = 1.8728, val loss = 1.9534\n",
            "step 15420: train loss = 2.0677, val loss = 1.9655\n",
            "step 15430: train loss = 1.7918, val loss = 1.7990\n",
            "step 15440: train loss = 1.9876, val loss = 1.9630\n",
            "step 15450: train loss = 1.9379, val loss = 1.8525\n",
            "step 15460: train loss = 2.0324, val loss = 1.8841\n",
            "step 15470: train loss = 1.9904, val loss = 1.8393\n",
            "step 15480: train loss = 1.8980, val loss = 1.9725\n",
            "step 15490: train loss = 2.0153, val loss = 1.8631\n",
            "step 15500: train loss = 1.8736, val loss = 2.0035\n",
            "step 15510: train loss = 1.8670, val loss = 1.9609\n",
            "step 15520: train loss = 1.9132, val loss = 1.9421\n",
            "step 15530: train loss = 1.9601, val loss = 1.8596\n",
            "step 15540: train loss = 1.9679, val loss = 1.9286\n",
            "step 15550: train loss = 2.0247, val loss = 1.9184\n",
            "step 15560: train loss = 1.9776, val loss = 1.9624\n",
            "step 15570: train loss = 1.8950, val loss = 1.8525\n",
            "step 15580: train loss = 1.9541, val loss = 2.0220\n",
            "step 15590: train loss = 1.8531, val loss = 1.8446\n",
            "step 15600: train loss = 1.8613, val loss = 1.9065\n",
            "step 15610: train loss = 2.0266, val loss = 1.9224\n",
            "step 15620: train loss = 1.9301, val loss = 1.9239\n",
            "step 15630: train loss = 1.8723, val loss = 1.8894\n",
            "step 15640: train loss = 1.8329, val loss = 1.8261\n",
            "step 15650: train loss = 1.8292, val loss = 1.8652\n",
            "step 15660: train loss = 1.9193, val loss = 1.8982\n",
            "step 15670: train loss = 1.8904, val loss = 1.8681\n",
            "step 15680: train loss = 1.9771, val loss = 2.0220\n",
            "step 15690: train loss = 1.8972, val loss = 2.0175\n",
            "step 15700: train loss = 1.9109, val loss = 1.8984\n",
            "step 15710: train loss = 1.8358, val loss = 2.0488\n",
            "step 15720: train loss = 1.9245, val loss = 2.0669\n",
            "step 15730: train loss = 1.9113, val loss = 2.0034\n",
            "step 15740: train loss = 1.9944, val loss = 1.8767\n",
            "step 15750: train loss = 1.9464, val loss = 1.9595\n",
            "step 15760: train loss = 1.9417, val loss = 1.8438\n",
            "step 15770: train loss = 1.8933, val loss = 1.8267\n",
            "step 15780: train loss = 1.9637, val loss = 1.9878\n",
            "step 15790: train loss = 1.8677, val loss = 1.9399\n",
            "step 15800: train loss = 1.9846, val loss = 1.8041\n",
            "step 15810: train loss = 1.8623, val loss = 1.9674\n",
            "step 15820: train loss = 1.9787, val loss = 1.8341\n",
            "step 15830: train loss = 1.9014, val loss = 1.9726\n",
            "step 15840: train loss = 1.9017, val loss = 1.9220\n",
            "step 15850: train loss = 2.0022, val loss = 2.0118\n",
            "step 15860: train loss = 1.9525, val loss = 1.8743\n",
            "step 15870: train loss = 1.9447, val loss = 1.8829\n",
            "step 15880: train loss = 2.0213, val loss = 1.8565\n",
            "step 15890: train loss = 1.9532, val loss = 1.9266\n",
            "step 15900: train loss = 2.0123, val loss = 1.9494\n",
            "step 15910: train loss = 1.8341, val loss = 1.8281\n",
            "step 15920: train loss = 1.7816, val loss = 1.8419\n",
            "step 15930: train loss = 1.9685, val loss = 1.9078\n",
            "step 15940: train loss = 1.9130, val loss = 1.7959\n",
            "step 15950: train loss = 1.9693, val loss = 1.8123\n",
            "step 15960: train loss = 1.9379, val loss = 2.0984\n",
            "step 15970: train loss = 1.9001, val loss = 1.7805\n",
            "step 15980: train loss = 1.8758, val loss = 1.9058\n",
            "step 15990: train loss = 1.9986, val loss = 1.9127\n",
            "step 16000: train loss = 1.9266, val loss = 1.9690\n",
            "step 16010: train loss = 2.0022, val loss = 2.0452\n",
            "step 16020: train loss = 1.9346, val loss = 2.0563\n",
            "step 16030: train loss = 1.9470, val loss = 2.0899\n",
            "step 16040: train loss = 2.0521, val loss = 1.8402\n",
            "step 16050: train loss = 1.7825, val loss = 1.8979\n",
            "step 16060: train loss = 1.8760, val loss = 1.9904\n",
            "step 16070: train loss = 1.9191, val loss = 1.8540\n",
            "step 16080: train loss = 1.9996, val loss = 1.8603\n",
            "step 16090: train loss = 1.9444, val loss = 1.9799\n",
            "step 16100: train loss = 2.0086, val loss = 2.0433\n",
            "step 16110: train loss = 1.9511, val loss = 1.7775\n",
            "step 16120: train loss = 1.8606, val loss = 1.8479\n",
            "step 16130: train loss = 1.7618, val loss = 1.9882\n",
            "step 16140: train loss = 2.0223, val loss = 1.9575\n",
            "step 16150: train loss = 2.0996, val loss = 1.9293\n",
            "step 16160: train loss = 1.9879, val loss = 2.0295\n",
            "step 16170: train loss = 1.9950, val loss = 1.9179\n",
            "step 16180: train loss = 1.9034, val loss = 2.0045\n",
            "step 16190: train loss = 1.8964, val loss = 1.9179\n",
            "step 16200: train loss = 1.9896, val loss = 1.9684\n",
            "step 16210: train loss = 1.8703, val loss = 1.8826\n",
            "step 16220: train loss = 1.8779, val loss = 1.9649\n",
            "step 16230: train loss = 1.9378, val loss = 1.8702\n",
            "step 16240: train loss = 1.8434, val loss = 1.8269\n",
            "step 16250: train loss = 1.8603, val loss = 1.9536\n",
            "step 16260: train loss = 1.8883, val loss = 1.9904\n",
            "step 16270: train loss = 1.9683, val loss = 2.0668\n",
            "step 16280: train loss = 1.9873, val loss = 2.0764\n",
            "step 16290: train loss = 1.8971, val loss = 1.8292\n",
            "step 16300: train loss = 1.9629, val loss = 1.9877\n",
            "step 16310: train loss = 1.9588, val loss = 1.9563\n",
            "step 16320: train loss = 1.9793, val loss = 2.0572\n",
            "step 16330: train loss = 1.8741, val loss = 1.9588\n",
            "step 16340: train loss = 1.8670, val loss = 1.8139\n",
            "step 16350: train loss = 1.9356, val loss = 1.9223\n",
            "step 16360: train loss = 1.9116, val loss = 1.9782\n",
            "step 16370: train loss = 1.7928, val loss = 1.8929\n",
            "step 16380: train loss = 1.9682, val loss = 1.9696\n",
            "step 16390: train loss = 1.9085, val loss = 1.8595\n",
            "step 16400: train loss = 1.8998, val loss = 1.8334\n",
            "step 16410: train loss = 1.9237, val loss = 1.8582\n",
            "step 16420: train loss = 1.8504, val loss = 1.8925\n",
            "step 16430: train loss = 1.9008, val loss = 2.0487\n",
            "step 16440: train loss = 1.9316, val loss = 1.9250\n",
            "step 16450: train loss = 1.9367, val loss = 1.9392\n",
            "step 16460: train loss = 1.8752, val loss = 1.9776\n",
            "step 16470: train loss = 1.8267, val loss = 1.9444\n",
            "step 16480: train loss = 1.9190, val loss = 1.8738\n",
            "step 16490: train loss = 1.8847, val loss = 2.0143\n",
            "step 16500: train loss = 2.0508, val loss = 1.9721\n",
            "step 16510: train loss = 1.8426, val loss = 2.0418\n",
            "step 16520: train loss = 1.8127, val loss = 1.9494\n",
            "step 16530: train loss = 1.8378, val loss = 1.8557\n",
            "step 16540: train loss = 1.9588, val loss = 1.9615\n",
            "step 16550: train loss = 1.9113, val loss = 1.9052\n",
            "step 16560: train loss = 1.8972, val loss = 1.9526\n",
            "step 16570: train loss = 1.8234, val loss = 1.9647\n",
            "step 16580: train loss = 1.8695, val loss = 1.9698\n",
            "step 16590: train loss = 1.8978, val loss = 1.8188\n",
            "step 16600: train loss = 1.9958, val loss = 1.9370\n",
            "step 16610: train loss = 2.0015, val loss = 1.8710\n",
            "step 16620: train loss = 2.0193, val loss = 1.8435\n",
            "step 16630: train loss = 1.9047, val loss = 1.9508\n",
            "step 16640: train loss = 1.8766, val loss = 1.9661\n",
            "step 16650: train loss = 2.0331, val loss = 2.0720\n",
            "step 16660: train loss = 1.7884, val loss = 1.9820\n",
            "step 16670: train loss = 1.8824, val loss = 1.9349\n",
            "step 16680: train loss = 1.9785, val loss = 2.0986\n",
            "step 16690: train loss = 1.8267, val loss = 2.0677\n",
            "step 16700: train loss = 1.9009, val loss = 2.0343\n",
            "step 16710: train loss = 1.9423, val loss = 1.9694\n",
            "step 16720: train loss = 1.9915, val loss = 1.9637\n",
            "step 16730: train loss = 1.8562, val loss = 1.9401\n",
            "step 16740: train loss = 2.0125, val loss = 1.8349\n",
            "step 16750: train loss = 1.9788, val loss = 1.8955\n",
            "step 16760: train loss = 1.8474, val loss = 1.9503\n",
            "step 16770: train loss = 1.9295, val loss = 1.9959\n",
            "step 16780: train loss = 1.8946, val loss = 1.9354\n",
            "step 16790: train loss = 2.0466, val loss = 1.9954\n",
            "step 16800: train loss = 2.0539, val loss = 1.9486\n",
            "step 16810: train loss = 1.9290, val loss = 2.0161\n",
            "step 16820: train loss = 1.9255, val loss = 1.8394\n",
            "step 16830: train loss = 1.8840, val loss = 1.8310\n",
            "step 16840: train loss = 1.9553, val loss = 2.0064\n",
            "step 16850: train loss = 2.0587, val loss = 2.0036\n",
            "step 16860: train loss = 1.8710, val loss = 1.9579\n",
            "step 16870: train loss = 1.9547, val loss = 1.9302\n",
            "step 16880: train loss = 1.9420, val loss = 1.8897\n",
            "step 16890: train loss = 1.9219, val loss = 1.8423\n",
            "step 16900: train loss = 1.8772, val loss = 1.8644\n",
            "step 16910: train loss = 1.8705, val loss = 1.9005\n",
            "step 16920: train loss = 1.8608, val loss = 1.9352\n",
            "step 16930: train loss = 1.8831, val loss = 1.9458\n",
            "step 16940: train loss = 1.8686, val loss = 1.9562\n",
            "step 16950: train loss = 1.8466, val loss = 1.8078\n",
            "step 16960: train loss = 1.9710, val loss = 1.9179\n",
            "step 16970: train loss = 1.9044, val loss = 1.7915\n",
            "step 16980: train loss = 1.7860, val loss = 1.9129\n",
            "step 16990: train loss = 1.9433, val loss = 1.8583\n",
            "step 17000: train loss = 1.9166, val loss = 2.0515\n",
            "step 17010: train loss = 2.0484, val loss = 1.8826\n",
            "step 17020: train loss = 1.8191, val loss = 1.9875\n",
            "step 17030: train loss = 1.9229, val loss = 1.8502\n",
            "step 17040: train loss = 2.0580, val loss = 1.9664\n",
            "step 17050: train loss = 1.9288, val loss = 1.8818\n",
            "step 17060: train loss = 2.0375, val loss = 1.8129\n",
            "step 17070: train loss = 1.9423, val loss = 1.9519\n",
            "step 17080: train loss = 1.9095, val loss = 1.8839\n",
            "step 17090: train loss = 1.9455, val loss = 2.0038\n",
            "step 17100: train loss = 1.8401, val loss = 1.8938\n",
            "step 17110: train loss = 1.8673, val loss = 2.0408\n",
            "step 17120: train loss = 1.8666, val loss = 2.0019\n",
            "step 17130: train loss = 1.9329, val loss = 2.0340\n",
            "step 17140: train loss = 1.8423, val loss = 2.1233\n",
            "step 17150: train loss = 1.7377, val loss = 1.9973\n",
            "step 17160: train loss = 2.0106, val loss = 1.9179\n",
            "step 17170: train loss = 1.8774, val loss = 1.9240\n",
            "step 17180: train loss = 1.9342, val loss = 2.0298\n",
            "step 17190: train loss = 1.8803, val loss = 1.9065\n",
            "step 17200: train loss = 1.9594, val loss = 1.9864\n",
            "step 17210: train loss = 1.8101, val loss = 2.0018\n",
            "step 17220: train loss = 2.0327, val loss = 1.9845\n",
            "step 17230: train loss = 1.8758, val loss = 1.8654\n",
            "step 17240: train loss = 1.9564, val loss = 2.0492\n",
            "step 17250: train loss = 1.9045, val loss = 1.8963\n",
            "step 17260: train loss = 1.8064, val loss = 1.8515\n",
            "step 17270: train loss = 1.9186, val loss = 1.9303\n",
            "step 17280: train loss = 1.7353, val loss = 1.8133\n",
            "step 17290: train loss = 1.7714, val loss = 1.8897\n",
            "step 17300: train loss = 2.0654, val loss = 2.0530\n",
            "step 17310: train loss = 2.0188, val loss = 1.8438\n",
            "step 17320: train loss = 1.9420, val loss = 1.9468\n",
            "step 17330: train loss = 1.8819, val loss = 1.9826\n",
            "step 17340: train loss = 1.9111, val loss = 1.8869\n",
            "step 17350: train loss = 1.9838, val loss = 1.9076\n",
            "step 17360: train loss = 1.8051, val loss = 2.0023\n",
            "step 17370: train loss = 1.8761, val loss = 1.8642\n",
            "step 17380: train loss = 1.9688, val loss = 1.9769\n",
            "step 17390: train loss = 1.9566, val loss = 1.8949\n",
            "step 17400: train loss = 1.8202, val loss = 1.9974\n",
            "step 17410: train loss = 1.9536, val loss = 1.9457\n",
            "step 17420: train loss = 1.8316, val loss = 1.9578\n",
            "step 17430: train loss = 1.8657, val loss = 2.0770\n",
            "step 17440: train loss = 1.9808, val loss = 1.8626\n",
            "step 17450: train loss = 1.8175, val loss = 1.9272\n",
            "step 17460: train loss = 1.8491, val loss = 1.8759\n",
            "step 17470: train loss = 1.8328, val loss = 1.9700\n",
            "step 17480: train loss = 1.9865, val loss = 1.9728\n",
            "step 17490: train loss = 1.8041, val loss = 1.8917\n",
            "step 17500: train loss = 1.7952, val loss = 1.8406\n",
            "step 17510: train loss = 1.8855, val loss = 1.9299\n",
            "step 17520: train loss = 2.0078, val loss = 1.8286\n",
            "step 17530: train loss = 1.9085, val loss = 1.8249\n",
            "step 17540: train loss = 1.7878, val loss = 1.9582\n",
            "step 17550: train loss = 1.9140, val loss = 1.9517\n",
            "step 17560: train loss = 1.8263, val loss = 1.8542\n",
            "step 17570: train loss = 1.9508, val loss = 2.0108\n",
            "step 17580: train loss = 1.8904, val loss = 1.9169\n",
            "step 17590: train loss = 1.9216, val loss = 1.8973\n",
            "step 17600: train loss = 1.9057, val loss = 1.9501\n",
            "step 17610: train loss = 1.8450, val loss = 1.8632\n",
            "step 17620: train loss = 1.9355, val loss = 1.8512\n",
            "step 17630: train loss = 1.8618, val loss = 1.8839\n",
            "step 17640: train loss = 1.9264, val loss = 1.9174\n",
            "step 17650: train loss = 1.8122, val loss = 1.7821\n",
            "step 17660: train loss = 1.7825, val loss = 1.9561\n",
            "step 17670: train loss = 1.9993, val loss = 1.8752\n",
            "step 17680: train loss = 1.8989, val loss = 1.7867\n",
            "step 17690: train loss = 1.8917, val loss = 1.8595\n",
            "step 17700: train loss = 1.9204, val loss = 1.8584\n",
            "step 17710: train loss = 1.9091, val loss = 1.9829\n",
            "step 17720: train loss = 1.7368, val loss = 1.9807\n",
            "step 17730: train loss = 1.8431, val loss = 1.8524\n",
            "step 17740: train loss = 2.0169, val loss = 1.9540\n",
            "step 17750: train loss = 1.8344, val loss = 1.8929\n",
            "step 17760: train loss = 1.9678, val loss = 1.7641\n",
            "step 17770: train loss = 1.8913, val loss = 1.8613\n",
            "step 17780: train loss = 2.0740, val loss = 2.0811\n",
            "step 17790: train loss = 1.8495, val loss = 1.9637\n",
            "step 17800: train loss = 1.9898, val loss = 1.8874\n",
            "step 17810: train loss = 1.8819, val loss = 1.9642\n",
            "step 17820: train loss = 1.8698, val loss = 1.9838\n",
            "step 17830: train loss = 2.0376, val loss = 1.8930\n",
            "step 17840: train loss = 1.8734, val loss = 1.8332\n",
            "step 17850: train loss = 2.1456, val loss = 1.9471\n",
            "step 17860: train loss = 1.8400, val loss = 2.1166\n",
            "step 17870: train loss = 1.8871, val loss = 1.9988\n",
            "step 17880: train loss = 1.9474, val loss = 1.7893\n",
            "step 17890: train loss = 1.8517, val loss = 1.8975\n",
            "step 17900: train loss = 1.9146, val loss = 1.9847\n",
            "step 17910: train loss = 1.9762, val loss = 1.9431\n",
            "step 17920: train loss = 1.9713, val loss = 1.8997\n",
            "step 17930: train loss = 1.8083, val loss = 1.8894\n",
            "step 17940: train loss = 2.0028, val loss = 1.9180\n",
            "step 17950: train loss = 1.9276, val loss = 2.0843\n",
            "step 17960: train loss = 1.8006, val loss = 1.7857\n",
            "step 17970: train loss = 1.8141, val loss = 1.8075\n",
            "step 17980: train loss = 2.0857, val loss = 1.9937\n",
            "step 17990: train loss = 1.9669, val loss = 1.8929\n",
            "step 18000: train loss = 1.8732, val loss = 1.8632\n",
            "step 18010: train loss = 1.9396, val loss = 1.7209\n",
            "step 18020: train loss = 1.8379, val loss = 1.9554\n",
            "step 18030: train loss = 1.9521, val loss = 1.8656\n",
            "step 18040: train loss = 1.9786, val loss = 1.9143\n",
            "step 18050: train loss = 1.7924, val loss = 1.8761\n",
            "step 18060: train loss = 1.8736, val loss = 2.0421\n",
            "step 18070: train loss = 1.7533, val loss = 1.8280\n",
            "step 18080: train loss = 2.0128, val loss = 1.9151\n",
            "step 18090: train loss = 1.8485, val loss = 1.9059\n",
            "step 18100: train loss = 1.7783, val loss = 1.9658\n",
            "step 18110: train loss = 1.8375, val loss = 1.9891\n",
            "step 18120: train loss = 1.9334, val loss = 1.9691\n",
            "step 18130: train loss = 1.8701, val loss = 2.0222\n",
            "step 18140: train loss = 1.9511, val loss = 1.9417\n",
            "step 18150: train loss = 1.7954, val loss = 1.8391\n",
            "step 18160: train loss = 1.8301, val loss = 1.8468\n",
            "step 18170: train loss = 1.9017, val loss = 1.8619\n",
            "step 18180: train loss = 1.8987, val loss = 1.8388\n",
            "step 18190: train loss = 1.8013, val loss = 1.9663\n",
            "step 18200: train loss = 1.9290, val loss = 1.9339\n",
            "step 18210: train loss = 1.9274, val loss = 2.0179\n",
            "step 18220: train loss = 1.9022, val loss = 1.9199\n",
            "step 18230: train loss = 1.9526, val loss = 1.9446\n",
            "step 18240: train loss = 2.1089, val loss = 1.9564\n",
            "step 18250: train loss = 1.9374, val loss = 1.9507\n",
            "step 18260: train loss = 1.9447, val loss = 1.8448\n",
            "step 18270: train loss = 2.0073, val loss = 1.9494\n",
            "step 18280: train loss = 1.7767, val loss = 1.9536\n",
            "step 18290: train loss = 1.9080, val loss = 2.0405\n",
            "step 18300: train loss = 1.9603, val loss = 2.0513\n",
            "step 18310: train loss = 1.9107, val loss = 1.8284\n",
            "step 18320: train loss = 1.9498, val loss = 1.9607\n",
            "step 18330: train loss = 1.9535, val loss = 1.7324\n",
            "step 18340: train loss = 2.0329, val loss = 1.9064\n",
            "step 18350: train loss = 1.8472, val loss = 1.8083\n",
            "step 18360: train loss = 1.9109, val loss = 1.9227\n",
            "step 18370: train loss = 2.0505, val loss = 1.9422\n",
            "step 18380: train loss = 1.7603, val loss = 1.7802\n",
            "step 18390: train loss = 1.9195, val loss = 1.8460\n",
            "step 18400: train loss = 1.9746, val loss = 1.9320\n",
            "step 18410: train loss = 1.9589, val loss = 1.9467\n",
            "step 18420: train loss = 1.9893, val loss = 1.8685\n",
            "step 18430: train loss = 1.8912, val loss = 1.8094\n",
            "step 18440: train loss = 2.0440, val loss = 1.9600\n",
            "step 18450: train loss = 1.9855, val loss = 1.8999\n",
            "step 18460: train loss = 1.9535, val loss = 1.8559\n",
            "step 18470: train loss = 1.8925, val loss = 2.0864\n",
            "step 18480: train loss = 1.9031, val loss = 1.8386\n",
            "step 18490: train loss = 1.7799, val loss = 1.8477\n",
            "step 18500: train loss = 1.9224, val loss = 1.8917\n",
            "step 18510: train loss = 1.8226, val loss = 1.9208\n",
            "step 18520: train loss = 1.9279, val loss = 1.9712\n",
            "step 18530: train loss = 2.0468, val loss = 1.9051\n",
            "step 18540: train loss = 2.0206, val loss = 2.0028\n",
            "step 18550: train loss = 1.9602, val loss = 1.9213\n",
            "step 18560: train loss = 2.0057, val loss = 1.8829\n",
            "step 18570: train loss = 2.0008, val loss = 1.9295\n",
            "step 18580: train loss = 1.9833, val loss = 2.0003\n",
            "step 18590: train loss = 1.9080, val loss = 1.9836\n",
            "step 18600: train loss = 1.9939, val loss = 2.0582\n",
            "step 18610: train loss = 1.8912, val loss = 1.7599\n",
            "step 18620: train loss = 1.9623, val loss = 1.8114\n",
            "step 18630: train loss = 1.9527, val loss = 1.9431\n",
            "step 18640: train loss = 1.9978, val loss = 1.9614\n",
            "step 18650: train loss = 2.0363, val loss = 1.9297\n",
            "step 18660: train loss = 1.9816, val loss = 1.9887\n",
            "step 18670: train loss = 1.8979, val loss = 1.9442\n",
            "step 18680: train loss = 1.9364, val loss = 1.9360\n",
            "step 18690: train loss = 1.8937, val loss = 1.8970\n",
            "step 18700: train loss = 2.1530, val loss = 2.0618\n",
            "step 18710: train loss = 1.9841, val loss = 1.8179\n",
            "step 18720: train loss = 1.8217, val loss = 1.9798\n",
            "step 18730: train loss = 2.0472, val loss = 1.9219\n",
            "step 18740: train loss = 1.8568, val loss = 1.8606\n",
            "step 18750: train loss = 1.8245, val loss = 2.0004\n",
            "step 18760: train loss = 1.9921, val loss = 1.9275\n",
            "step 18770: train loss = 1.9302, val loss = 1.9080\n",
            "step 18780: train loss = 1.8848, val loss = 2.1006\n",
            "step 18790: train loss = 1.8867, val loss = 1.9100\n",
            "step 18800: train loss = 1.9050, val loss = 1.9858\n",
            "step 18810: train loss = 1.7742, val loss = 1.9372\n",
            "step 18820: train loss = 1.8247, val loss = 1.8839\n",
            "step 18830: train loss = 1.7958, val loss = 1.9163\n",
            "step 18840: train loss = 1.9024, val loss = 2.0855\n",
            "step 18850: train loss = 1.9440, val loss = 1.8369\n",
            "step 18860: train loss = 2.0811, val loss = 1.8881\n",
            "step 18870: train loss = 1.8040, val loss = 1.8462\n",
            "step 18880: train loss = 1.9231, val loss = 2.0278\n",
            "step 18890: train loss = 1.8410, val loss = 1.8977\n",
            "step 18900: train loss = 1.9308, val loss = 1.8881\n",
            "step 18910: train loss = 2.0314, val loss = 1.8179\n",
            "step 18920: train loss = 1.8647, val loss = 1.9621\n",
            "step 18930: train loss = 1.7887, val loss = 1.9428\n",
            "step 18940: train loss = 1.9149, val loss = 1.8313\n",
            "step 18950: train loss = 2.0278, val loss = 1.8946\n",
            "step 18960: train loss = 1.9370, val loss = 1.9531\n",
            "step 18970: train loss = 1.8685, val loss = 1.9771\n",
            "step 18980: train loss = 1.9688, val loss = 1.8904\n",
            "step 18990: train loss = 1.7837, val loss = 1.7776\n",
            "step 19000: train loss = 1.8926, val loss = 2.0658\n",
            "step 19010: train loss = 2.0523, val loss = 1.9626\n",
            "step 19020: train loss = 1.9896, val loss = 1.8586\n",
            "step 19030: train loss = 1.8161, val loss = 1.9200\n",
            "step 19040: train loss = 1.9762, val loss = 1.9543\n",
            "step 19050: train loss = 2.0637, val loss = 1.9056\n",
            "step 19060: train loss = 1.7181, val loss = 1.8841\n",
            "step 19070: train loss = 1.9413, val loss = 1.9033\n",
            "step 19080: train loss = 1.8617, val loss = 1.7233\n",
            "step 19090: train loss = 1.9277, val loss = 1.8219\n",
            "step 19100: train loss = 1.8277, val loss = 1.8799\n",
            "step 19110: train loss = 1.8500, val loss = 1.9492\n",
            "step 19120: train loss = 1.9319, val loss = 2.1107\n",
            "step 19130: train loss = 2.0478, val loss = 1.9346\n",
            "step 19140: train loss = 1.9609, val loss = 1.8700\n",
            "step 19150: train loss = 1.8245, val loss = 1.9113\n",
            "step 19160: train loss = 1.9221, val loss = 1.8060\n",
            "step 19170: train loss = 1.9087, val loss = 1.7918\n",
            "step 19180: train loss = 1.7971, val loss = 1.8004\n",
            "step 19190: train loss = 1.8791, val loss = 1.8839\n",
            "step 19200: train loss = 1.9235, val loss = 1.9549\n",
            "step 19210: train loss = 2.0641, val loss = 1.6746\n",
            "step 19220: train loss = 2.0165, val loss = 2.1071\n",
            "step 19230: train loss = 2.0195, val loss = 2.0237\n",
            "step 19240: train loss = 2.0700, val loss = 1.8709\n",
            "step 19250: train loss = 2.0104, val loss = 1.8359\n",
            "step 19260: train loss = 1.9052, val loss = 1.8302\n",
            "step 19270: train loss = 1.8895, val loss = 1.9125\n",
            "step 19280: train loss = 1.8468, val loss = 2.0984\n",
            "step 19290: train loss = 1.8553, val loss = 1.8401\n",
            "step 19300: train loss = 1.8890, val loss = 1.8984\n",
            "step 19310: train loss = 1.8249, val loss = 1.9591\n",
            "step 19320: train loss = 2.0108, val loss = 1.9628\n",
            "step 19330: train loss = 1.9353, val loss = 2.0603\n",
            "step 19340: train loss = 1.8040, val loss = 1.7521\n",
            "step 19350: train loss = 2.0157, val loss = 1.9214\n",
            "step 19360: train loss = 1.9747, val loss = 1.8434\n",
            "step 19370: train loss = 1.8743, val loss = 1.8580\n",
            "step 19380: train loss = 1.8016, val loss = 1.9692\n",
            "step 19390: train loss = 1.9372, val loss = 1.9147\n",
            "step 19400: train loss = 1.9039, val loss = 1.8510\n",
            "step 19410: train loss = 2.0133, val loss = 1.9942\n",
            "step 19420: train loss = 1.9216, val loss = 2.0013\n",
            "step 19430: train loss = 1.8874, val loss = 2.0473\n",
            "step 19440: train loss = 1.8503, val loss = 1.9637\n",
            "step 19450: train loss = 2.0023, val loss = 1.9864\n",
            "step 19460: train loss = 1.8253, val loss = 1.9848\n",
            "step 19470: train loss = 1.9009, val loss = 1.8664\n",
            "step 19480: train loss = 1.9491, val loss = 1.9475\n",
            "step 19490: train loss = 1.9274, val loss = 1.9645\n",
            "step 19500: train loss = 2.0640, val loss = 2.0937\n",
            "step 19510: train loss = 2.0991, val loss = 1.9038\n",
            "step 19520: train loss = 1.8706, val loss = 1.8342\n",
            "step 19530: train loss = 1.9231, val loss = 1.8973\n",
            "step 19540: train loss = 1.9698, val loss = 1.8289\n",
            "step 19550: train loss = 1.9107, val loss = 2.0098\n",
            "step 19560: train loss = 1.9261, val loss = 1.9115\n",
            "step 19570: train loss = 1.8602, val loss = 1.9252\n",
            "step 19580: train loss = 1.9658, val loss = 2.1158\n",
            "step 19590: train loss = 1.9293, val loss = 1.8942\n",
            "step 19600: train loss = 1.8433, val loss = 1.9643\n",
            "step 19610: train loss = 1.8836, val loss = 1.8733\n",
            "step 19620: train loss = 1.6659, val loss = 2.0353\n",
            "step 19630: train loss = 2.1294, val loss = 2.0539\n",
            "step 19640: train loss = 1.8249, val loss = 1.9463\n",
            "step 19650: train loss = 1.9077, val loss = 1.8886\n",
            "step 19660: train loss = 2.0013, val loss = 1.9299\n",
            "step 19670: train loss = 1.9057, val loss = 1.8780\n",
            "step 19680: train loss = 2.0345, val loss = 1.9903\n",
            "step 19690: train loss = 1.9211, val loss = 1.9885\n",
            "step 19700: train loss = 2.0594, val loss = 1.8466\n",
            "step 19710: train loss = 1.8876, val loss = 1.8263\n",
            "step 19720: train loss = 1.7520, val loss = 1.9635\n",
            "step 19730: train loss = 1.9835, val loss = 1.9190\n",
            "step 19740: train loss = 1.9746, val loss = 1.7343\n",
            "step 19750: train loss = 1.8314, val loss = 2.0078\n",
            "step 19760: train loss = 1.8893, val loss = 1.8838\n",
            "step 19770: train loss = 1.8476, val loss = 1.8894\n",
            "step 19780: train loss = 1.8851, val loss = 1.9308\n",
            "step 19790: train loss = 1.9965, val loss = 1.8611\n",
            "step 19800: train loss = 1.9980, val loss = 1.8675\n",
            "step 19810: train loss = 1.9192, val loss = 2.0016\n",
            "step 19820: train loss = 1.8146, val loss = 1.8506\n",
            "step 19830: train loss = 1.9310, val loss = 1.8853\n",
            "step 19840: train loss = 2.0113, val loss = 1.8788\n",
            "step 19850: train loss = 1.9359, val loss = 1.7791\n",
            "step 19860: train loss = 1.8297, val loss = 2.0018\n",
            "step 19870: train loss = 1.8781, val loss = 2.0204\n",
            "step 19880: train loss = 1.8112, val loss = 1.9958\n",
            "step 19890: train loss = 2.0354, val loss = 2.0990\n",
            "step 19900: train loss = 1.7752, val loss = 1.8920\n",
            "step 19910: train loss = 1.9912, val loss = 1.9092\n",
            "step 19920: train loss = 2.0695, val loss = 1.9513\n",
            "step 19930: train loss = 1.8250, val loss = 1.7907\n",
            "step 19940: train loss = 1.8769, val loss = 1.9624\n",
            "step 19950: train loss = 1.8006, val loss = 2.0400\n",
            "step 19960: train loss = 1.8961, val loss = 1.9034\n",
            "step 19970: train loss = 1.9144, val loss = 1.9193\n",
            "step 19980: train loss = 2.0391, val loss = 1.9052\n",
            "step 19990: train loss = 1.8885, val loss = 1.9113\n",
            "step 20000: train loss = 1.9210, val loss = 1.7469\n",
            "step 20010: train loss = 1.9951, val loss = 1.9053\n",
            "step 20020: train loss = 1.9561, val loss = 1.9519\n",
            "step 20030: train loss = 1.9704, val loss = 1.8991\n",
            "step 20040: train loss = 1.8855, val loss = 1.8420\n",
            "step 20050: train loss = 1.8948, val loss = 1.9704\n",
            "step 20060: train loss = 1.9762, val loss = 1.8495\n",
            "step 20070: train loss = 1.9060, val loss = 1.8349\n",
            "step 20080: train loss = 1.8882, val loss = 1.9891\n",
            "step 20090: train loss = 1.8232, val loss = 1.9295\n",
            "step 20100: train loss = 1.8130, val loss = 1.8606\n",
            "step 20110: train loss = 1.9635, val loss = 1.9580\n",
            "step 20120: train loss = 1.8795, val loss = 1.9273\n",
            "step 20130: train loss = 1.9471, val loss = 1.9847\n",
            "step 20140: train loss = 1.8867, val loss = 1.8464\n",
            "step 20150: train loss = 1.6919, val loss = 1.9060\n",
            "step 20160: train loss = 1.9849, val loss = 1.8882\n",
            "step 20170: train loss = 1.8722, val loss = 1.8960\n",
            "step 20180: train loss = 1.9080, val loss = 1.9721\n",
            "step 20190: train loss = 1.7750, val loss = 1.9032\n",
            "step 20200: train loss = 1.8317, val loss = 1.8951\n",
            "step 20210: train loss = 1.9779, val loss = 1.7606\n",
            "step 20220: train loss = 2.0810, val loss = 1.9122\n",
            "step 20230: train loss = 1.9817, val loss = 1.8466\n",
            "step 20240: train loss = 1.8633, val loss = 1.6839\n",
            "step 20250: train loss = 1.9532, val loss = 1.9030\n",
            "step 20260: train loss = 2.0063, val loss = 1.9067\n",
            "step 20270: train loss = 1.8550, val loss = 1.8484\n",
            "step 20280: train loss = 1.8996, val loss = 1.9905\n",
            "step 20290: train loss = 1.9747, val loss = 1.9159\n",
            "step 20300: train loss = 2.0553, val loss = 1.9794\n",
            "step 20310: train loss = 1.9019, val loss = 1.9895\n",
            "step 20320: train loss = 1.8839, val loss = 1.8782\n",
            "step 20330: train loss = 1.9078, val loss = 1.9820\n",
            "step 20340: train loss = 1.8358, val loss = 1.8801\n",
            "step 20350: train loss = 1.9068, val loss = 2.0253\n",
            "step 20360: train loss = 2.1125, val loss = 2.0212\n",
            "step 20370: train loss = 1.8922, val loss = 1.8876\n",
            "step 20380: train loss = 2.0995, val loss = 2.0039\n",
            "step 20390: train loss = 1.8701, val loss = 2.0153\n",
            "step 20400: train loss = 1.8854, val loss = 1.7662\n",
            "step 20410: train loss = 1.9784, val loss = 1.8078\n",
            "step 20420: train loss = 1.9655, val loss = 1.7987\n",
            "step 20430: train loss = 1.9970, val loss = 1.8781\n",
            "step 20440: train loss = 1.8783, val loss = 1.9357\n",
            "step 20450: train loss = 2.0151, val loss = 1.9265\n",
            "step 20460: train loss = 2.0227, val loss = 1.9500\n",
            "step 20470: train loss = 1.9251, val loss = 1.9404\n",
            "step 20480: train loss = 1.9485, val loss = 1.9774\n",
            "step 20490: train loss = 1.7787, val loss = 1.9121\n",
            "step 20500: train loss = 1.9329, val loss = 1.9313\n",
            "step 20510: train loss = 1.9452, val loss = 1.9236\n",
            "step 20520: train loss = 1.9148, val loss = 1.8556\n",
            "step 20530: train loss = 1.8847, val loss = 1.9265\n",
            "step 20540: train loss = 1.8977, val loss = 1.8617\n",
            "step 20550: train loss = 2.0251, val loss = 1.8237\n",
            "step 20560: train loss = 1.8949, val loss = 1.7815\n",
            "step 20570: train loss = 1.9634, val loss = 1.8529\n",
            "step 20580: train loss = 1.8719, val loss = 1.8849\n",
            "step 20590: train loss = 2.0372, val loss = 1.7518\n",
            "step 20600: train loss = 1.8879, val loss = 2.1236\n",
            "step 20610: train loss = 1.8550, val loss = 1.9428\n",
            "step 20620: train loss = 1.9334, val loss = 1.8309\n",
            "step 20630: train loss = 1.9533, val loss = 1.7949\n",
            "step 20640: train loss = 1.9259, val loss = 1.8403\n",
            "step 20650: train loss = 1.8758, val loss = 2.0116\n",
            "step 20660: train loss = 1.8790, val loss = 1.8815\n",
            "step 20670: train loss = 1.8892, val loss = 1.9932\n",
            "step 20680: train loss = 1.8625, val loss = 2.0236\n",
            "step 20690: train loss = 1.8339, val loss = 1.9956\n",
            "step 20700: train loss = 1.9378, val loss = 1.8735\n",
            "step 20710: train loss = 1.9942, val loss = 2.0625\n",
            "step 20720: train loss = 1.8699, val loss = 1.8972\n",
            "step 20730: train loss = 1.9929, val loss = 1.8549\n",
            "step 20740: train loss = 1.9372, val loss = 1.8917\n",
            "step 20750: train loss = 1.9856, val loss = 1.9137\n",
            "step 20760: train loss = 1.8703, val loss = 1.9672\n",
            "step 20770: train loss = 1.9699, val loss = 1.7862\n",
            "step 20780: train loss = 1.9028, val loss = 1.7792\n",
            "step 20790: train loss = 1.9738, val loss = 1.9342\n",
            "step 20800: train loss = 1.9544, val loss = 1.8679\n",
            "step 20810: train loss = 1.7951, val loss = 1.8006\n",
            "step 20820: train loss = 1.9679, val loss = 1.9872\n",
            "step 20830: train loss = 1.9751, val loss = 1.9073\n",
            "step 20840: train loss = 1.7856, val loss = 1.9531\n",
            "step 20850: train loss = 1.9599, val loss = 1.9308\n",
            "step 20860: train loss = 1.8662, val loss = 1.9831\n",
            "step 20870: train loss = 1.9277, val loss = 1.8695\n",
            "step 20880: train loss = 2.0363, val loss = 1.8698\n",
            "step 20890: train loss = 1.8227, val loss = 1.9873\n",
            "step 20900: train loss = 2.0112, val loss = 1.9880\n",
            "step 20910: train loss = 1.9693, val loss = 1.8080\n",
            "step 20920: train loss = 1.9110, val loss = 1.9115\n",
            "step 20930: train loss = 1.8965, val loss = 1.9011\n",
            "step 20940: train loss = 1.9370, val loss = 1.9890\n",
            "step 20950: train loss = 1.9034, val loss = 1.8446\n",
            "step 20960: train loss = 1.8437, val loss = 1.9036\n",
            "step 20970: train loss = 1.8994, val loss = 1.9077\n",
            "step 20980: train loss = 1.9691, val loss = 1.8648\n",
            "step 20990: train loss = 1.9661, val loss = 1.8596\n",
            "step 21000: train loss = 1.9009, val loss = 1.9032\n",
            "step 21010: train loss = 1.9803, val loss = 1.9227\n",
            "step 21020: train loss = 1.7831, val loss = 1.8804\n",
            "step 21030: train loss = 2.0331, val loss = 1.8992\n",
            "step 21040: train loss = 1.9685, val loss = 2.0870\n",
            "step 21050: train loss = 1.7681, val loss = 1.8234\n",
            "step 21060: train loss = 1.8608, val loss = 1.8839\n",
            "step 21070: train loss = 1.8464, val loss = 1.9319\n",
            "step 21080: train loss = 1.8679, val loss = 1.8370\n",
            "step 21090: train loss = 1.9249, val loss = 1.9710\n",
            "step 21100: train loss = 1.8372, val loss = 1.9123\n",
            "step 21110: train loss = 1.9763, val loss = 1.7893\n",
            "step 21120: train loss = 2.0015, val loss = 2.0409\n",
            "step 21130: train loss = 1.8606, val loss = 1.9456\n",
            "step 21140: train loss = 2.0237, val loss = 2.1116\n",
            "step 21150: train loss = 1.8813, val loss = 1.7405\n",
            "step 21160: train loss = 1.9098, val loss = 2.0378\n",
            "step 21170: train loss = 1.9120, val loss = 1.9531\n",
            "step 21180: train loss = 2.0446, val loss = 1.9862\n",
            "step 21190: train loss = 1.8070, val loss = 1.9302\n",
            "step 21200: train loss = 1.8689, val loss = 1.8723\n",
            "step 21210: train loss = 1.8606, val loss = 2.1840\n",
            "step 21220: train loss = 1.9946, val loss = 1.9808\n",
            "step 21230: train loss = 1.7963, val loss = 1.9609\n",
            "step 21240: train loss = 1.8313, val loss = 1.9460\n",
            "step 21250: train loss = 2.0551, val loss = 1.9137\n",
            "step 21260: train loss = 2.0418, val loss = 1.9556\n",
            "step 21270: train loss = 2.0604, val loss = 2.0291\n",
            "step 21280: train loss = 1.9736, val loss = 1.8664\n",
            "step 21290: train loss = 1.9341, val loss = 2.0078\n",
            "step 21300: train loss = 1.8470, val loss = 1.8869\n",
            "step 21310: train loss = 2.0318, val loss = 1.7946\n",
            "step 21320: train loss = 1.7798, val loss = 1.9878\n",
            "step 21330: train loss = 1.8588, val loss = 1.9017\n",
            "step 21340: train loss = 1.9754, val loss = 1.8596\n",
            "step 21350: train loss = 1.8497, val loss = 1.9278\n",
            "step 21360: train loss = 1.9455, val loss = 2.0730\n",
            "step 21370: train loss = 1.7973, val loss = 1.9009\n",
            "step 21380: train loss = 1.9326, val loss = 1.8887\n",
            "step 21390: train loss = 1.8931, val loss = 1.8297\n",
            "step 21400: train loss = 1.8737, val loss = 2.0274\n",
            "step 21410: train loss = 1.9336, val loss = 1.8974\n",
            "step 21420: train loss = 1.9033, val loss = 1.9471\n",
            "step 21430: train loss = 1.9815, val loss = 1.8337\n",
            "step 21440: train loss = 2.0521, val loss = 1.8723\n",
            "step 21450: train loss = 1.9730, val loss = 1.9959\n",
            "step 21460: train loss = 1.9531, val loss = 1.9635\n",
            "step 21470: train loss = 1.9019, val loss = 1.9841\n",
            "step 21480: train loss = 1.8743, val loss = 1.8172\n",
            "step 21490: train loss = 1.9733, val loss = 1.9086\n",
            "step 21500: train loss = 2.0080, val loss = 1.9503\n",
            "step 21510: train loss = 1.9640, val loss = 1.9249\n",
            "step 21520: train loss = 1.9451, val loss = 1.8852\n",
            "step 21530: train loss = 1.9351, val loss = 2.0041\n",
            "step 21540: train loss = 1.8803, val loss = 1.8025\n",
            "step 21550: train loss = 1.9897, val loss = 1.8140\n",
            "step 21560: train loss = 1.8916, val loss = 2.0244\n",
            "step 21570: train loss = 1.9916, val loss = 1.8216\n",
            "step 21580: train loss = 1.9506, val loss = 2.0373\n",
            "step 21590: train loss = 1.9720, val loss = 1.8187\n",
            "step 21600: train loss = 1.9426, val loss = 1.8376\n",
            "step 21610: train loss = 1.8658, val loss = 1.8930\n",
            "step 21620: train loss = 1.8977, val loss = 1.9058\n",
            "step 21630: train loss = 1.8980, val loss = 1.7623\n",
            "step 21640: train loss = 1.7795, val loss = 1.7329\n",
            "step 21650: train loss = 1.8923, val loss = 1.9562\n",
            "step 21660: train loss = 1.8183, val loss = 1.9479\n",
            "step 21670: train loss = 1.9762, val loss = 1.8960\n",
            "step 21680: train loss = 1.7224, val loss = 1.9008\n",
            "step 21690: train loss = 1.9570, val loss = 1.9487\n",
            "step 21700: train loss = 1.7956, val loss = 1.7559\n",
            "step 21710: train loss = 1.8788, val loss = 1.9614\n",
            "step 21720: train loss = 1.9861, val loss = 1.9036\n",
            "step 21730: train loss = 1.9034, val loss = 1.7568\n",
            "step 21740: train loss = 1.9694, val loss = 1.9495\n",
            "step 21750: train loss = 1.9454, val loss = 1.8498\n",
            "step 21760: train loss = 2.0073, val loss = 2.0387\n",
            "step 21770: train loss = 1.8424, val loss = 1.6094\n",
            "step 21780: train loss = 1.7602, val loss = 1.7540\n",
            "step 21790: train loss = 2.0351, val loss = 1.8855\n",
            "step 21800: train loss = 1.9563, val loss = 1.9890\n",
            "step 21810: train loss = 1.9567, val loss = 2.2065\n",
            "step 21820: train loss = 1.8376, val loss = 2.0602\n",
            "step 21830: train loss = 1.9088, val loss = 2.0429\n",
            "step 21840: train loss = 1.9431, val loss = 1.8280\n",
            "step 21850: train loss = 1.7677, val loss = 2.0428\n",
            "step 21860: train loss = 1.9037, val loss = 1.7854\n",
            "step 21870: train loss = 1.8673, val loss = 1.8259\n",
            "step 21880: train loss = 1.8916, val loss = 1.9374\n",
            "step 21890: train loss = 1.9694, val loss = 1.9984\n",
            "step 21900: train loss = 1.8372, val loss = 2.1245\n",
            "step 21910: train loss = 1.9785, val loss = 1.9563\n",
            "step 21920: train loss = 1.8982, val loss = 1.9364\n",
            "step 21930: train loss = 1.8308, val loss = 1.8529\n",
            "step 21940: train loss = 1.8136, val loss = 1.9075\n",
            "step 21950: train loss = 1.8446, val loss = 1.9611\n",
            "step 21960: train loss = 2.0352, val loss = 2.1026\n",
            "step 21970: train loss = 1.9866, val loss = 1.9781\n",
            "step 21980: train loss = 1.9156, val loss = 1.9388\n",
            "step 21990: train loss = 1.9525, val loss = 1.9596\n",
            "step 22000: train loss = 2.1009, val loss = 1.8988\n",
            "step 22010: train loss = 2.2149, val loss = 2.1250\n",
            "step 22020: train loss = 2.0829, val loss = 2.0443\n",
            "step 22030: train loss = 2.0157, val loss = 1.9083\n",
            "step 22040: train loss = 1.9233, val loss = 1.9290\n",
            "step 22050: train loss = 1.9604, val loss = 1.9664\n",
            "step 22060: train loss = 1.9449, val loss = 2.0545\n",
            "step 22070: train loss = 1.9469, val loss = 2.0034\n",
            "step 22080: train loss = 1.8927, val loss = 1.8714\n",
            "step 22090: train loss = 1.9165, val loss = 1.9657\n",
            "step 22100: train loss = 1.9213, val loss = 2.0051\n",
            "step 22110: train loss = 1.9262, val loss = 1.9201\n",
            "step 22120: train loss = 2.0551, val loss = 1.9754\n",
            "step 22130: train loss = 2.0262, val loss = 1.7577\n",
            "step 22140: train loss = 2.0117, val loss = 2.0390\n",
            "step 22150: train loss = 1.8803, val loss = 1.9788\n",
            "step 22160: train loss = 2.0214, val loss = 1.9867\n",
            "step 22170: train loss = 1.9327, val loss = 1.8634\n",
            "step 22180: train loss = 2.0652, val loss = 1.8657\n",
            "step 22190: train loss = 1.9347, val loss = 1.9072\n",
            "step 22200: train loss = 1.9070, val loss = 1.9519\n",
            "step 22210: train loss = 2.0294, val loss = 2.0779\n",
            "step 22220: train loss = 2.0042, val loss = 1.9519\n",
            "step 22230: train loss = 2.0517, val loss = 1.9071\n",
            "step 22240: train loss = 1.8685, val loss = 1.9199\n",
            "step 22250: train loss = 1.9692, val loss = 1.8555\n",
            "step 22260: train loss = 2.0006, val loss = 1.8631\n",
            "step 22270: train loss = 1.9112, val loss = 1.8597\n",
            "step 22280: train loss = 1.9247, val loss = 1.9550\n",
            "step 22290: train loss = 1.9462, val loss = 1.8815\n",
            "step 22300: train loss = 1.7718, val loss = 2.0213\n",
            "step 22310: train loss = 1.9320, val loss = 2.0104\n",
            "step 22320: train loss = 2.0225, val loss = 1.9118\n",
            "step 22330: train loss = 1.8702, val loss = 1.8954\n",
            "step 22340: train loss = 1.9925, val loss = 1.9733\n",
            "step 22350: train loss = 1.9059, val loss = 1.8305\n",
            "step 22360: train loss = 1.8542, val loss = 1.8639\n",
            "step 22370: train loss = 1.9810, val loss = 2.0398\n",
            "step 22380: train loss = 1.9312, val loss = 1.9136\n",
            "step 22390: train loss = 2.0237, val loss = 1.8593\n",
            "step 22400: train loss = 1.8972, val loss = 1.9391\n",
            "step 22410: train loss = 1.7799, val loss = 1.9546\n",
            "step 22420: train loss = 1.8484, val loss = 1.7984\n",
            "step 22430: train loss = 1.8685, val loss = 1.8297\n",
            "step 22440: train loss = 1.9421, val loss = 1.8475\n",
            "step 22450: train loss = 1.8624, val loss = 1.9484\n",
            "step 22460: train loss = 1.8648, val loss = 1.9094\n",
            "step 22470: train loss = 1.8632, val loss = 1.8653\n",
            "step 22480: train loss = 2.0379, val loss = 2.1016\n",
            "step 22490: train loss = 1.9766, val loss = 1.9311\n",
            "step 22500: train loss = 2.0330, val loss = 1.8724\n",
            "step 22510: train loss = 1.9251, val loss = 1.8555\n",
            "step 22520: train loss = 1.9027, val loss = 1.9429\n",
            "step 22530: train loss = 2.0091, val loss = 1.8716\n",
            "step 22540: train loss = 1.9196, val loss = 1.9221\n",
            "step 22550: train loss = 1.8716, val loss = 1.8814\n",
            "step 22560: train loss = 1.9315, val loss = 1.8687\n",
            "step 22570: train loss = 1.9731, val loss = 1.8585\n",
            "step 22580: train loss = 1.8207, val loss = 1.8146\n",
            "step 22590: train loss = 1.9795, val loss = 1.8555\n",
            "step 22600: train loss = 1.9177, val loss = 1.8931\n",
            "step 22610: train loss = 1.9289, val loss = 1.9476\n",
            "step 22620: train loss = 1.9530, val loss = 1.9517\n",
            "step 22630: train loss = 1.8236, val loss = 2.0738\n",
            "step 22640: train loss = 1.8710, val loss = 1.9243\n",
            "step 22650: train loss = 1.9545, val loss = 1.8928\n",
            "step 22660: train loss = 2.0772, val loss = 1.8152\n",
            "step 22670: train loss = 1.9164, val loss = 1.8762\n",
            "step 22680: train loss = 1.9022, val loss = 2.1175\n",
            "step 22690: train loss = 1.9579, val loss = 1.9463\n",
            "step 22700: train loss = 1.8510, val loss = 1.8239\n",
            "step 22710: train loss = 2.0126, val loss = 1.9948\n",
            "step 22720: train loss = 1.9609, val loss = 1.9316\n",
            "step 22730: train loss = 1.8296, val loss = 2.1181\n",
            "step 22740: train loss = 1.9026, val loss = 2.0432\n",
            "step 22750: train loss = 1.8986, val loss = 2.0344\n",
            "step 22760: train loss = 2.0024, val loss = 1.8570\n",
            "step 22770: train loss = 1.9143, val loss = 1.8968\n",
            "step 22780: train loss = 1.9810, val loss = 1.8819\n",
            "step 22790: train loss = 1.9011, val loss = 1.8456\n",
            "step 22800: train loss = 1.8960, val loss = 1.9306\n",
            "step 22810: train loss = 1.8915, val loss = 1.9680\n",
            "step 22820: train loss = 1.9471, val loss = 1.7428\n",
            "step 22830: train loss = 1.9541, val loss = 1.7888\n",
            "step 22840: train loss = 2.0285, val loss = 1.8509\n",
            "step 22850: train loss = 1.7383, val loss = 1.8158\n",
            "step 22860: train loss = 1.9390, val loss = 1.9599\n",
            "step 22870: train loss = 1.8847, val loss = 1.7128\n",
            "step 22880: train loss = 1.9473, val loss = 2.0186\n",
            "step 22890: train loss = 1.9696, val loss = 1.9922\n",
            "step 22900: train loss = 1.8542, val loss = 1.8905\n",
            "step 22910: train loss = 1.8950, val loss = 1.8621\n",
            "step 22920: train loss = 1.9092, val loss = 1.8772\n",
            "step 22930: train loss = 1.9656, val loss = 1.7579\n",
            "step 22940: train loss = 1.9142, val loss = 1.7976\n",
            "step 22950: train loss = 1.9839, val loss = 1.7642\n",
            "step 22960: train loss = 1.9863, val loss = 1.8842\n",
            "step 22970: train loss = 1.9963, val loss = 1.8490\n",
            "step 22980: train loss = 1.8116, val loss = 1.9886\n",
            "step 22990: train loss = 2.0353, val loss = 1.8780\n",
            "step 23000: train loss = 2.0147, val loss = 2.1062\n",
            "step 23010: train loss = 1.8769, val loss = 1.9181\n",
            "step 23020: train loss = 2.0228, val loss = 1.9434\n",
            "step 23030: train loss = 1.9262, val loss = 1.8584\n",
            "step 23040: train loss = 1.9442, val loss = 1.8797\n",
            "step 23050: train loss = 1.9200, val loss = 1.9883\n",
            "step 23060: train loss = 1.9075, val loss = 1.8476\n",
            "step 23070: train loss = 1.9867, val loss = 1.9659\n",
            "step 23080: train loss = 1.8233, val loss = 1.9508\n",
            "step 23090: train loss = 2.0534, val loss = 2.0396\n",
            "step 23100: train loss = 1.9509, val loss = 1.8384\n",
            "step 23110: train loss = 1.7397, val loss = 1.9298\n",
            "step 23120: train loss = 1.9023, val loss = 1.9817\n",
            "step 23130: train loss = 2.0275, val loss = 1.9659\n",
            "step 23140: train loss = 1.9442, val loss = 1.9501\n",
            "step 23150: train loss = 1.9631, val loss = 1.7614\n",
            "step 23160: train loss = 1.9998, val loss = 1.7875\n",
            "step 23170: train loss = 1.9388, val loss = 1.7359\n",
            "step 23180: train loss = 1.7905, val loss = 1.8808\n",
            "step 23190: train loss = 1.8691, val loss = 1.9921\n",
            "step 23200: train loss = 1.9451, val loss = 2.0053\n",
            "step 23210: train loss = 1.9998, val loss = 1.8710\n",
            "step 23220: train loss = 1.9025, val loss = 1.9800\n",
            "step 23230: train loss = 1.8391, val loss = 1.9628\n",
            "step 23240: train loss = 1.8537, val loss = 2.0110\n",
            "step 23250: train loss = 1.8320, val loss = 1.8799\n",
            "step 23260: train loss = 1.8695, val loss = 2.0366\n",
            "step 23270: train loss = 1.8803, val loss = 1.8651\n",
            "step 23280: train loss = 1.8286, val loss = 1.9493\n",
            "step 23290: train loss = 1.8523, val loss = 1.9081\n",
            "step 23300: train loss = 1.7688, val loss = 1.9142\n",
            "step 23310: train loss = 1.8879, val loss = 2.0529\n",
            "step 23320: train loss = 1.8905, val loss = 1.9602\n",
            "step 23330: train loss = 2.0361, val loss = 2.0750\n",
            "step 23340: train loss = 1.9701, val loss = 1.9560\n",
            "step 23350: train loss = 1.7121, val loss = 1.8912\n",
            "step 23360: train loss = 1.9658, val loss = 2.0102\n",
            "step 23370: train loss = 1.8858, val loss = 1.8884\n",
            "step 23380: train loss = 1.9962, val loss = 1.8259\n",
            "step 23390: train loss = 1.9209, val loss = 1.9384\n",
            "step 23400: train loss = 1.9015, val loss = 1.9890\n",
            "step 23410: train loss = 1.7741, val loss = 1.9451\n",
            "step 23420: train loss = 1.7957, val loss = 2.0312\n",
            "step 23430: train loss = 1.8800, val loss = 1.9754\n",
            "step 23440: train loss = 1.8964, val loss = 1.9144\n",
            "step 23450: train loss = 1.8808, val loss = 1.9646\n",
            "step 23460: train loss = 1.9371, val loss = 1.8249\n",
            "step 23470: train loss = 1.8462, val loss = 2.0451\n",
            "step 23480: train loss = 2.0126, val loss = 1.9517\n",
            "step 23490: train loss = 1.8893, val loss = 1.8309\n",
            "step 23500: train loss = 1.9332, val loss = 1.8316\n",
            "step 23510: train loss = 1.8804, val loss = 1.8471\n",
            "step 23520: train loss = 1.9683, val loss = 1.8888\n",
            "step 23530: train loss = 1.8413, val loss = 1.8912\n",
            "step 23540: train loss = 1.8626, val loss = 1.9960\n",
            "step 23550: train loss = 1.8652, val loss = 1.8532\n",
            "step 23560: train loss = 1.9101, val loss = 2.0746\n",
            "step 23570: train loss = 1.9561, val loss = 2.0191\n",
            "step 23580: train loss = 1.8249, val loss = 1.9077\n",
            "step 23590: train loss = 1.7925, val loss = 1.8286\n",
            "step 23600: train loss = 1.8319, val loss = 1.9750\n",
            "step 23610: train loss = 1.7736, val loss = 2.0826\n",
            "step 23620: train loss = 2.0177, val loss = 1.8138\n",
            "step 23630: train loss = 1.9894, val loss = 1.9202\n",
            "step 23640: train loss = 1.8618, val loss = 1.8882\n",
            "step 23650: train loss = 1.9091, val loss = 2.0220\n",
            "step 23660: train loss = 1.9180, val loss = 1.9864\n",
            "step 23670: train loss = 2.0088, val loss = 2.0765\n",
            "step 23680: train loss = 1.9989, val loss = 1.8898\n",
            "step 23690: train loss = 1.7922, val loss = 1.8808\n",
            "step 23700: train loss = 1.8969, val loss = 1.9703\n",
            "step 23710: train loss = 1.8078, val loss = 1.8040\n",
            "step 23720: train loss = 1.9227, val loss = 1.9482\n",
            "step 23730: train loss = 1.8931, val loss = 1.7961\n",
            "step 23740: train loss = 1.7807, val loss = 1.7270\n",
            "step 23750: train loss = 1.8600, val loss = 1.8200\n",
            "step 23760: train loss = 1.8115, val loss = 1.9016\n",
            "step 23770: train loss = 1.8818, val loss = 1.9439\n",
            "step 23780: train loss = 1.9029, val loss = 2.0268\n",
            "step 23790: train loss = 1.8381, val loss = 1.7993\n",
            "step 23800: train loss = 1.9128, val loss = 1.8522\n",
            "step 23810: train loss = 1.8290, val loss = 1.8997\n",
            "step 23820: train loss = 1.8818, val loss = 1.9509\n",
            "step 23830: train loss = 1.9969, val loss = 1.8892\n",
            "step 23840: train loss = 1.8288, val loss = 1.9255\n",
            "step 23850: train loss = 1.7847, val loss = 1.9375\n",
            "step 23860: train loss = 1.8105, val loss = 1.9734\n",
            "step 23870: train loss = 1.8120, val loss = 1.8919\n",
            "step 23880: train loss = 1.9104, val loss = 2.0595\n",
            "step 23890: train loss = 1.8751, val loss = 1.9375\n",
            "step 23900: train loss = 1.9926, val loss = 1.8524\n",
            "step 23910: train loss = 1.8341, val loss = 1.9048\n",
            "step 23920: train loss = 1.9919, val loss = 2.0283\n",
            "step 23930: train loss = 1.7876, val loss = 1.8920\n",
            "step 23940: train loss = 1.9836, val loss = 1.9174\n",
            "step 23950: train loss = 1.9323, val loss = 2.1014\n",
            "step 23960: train loss = 1.8123, val loss = 1.7169\n",
            "step 23970: train loss = 1.8898, val loss = 1.7297\n",
            "step 23980: train loss = 1.7916, val loss = 1.9504\n",
            "step 23990: train loss = 1.8095, val loss = 2.0129\n",
            "step 24000: train loss = 1.9096, val loss = 2.0449\n",
            "step 24010: train loss = 1.8977, val loss = 1.8325\n",
            "step 24020: train loss = 1.8166, val loss = 1.8656\n",
            "step 24030: train loss = 1.7904, val loss = 1.9282\n",
            "step 24040: train loss = 1.9013, val loss = 1.9466\n",
            "step 24050: train loss = 1.8599, val loss = 1.7018\n",
            "step 24060: train loss = 1.7625, val loss = 1.9026\n",
            "step 24070: train loss = 1.8926, val loss = 1.8720\n",
            "step 24080: train loss = 1.9603, val loss = 1.8604\n",
            "step 24090: train loss = 1.8199, val loss = 1.9503\n",
            "step 24100: train loss = 1.9134, val loss = 1.8946\n",
            "step 24110: train loss = 1.9972, val loss = 1.8791\n",
            "step 24120: train loss = 1.8970, val loss = 1.8634\n",
            "step 24130: train loss = 1.8591, val loss = 1.8490\n",
            "step 24140: train loss = 1.7603, val loss = 1.8563\n",
            "step 24150: train loss = 1.9154, val loss = 1.8873\n",
            "step 24160: train loss = 1.8771, val loss = 1.9257\n",
            "step 24170: train loss = 1.9108, val loss = 1.7962\n",
            "step 24180: train loss = 1.9248, val loss = 1.7811\n",
            "step 24190: train loss = 1.8278, val loss = 1.8711\n",
            "step 24200: train loss = 1.9493, val loss = 1.8747\n",
            "step 24210: train loss = 1.8807, val loss = 1.7505\n",
            "step 24220: train loss = 1.8545, val loss = 1.9389\n",
            "step 24230: train loss = 1.9012, val loss = 1.8369\n",
            "step 24240: train loss = 1.9733, val loss = 1.9900\n",
            "step 24250: train loss = 1.8177, val loss = 1.9218\n",
            "step 24260: train loss = 1.8441, val loss = 1.9932\n",
            "step 24270: train loss = 1.8848, val loss = 1.9635\n",
            "step 24280: train loss = 1.8481, val loss = 2.0424\n",
            "step 24290: train loss = 2.0840, val loss = 1.9746\n",
            "step 24300: train loss = 1.8997, val loss = 1.9080\n",
            "step 24310: train loss = 1.8505, val loss = 1.9365\n",
            "step 24320: train loss = 1.8362, val loss = 1.7902\n",
            "step 24330: train loss = 1.8599, val loss = 2.1466\n",
            "step 24340: train loss = 1.8993, val loss = 1.8730\n",
            "step 24350: train loss = 1.8689, val loss = 1.8257\n",
            "step 24360: train loss = 2.0327, val loss = 1.8213\n",
            "step 24370: train loss = 1.9469, val loss = 1.8092\n",
            "step 24380: train loss = 1.8762, val loss = 1.9880\n",
            "step 24390: train loss = 1.9805, val loss = 1.8287\n",
            "step 24400: train loss = 1.9010, val loss = 1.9099\n",
            "step 24410: train loss = 1.8482, val loss = 1.7774\n",
            "step 24420: train loss = 1.8274, val loss = 1.8120\n",
            "step 24430: train loss = 1.8300, val loss = 1.8273\n",
            "step 24440: train loss = 1.7840, val loss = 1.7469\n",
            "step 24450: train loss = 1.7959, val loss = 1.9972\n",
            "step 24460: train loss = 1.9918, val loss = 1.7820\n",
            "step 24470: train loss = 1.9049, val loss = 1.8387\n",
            "step 24480: train loss = 1.9723, val loss = 1.8066\n",
            "step 24490: train loss = 1.8888, val loss = 1.8845\n",
            "step 24500: train loss = 1.9576, val loss = 1.8860\n",
            "step 24510: train loss = 1.9643, val loss = 1.9084\n",
            "step 24520: train loss = 1.9560, val loss = 1.9934\n",
            "step 24530: train loss = 1.9090, val loss = 2.0420\n",
            "step 24540: train loss = 1.8465, val loss = 1.8250\n",
            "step 24550: train loss = 1.9159, val loss = 1.9989\n",
            "step 24560: train loss = 1.8242, val loss = 2.0012\n",
            "step 24570: train loss = 1.9430, val loss = 1.9601\n",
            "step 24580: train loss = 1.9517, val loss = 1.8950\n",
            "step 24590: train loss = 1.8768, val loss = 1.8677\n",
            "step 24600: train loss = 1.8264, val loss = 1.8379\n",
            "step 24610: train loss = 1.8623, val loss = 1.8508\n",
            "step 24620: train loss = 1.8407, val loss = 1.8383\n",
            "step 24630: train loss = 1.9365, val loss = 2.0662\n",
            "step 24640: train loss = 1.8967, val loss = 1.8873\n",
            "step 24650: train loss = 1.9233, val loss = 1.9955\n",
            "step 24660: train loss = 1.7028, val loss = 1.9301\n",
            "step 24670: train loss = 1.9069, val loss = 1.9316\n",
            "step 24680: train loss = 1.7547, val loss = 1.9289\n",
            "step 24690: train loss = 1.9444, val loss = 1.8146\n",
            "step 24700: train loss = 2.0368, val loss = 1.9167\n",
            "step 24710: train loss = 2.0544, val loss = 1.8948\n",
            "step 24720: train loss = 1.8046, val loss = 2.0360\n",
            "step 24730: train loss = 2.0353, val loss = 2.0242\n",
            "step 24740: train loss = 1.8633, val loss = 1.9036\n",
            "step 24750: train loss = 2.0242, val loss = 2.0140\n",
            "step 24760: train loss = 1.7850, val loss = 1.9146\n",
            "step 24770: train loss = 1.8542, val loss = 1.7749\n",
            "step 24780: train loss = 1.8962, val loss = 1.8156\n",
            "step 24790: train loss = 1.9056, val loss = 1.8729\n",
            "step 24800: train loss = 1.8853, val loss = 1.8290\n",
            "step 24810: train loss = 1.8335, val loss = 1.8128\n",
            "step 24820: train loss = 2.0339, val loss = 1.7887\n",
            "step 24830: train loss = 1.8570, val loss = 1.7912\n",
            "step 24840: train loss = 1.7709, val loss = 1.7939\n",
            "step 24850: train loss = 1.8534, val loss = 1.8115\n",
            "step 24860: train loss = 1.9132, val loss = 1.8498\n",
            "step 24870: train loss = 2.0568, val loss = 1.8578\n",
            "step 24880: train loss = 1.8629, val loss = 1.8318\n",
            "step 24890: train loss = 1.9223, val loss = 1.8358\n",
            "step 24900: train loss = 1.9717, val loss = 1.9285\n",
            "step 24910: train loss = 1.7779, val loss = 1.8111\n",
            "step 24920: train loss = 1.9384, val loss = 1.8911\n",
            "step 24930: train loss = 1.7713, val loss = 2.0296\n",
            "step 24940: train loss = 1.8042, val loss = 2.0443\n",
            "step 24950: train loss = 1.9174, val loss = 1.8840\n",
            "step 24960: train loss = 1.8310, val loss = 1.9897\n",
            "step 24970: train loss = 1.8273, val loss = 1.9099\n",
            "step 24980: train loss = 2.0374, val loss = 1.9588\n",
            "step 24990: train loss = 1.8906, val loss = 1.8136\n",
            "step 25000: train loss = 1.7985, val loss = 1.9013\n",
            "step 25010: train loss = 1.8652, val loss = 1.9960\n",
            "step 25020: train loss = 1.8255, val loss = 1.8068\n",
            "step 25030: train loss = 1.8934, val loss = 1.8080\n",
            "step 25040: train loss = 1.8082, val loss = 1.9140\n",
            "step 25050: train loss = 1.8797, val loss = 1.9188\n",
            "step 25060: train loss = 2.0030, val loss = 1.8920\n",
            "step 25070: train loss = 1.8409, val loss = 1.8641\n",
            "step 25080: train loss = 1.9526, val loss = 2.0130\n",
            "step 25090: train loss = 1.7657, val loss = 1.8148\n",
            "step 25100: train loss = 1.7756, val loss = 1.8144\n",
            "step 25110: train loss = 1.8229, val loss = 1.8354\n",
            "step 25120: train loss = 1.9615, val loss = 1.9651\n",
            "step 25130: train loss = 1.8121, val loss = 1.8607\n",
            "step 25140: train loss = 2.0052, val loss = 1.8371\n",
            "step 25150: train loss = 1.8591, val loss = 1.8050\n",
            "step 25160: train loss = 1.9041, val loss = 1.8011\n",
            "step 25170: train loss = 1.9365, val loss = 1.8571\n",
            "step 25180: train loss = 1.8859, val loss = 1.9777\n",
            "step 25190: train loss = 1.8872, val loss = 1.8743\n",
            "step 25200: train loss = 1.8484, val loss = 1.9819\n",
            "step 25210: train loss = 1.9604, val loss = 1.8867\n",
            "step 25220: train loss = 2.0476, val loss = 1.8039\n",
            "step 25230: train loss = 1.7311, val loss = 1.7893\n",
            "step 25240: train loss = 1.9461, val loss = 1.9014\n",
            "step 25250: train loss = 1.8133, val loss = 1.9989\n",
            "step 25260: train loss = 1.7647, val loss = 1.9763\n",
            "step 25270: train loss = 1.8692, val loss = 2.0078\n",
            "step 25280: train loss = 1.8502, val loss = 1.8726\n",
            "step 25290: train loss = 1.9565, val loss = 1.8928\n",
            "step 25300: train loss = 1.9479, val loss = 1.8028\n",
            "step 25310: train loss = 1.8203, val loss = 1.7961\n",
            "step 25320: train loss = 1.9917, val loss = 1.7671\n",
            "step 25330: train loss = 1.9612, val loss = 1.9206\n",
            "step 25340: train loss = 1.9713, val loss = 1.8523\n",
            "step 25350: train loss = 1.8133, val loss = 1.7563\n",
            "step 25360: train loss = 1.8738, val loss = 1.8987\n",
            "step 25370: train loss = 1.9081, val loss = 1.7821\n",
            "step 25380: train loss = 1.7799, val loss = 1.8609\n",
            "step 25390: train loss = 1.9369, val loss = 1.9072\n",
            "step 25400: train loss = 1.8909, val loss = 1.9475\n",
            "step 25410: train loss = 1.8363, val loss = 1.9523\n",
            "step 25420: train loss = 1.8739, val loss = 2.0406\n",
            "step 25430: train loss = 1.8211, val loss = 1.8116\n",
            "step 25440: train loss = 1.8774, val loss = 2.0441\n",
            "step 25450: train loss = 1.9273, val loss = 1.9823\n",
            "step 25460: train loss = 1.7858, val loss = 1.8314\n",
            "step 25470: train loss = 1.7987, val loss = 1.8996\n",
            "step 25480: train loss = 1.8084, val loss = 1.8882\n",
            "step 25490: train loss = 1.8622, val loss = 1.7954\n",
            "step 25500: train loss = 1.8603, val loss = 1.8722\n",
            "step 25510: train loss = 1.9376, val loss = 1.7221\n",
            "step 25520: train loss = 1.9496, val loss = 1.9681\n",
            "step 25530: train loss = 1.8346, val loss = 1.8994\n",
            "step 25540: train loss = 1.7799, val loss = 1.8970\n",
            "step 25550: train loss = 1.7956, val loss = 1.9603\n",
            "step 25560: train loss = 1.8985, val loss = 1.9073\n",
            "step 25570: train loss = 2.0114, val loss = 1.8684\n",
            "step 25580: train loss = 1.7741, val loss = 1.6996\n",
            "step 25590: train loss = 1.9517, val loss = 1.8855\n",
            "step 25600: train loss = 1.9708, val loss = 1.9763\n",
            "step 25610: train loss = 1.8478, val loss = 1.8864\n",
            "step 25620: train loss = 2.0386, val loss = 1.8708\n",
            "step 25630: train loss = 1.8434, val loss = 1.8608\n",
            "step 25640: train loss = 1.8823, val loss = 1.9068\n",
            "step 25650: train loss = 2.0579, val loss = 1.9676\n",
            "step 25660: train loss = 1.7911, val loss = 1.7901\n",
            "step 25670: train loss = 1.8136, val loss = 1.9153\n",
            "step 25680: train loss = 1.8323, val loss = 1.9138\n",
            "step 25690: train loss = 1.8252, val loss = 2.0298\n",
            "step 25700: train loss = 1.8635, val loss = 1.9960\n",
            "step 25710: train loss = 1.7816, val loss = 1.8289\n",
            "step 25720: train loss = 2.0219, val loss = 1.7155\n",
            "step 25730: train loss = 1.8009, val loss = 1.8591\n",
            "step 25740: train loss = 1.8144, val loss = 1.9788\n",
            "step 25750: train loss = 1.8437, val loss = 1.7608\n",
            "step 25760: train loss = 1.9841, val loss = 1.8669\n",
            "step 25770: train loss = 2.0035, val loss = 1.9851\n",
            "step 25780: train loss = 1.9421, val loss = 1.8054\n",
            "step 25790: train loss = 1.7424, val loss = 1.9032\n",
            "step 25800: train loss = 2.0368, val loss = 1.8534\n",
            "step 25810: train loss = 1.8401, val loss = 1.9213\n",
            "step 25820: train loss = 1.9434, val loss = 1.8201\n",
            "step 25830: train loss = 1.7902, val loss = 1.8004\n",
            "step 25840: train loss = 1.8338, val loss = 1.7982\n",
            "step 25850: train loss = 1.9092, val loss = 1.8164\n",
            "step 25860: train loss = 2.0231, val loss = 1.9124\n",
            "step 25870: train loss = 1.9644, val loss = 1.7523\n",
            "step 25880: train loss = 1.8337, val loss = 1.9349\n",
            "step 25890: train loss = 1.9334, val loss = 1.6821\n",
            "step 25900: train loss = 1.7867, val loss = 1.8657\n",
            "step 25910: train loss = 2.0079, val loss = 1.9575\n",
            "step 25920: train loss = 1.8420, val loss = 2.0216\n",
            "step 25930: train loss = 2.0254, val loss = 1.8890\n",
            "step 25940: train loss = 1.9590, val loss = 1.9143\n",
            "step 25950: train loss = 1.9141, val loss = 1.8594\n",
            "step 25960: train loss = 1.7604, val loss = 1.9140\n",
            "step 25970: train loss = 1.7922, val loss = 1.9369\n",
            "step 25980: train loss = 1.9357, val loss = 1.8501\n",
            "step 25990: train loss = 1.8732, val loss = 1.7061\n",
            "step 26000: train loss = 1.8152, val loss = 1.9513\n",
            "step 26010: train loss = 1.7603, val loss = 1.9126\n",
            "step 26020: train loss = 1.8948, val loss = 1.7548\n",
            "step 26030: train loss = 1.9108, val loss = 1.8450\n",
            "step 26040: train loss = 1.8544, val loss = 1.8267\n",
            "step 26050: train loss = 1.9188, val loss = 1.8435\n",
            "step 26060: train loss = 1.8525, val loss = 1.8615\n",
            "step 26070: train loss = 2.0175, val loss = 1.8197\n",
            "step 26080: train loss = 1.7390, val loss = 1.8448\n",
            "step 26090: train loss = 1.9258, val loss = 1.6331\n",
            "step 26100: train loss = 1.7865, val loss = 1.9838\n",
            "step 26110: train loss = 1.6976, val loss = 1.8694\n",
            "step 26120: train loss = 1.8117, val loss = 1.9342\n",
            "step 26130: train loss = 1.7788, val loss = 1.8635\n",
            "step 26140: train loss = 1.8763, val loss = 2.0010\n",
            "step 26150: train loss = 1.7922, val loss = 1.8889\n",
            "step 26160: train loss = 1.7826, val loss = 1.8977\n",
            "step 26170: train loss = 1.8854, val loss = 2.0702\n",
            "step 26180: train loss = 1.9505, val loss = 1.8633\n",
            "step 26190: train loss = 1.9158, val loss = 1.8521\n",
            "step 26200: train loss = 1.8737, val loss = 1.7407\n",
            "step 26210: train loss = 1.8297, val loss = 1.7766\n",
            "step 26220: train loss = 1.7410, val loss = 1.8035\n",
            "step 26230: train loss = 1.8184, val loss = 1.9830\n",
            "step 26240: train loss = 1.8298, val loss = 1.8925\n",
            "step 26250: train loss = 1.8071, val loss = 1.7785\n",
            "step 26260: train loss = 1.9194, val loss = 2.0749\n",
            "step 26270: train loss = 1.8292, val loss = 1.9832\n",
            "step 26280: train loss = 1.7477, val loss = 1.8816\n",
            "step 26290: train loss = 1.9805, val loss = 1.7982\n",
            "step 26300: train loss = 1.8907, val loss = 1.9223\n",
            "step 26310: train loss = 1.9928, val loss = 1.8706\n",
            "step 26320: train loss = 1.8321, val loss = 1.9435\n",
            "step 26330: train loss = 1.8679, val loss = 1.9150\n",
            "step 26340: train loss = 1.8912, val loss = 1.8724\n",
            "step 26350: train loss = 1.7422, val loss = 1.9263\n",
            "step 26360: train loss = 1.9262, val loss = 2.0235\n",
            "step 26370: train loss = 1.9747, val loss = 1.8559\n",
            "step 26380: train loss = 1.9540, val loss = 1.6842\n",
            "step 26390: train loss = 1.6973, val loss = 1.9254\n",
            "step 26400: train loss = 1.8607, val loss = 1.8353\n",
            "step 26410: train loss = 1.7969, val loss = 1.7429\n",
            "step 26420: train loss = 1.9098, val loss = 1.9429\n",
            "step 26430: train loss = 1.7516, val loss = 1.7936\n",
            "step 26440: train loss = 1.8251, val loss = 1.9023\n",
            "step 26450: train loss = 1.8908, val loss = 1.8994\n",
            "step 26460: train loss = 1.9650, val loss = 1.8963\n",
            "step 26470: train loss = 1.9377, val loss = 1.8694\n",
            "step 26480: train loss = 1.8491, val loss = 2.1631\n",
            "step 26490: train loss = 1.9313, val loss = 1.8782\n",
            "step 26500: train loss = 1.8767, val loss = 1.8923\n",
            "step 26510: train loss = 1.9182, val loss = 1.8044\n",
            "step 26520: train loss = 1.7803, val loss = 1.8675\n",
            "step 26530: train loss = 1.9287, val loss = 1.9883\n",
            "step 26540: train loss = 1.9603, val loss = 1.9292\n",
            "step 26550: train loss = 1.9281, val loss = 1.8993\n",
            "step 26560: train loss = 1.7752, val loss = 1.7659\n",
            "step 26570: train loss = 1.9874, val loss = 1.8669\n",
            "step 26580: train loss = 1.9859, val loss = 2.0222\n",
            "step 26590: train loss = 1.7171, val loss = 1.9592\n",
            "step 26600: train loss = 1.9216, val loss = 1.8882\n",
            "step 26610: train loss = 1.8681, val loss = 1.8763\n",
            "step 26620: train loss = 1.9282, val loss = 1.8246\n",
            "step 26630: train loss = 1.7775, val loss = 1.8434\n",
            "step 26640: train loss = 1.8348, val loss = 1.7927\n",
            "step 26650: train loss = 1.8681, val loss = 1.9617\n",
            "step 26660: train loss = 1.8588, val loss = 1.8474\n",
            "step 26670: train loss = 1.9885, val loss = 1.8975\n",
            "step 26680: train loss = 2.0346, val loss = 1.9489\n",
            "step 26690: train loss = 1.8174, val loss = 1.8371\n",
            "step 26700: train loss = 1.8616, val loss = 1.8873\n",
            "step 26710: train loss = 1.8588, val loss = 1.9363\n",
            "step 26720: train loss = 1.9663, val loss = 1.8801\n",
            "step 26730: train loss = 1.9409, val loss = 1.8872\n",
            "step 26740: train loss = 1.9281, val loss = 1.8347\n",
            "step 26750: train loss = 1.8000, val loss = 1.9752\n",
            "step 26760: train loss = 1.9656, val loss = 1.7871\n",
            "step 26770: train loss = 1.8985, val loss = 1.9851\n",
            "step 26780: train loss = 1.8657, val loss = 2.0308\n",
            "step 26790: train loss = 1.8676, val loss = 1.9551\n",
            "step 26800: train loss = 1.9948, val loss = 1.9488\n",
            "step 26810: train loss = 1.7057, val loss = 1.8486\n",
            "step 26820: train loss = 1.8186, val loss = 1.8559\n",
            "step 26830: train loss = 1.9371, val loss = 1.9844\n",
            "step 26840: train loss = 1.9856, val loss = 1.8546\n",
            "step 26850: train loss = 1.9642, val loss = 1.9243\n",
            "step 26860: train loss = 1.8838, val loss = 1.8366\n",
            "step 26870: train loss = 1.9687, val loss = 1.8909\n",
            "step 26880: train loss = 1.8492, val loss = 1.7832\n",
            "step 26890: train loss = 1.8351, val loss = 1.8402\n",
            "step 26900: train loss = 1.9487, val loss = 1.8518\n",
            "step 26910: train loss = 1.9920, val loss = 1.9319\n",
            "step 26920: train loss = 1.9116, val loss = 1.9026\n",
            "step 26930: train loss = 1.8449, val loss = 2.0649\n",
            "step 26940: train loss = 1.8306, val loss = 1.9713\n",
            "step 26950: train loss = 1.9244, val loss = 1.9379\n",
            "step 26960: train loss = 1.8917, val loss = 1.8468\n",
            "step 26970: train loss = 1.8998, val loss = 1.8286\n",
            "step 26980: train loss = 1.7699, val loss = 1.8965\n",
            "step 26990: train loss = 1.8695, val loss = 1.8510\n",
            "step 27000: train loss = 1.9751, val loss = 1.8737\n",
            "step 27010: train loss = 1.9218, val loss = 1.8547\n",
            "step 27020: train loss = 1.8248, val loss = 1.7581\n",
            "step 27030: train loss = 1.8685, val loss = 1.7952\n",
            "step 27040: train loss = 1.9131, val loss = 1.8285\n",
            "step 27050: train loss = 1.7991, val loss = 1.8758\n",
            "step 27060: train loss = 1.7719, val loss = 1.9047\n",
            "step 27070: train loss = 1.8482, val loss = 1.8807\n",
            "step 27080: train loss = 1.9032, val loss = 1.9623\n",
            "step 27090: train loss = 1.7544, val loss = 1.9434\n",
            "step 27100: train loss = 2.0170, val loss = 1.7605\n",
            "step 27110: train loss = 1.8059, val loss = 1.9007\n",
            "step 27120: train loss = 1.9250, val loss = 1.9534\n",
            "step 27130: train loss = 1.8309, val loss = 1.7917\n",
            "step 27140: train loss = 1.9434, val loss = 1.8128\n",
            "step 27150: train loss = 1.9689, val loss = 1.8450\n",
            "step 27160: train loss = 2.0156, val loss = 1.8838\n",
            "step 27170: train loss = 1.9356, val loss = 1.9562\n",
            "step 27180: train loss = 1.8837, val loss = 2.0743\n",
            "step 27190: train loss = 1.7706, val loss = 1.9091\n",
            "step 27200: train loss = 1.8610, val loss = 1.9687\n",
            "step 27210: train loss = 1.9948, val loss = 1.8271\n",
            "step 27220: train loss = 1.8563, val loss = 1.6797\n",
            "step 27230: train loss = 1.8927, val loss = 1.9784\n",
            "step 27240: train loss = 1.8747, val loss = 1.8881\n",
            "step 27250: train loss = 1.8849, val loss = 1.8690\n",
            "step 27260: train loss = 2.0509, val loss = 1.9644\n",
            "step 27270: train loss = 1.9200, val loss = 1.9746\n",
            "step 27280: train loss = 1.9338, val loss = 1.8399\n",
            "step 27290: train loss = 1.8072, val loss = 1.9670\n",
            "step 27300: train loss = 1.9638, val loss = 1.7939\n",
            "step 27310: train loss = 1.8819, val loss = 1.9615\n",
            "step 27320: train loss = 1.7935, val loss = 1.8001\n",
            "step 27330: train loss = 1.8850, val loss = 1.9795\n",
            "step 27340: train loss = 1.9233, val loss = 1.9428\n",
            "step 27350: train loss = 1.9864, val loss = 1.8807\n",
            "step 27360: train loss = 1.7957, val loss = 2.0822\n",
            "step 27370: train loss = 1.9105, val loss = 1.9868\n",
            "step 27380: train loss = 1.9128, val loss = 1.7517\n",
            "step 27390: train loss = 1.8885, val loss = 1.6507\n",
            "step 27400: train loss = 1.7880, val loss = 1.8327\n",
            "step 27410: train loss = 1.9470, val loss = 1.9240\n",
            "step 27420: train loss = 1.7958, val loss = 1.8861\n",
            "step 27430: train loss = 1.7701, val loss = 1.7618\n",
            "step 27440: train loss = 2.0364, val loss = 1.8608\n",
            "step 27450: train loss = 1.7488, val loss = 1.9923\n",
            "step 27460: train loss = 1.9403, val loss = 1.9988\n",
            "step 27470: train loss = 1.7823, val loss = 2.0240\n",
            "step 27480: train loss = 1.7347, val loss = 1.8716\n",
            "step 27490: train loss = 1.9323, val loss = 1.9180\n",
            "step 27500: train loss = 2.0069, val loss = 1.8242\n",
            "step 27510: train loss = 1.8719, val loss = 1.9022\n",
            "step 27520: train loss = 1.9366, val loss = 2.0226\n",
            "step 27530: train loss = 1.9014, val loss = 1.9516\n",
            "step 27540: train loss = 1.7975, val loss = 2.0319\n",
            "step 27550: train loss = 1.8882, val loss = 1.8403\n",
            "step 27560: train loss = 1.8365, val loss = 1.9834\n",
            "step 27570: train loss = 1.8453, val loss = 1.8370\n",
            "step 27580: train loss = 1.9909, val loss = 1.8648\n",
            "step 27590: train loss = 1.8565, val loss = 1.8132\n",
            "step 27600: train loss = 2.0962, val loss = 1.8789\n",
            "step 27610: train loss = 1.9177, val loss = 1.9018\n",
            "step 27620: train loss = 1.7946, val loss = 1.8572\n",
            "step 27630: train loss = 1.8972, val loss = 1.9914\n",
            "step 27640: train loss = 1.8125, val loss = 1.7904\n",
            "step 27650: train loss = 1.8516, val loss = 1.8562\n",
            "step 27660: train loss = 1.9291, val loss = 1.8893\n",
            "step 27670: train loss = 2.0339, val loss = 1.8211\n",
            "step 27680: train loss = 2.0612, val loss = 1.9287\n",
            "step 27690: train loss = 1.9512, val loss = 1.8500\n",
            "step 27700: train loss = 1.8115, val loss = 1.9273\n",
            "step 27710: train loss = 1.7155, val loss = 1.8365\n",
            "step 27720: train loss = 2.0575, val loss = 1.9019\n",
            "step 27730: train loss = 2.0591, val loss = 1.7739\n",
            "step 27740: train loss = 1.8420, val loss = 1.9307\n",
            "step 27750: train loss = 1.8737, val loss = 1.9919\n",
            "step 27760: train loss = 1.9476, val loss = 1.9101\n",
            "step 27770: train loss = 1.8625, val loss = 1.9248\n",
            "step 27780: train loss = 1.7867, val loss = 1.9044\n",
            "step 27790: train loss = 1.9530, val loss = 1.9358\n",
            "step 27800: train loss = 1.8272, val loss = 1.8125\n",
            "step 27810: train loss = 1.9814, val loss = 1.7758\n",
            "step 27820: train loss = 1.8614, val loss = 1.9228\n",
            "step 27830: train loss = 1.9428, val loss = 1.9995\n",
            "step 27840: train loss = 1.8677, val loss = 1.7662\n",
            "step 27850: train loss = 1.7264, val loss = 1.7811\n",
            "step 27860: train loss = 1.8074, val loss = 1.8077\n",
            "step 27870: train loss = 1.7654, val loss = 1.9507\n",
            "step 27880: train loss = 1.9039, val loss = 1.7646\n",
            "step 27890: train loss = 1.8371, val loss = 1.9726\n",
            "step 27900: train loss = 1.8165, val loss = 1.8825\n",
            "step 27910: train loss = 1.9054, val loss = 1.9913\n",
            "step 27920: train loss = 1.9633, val loss = 1.8044\n",
            "step 27930: train loss = 1.7409, val loss = 1.9105\n",
            "step 27940: train loss = 1.8799, val loss = 1.8541\n",
            "step 27950: train loss = 1.9427, val loss = 1.8854\n",
            "step 27960: train loss = 1.8212, val loss = 1.8948\n",
            "step 27970: train loss = 1.9524, val loss = 2.0881\n",
            "step 27980: train loss = 1.8955, val loss = 1.9383\n",
            "step 27990: train loss = 1.9839, val loss = 1.8726\n",
            "step 28000: train loss = 1.8266, val loss = 1.7387\n",
            "step 28010: train loss = 1.9776, val loss = 2.0057\n",
            "step 28020: train loss = 1.9868, val loss = 1.7533\n",
            "step 28030: train loss = 1.9611, val loss = 1.7850\n",
            "step 28040: train loss = 1.7918, val loss = 1.7956\n",
            "step 28050: train loss = 1.9297, val loss = 2.0353\n",
            "step 28060: train loss = 1.8162, val loss = 1.9588\n",
            "step 28070: train loss = 1.9036, val loss = 1.8500\n",
            "step 28080: train loss = 1.6710, val loss = 1.8558\n",
            "step 28090: train loss = 2.2084, val loss = 2.0047\n",
            "step 28100: train loss = 1.9044, val loss = 1.8791\n",
            "step 28110: train loss = 1.9063, val loss = 1.9357\n",
            "step 28120: train loss = 1.9124, val loss = 1.8137\n",
            "step 28130: train loss = 1.8802, val loss = 1.7772\n",
            "step 28140: train loss = 1.9067, val loss = 1.9393\n",
            "step 28150: train loss = 1.9781, val loss = 2.0334\n",
            "step 28160: train loss = 1.9536, val loss = 2.0167\n",
            "step 28170: train loss = 1.8589, val loss = 1.9215\n",
            "step 28180: train loss = 1.8953, val loss = 2.0004\n",
            "step 28190: train loss = 1.8217, val loss = 1.8576\n",
            "step 28200: train loss = 1.7237, val loss = 1.7705\n",
            "step 28210: train loss = 1.8033, val loss = 1.7381\n",
            "step 28220: train loss = 1.8093, val loss = 1.8572\n",
            "step 28230: train loss = 1.9293, val loss = 2.0389\n",
            "step 28240: train loss = 1.8332, val loss = 1.8031\n",
            "step 28250: train loss = 1.7907, val loss = 2.0225\n",
            "step 28260: train loss = 1.8243, val loss = 1.9552\n",
            "step 28270: train loss = 1.8646, val loss = 1.8119\n",
            "step 28280: train loss = 1.8678, val loss = 1.8569\n",
            "step 28290: train loss = 1.8719, val loss = 1.9202\n",
            "step 28300: train loss = 1.9897, val loss = 1.9585\n",
            "step 28310: train loss = 1.7892, val loss = 1.7911\n",
            "step 28320: train loss = 2.0265, val loss = 1.9479\n",
            "step 28330: train loss = 1.9266, val loss = 2.0075\n",
            "step 28340: train loss = 2.1069, val loss = 1.8565\n",
            "step 28350: train loss = 1.8585, val loss = 1.6838\n",
            "step 28360: train loss = 1.9155, val loss = 1.8594\n",
            "step 28370: train loss = 1.9567, val loss = 1.8646\n",
            "step 28380: train loss = 1.9214, val loss = 1.8484\n",
            "step 28390: train loss = 1.8596, val loss = 1.8277\n",
            "step 28400: train loss = 1.8953, val loss = 2.0445\n",
            "step 28410: train loss = 1.8970, val loss = 1.9439\n",
            "step 28420: train loss = 2.0831, val loss = 1.9093\n",
            "step 28430: train loss = 1.8579, val loss = 1.7979\n",
            "step 28440: train loss = 1.8130, val loss = 1.8657\n",
            "step 28450: train loss = 1.8858, val loss = 1.9381\n",
            "step 28460: train loss = 2.0477, val loss = 2.0371\n",
            "step 28470: train loss = 1.8219, val loss = 1.9313\n",
            "step 28480: train loss = 1.8396, val loss = 1.8989\n",
            "step 28490: train loss = 1.6434, val loss = 1.8241\n",
            "step 28500: train loss = 1.9375, val loss = 1.8651\n",
            "step 28510: train loss = 1.9301, val loss = 1.8959\n",
            "step 28520: train loss = 1.8794, val loss = 1.9155\n",
            "step 28530: train loss = 1.9315, val loss = 2.0190\n",
            "step 28540: train loss = 1.9563, val loss = 1.8376\n",
            "step 28550: train loss = 1.7232, val loss = 1.7711\n",
            "step 28560: train loss = 1.8411, val loss = 1.9546\n",
            "step 28570: train loss = 1.7855, val loss = 1.8632\n",
            "step 28580: train loss = 1.9474, val loss = 1.9536\n",
            "step 28590: train loss = 1.8757, val loss = 2.0238\n",
            "step 28600: train loss = 2.0110, val loss = 2.1117\n",
            "step 28610: train loss = 1.9435, val loss = 1.9768\n",
            "step 28620: train loss = 1.8757, val loss = 1.9823\n",
            "step 28630: train loss = 1.8838, val loss = 1.8666\n",
            "step 28640: train loss = 1.8717, val loss = 1.9840\n",
            "step 28650: train loss = 1.8653, val loss = 1.7204\n",
            "step 28660: train loss = 1.7650, val loss = 1.9040\n",
            "step 28670: train loss = 2.0500, val loss = 1.8084\n",
            "step 28680: train loss = 1.8596, val loss = 1.8182\n",
            "step 28690: train loss = 1.8884, val loss = 1.8817\n",
            "step 28700: train loss = 1.8984, val loss = 1.9727\n",
            "step 28710: train loss = 1.8929, val loss = 2.0116\n",
            "step 28720: train loss = 1.9742, val loss = 1.8224\n",
            "step 28730: train loss = 1.9189, val loss = 1.7829\n",
            "step 28740: train loss = 1.7660, val loss = 1.7403\n",
            "step 28750: train loss = 1.9305, val loss = 1.9293\n",
            "step 28760: train loss = 1.8369, val loss = 1.9318\n",
            "step 28770: train loss = 1.8836, val loss = 1.9532\n",
            "step 28780: train loss = 1.9309, val loss = 1.8907\n",
            "step 28790: train loss = 1.9063, val loss = 1.9533\n",
            "step 28800: train loss = 1.9862, val loss = 1.8824\n",
            "step 28810: train loss = 1.9235, val loss = 1.9447\n",
            "step 28820: train loss = 1.8268, val loss = 1.8565\n",
            "step 28830: train loss = 1.9466, val loss = 1.9400\n",
            "step 28840: train loss = 1.7534, val loss = 1.8872\n",
            "step 28850: train loss = 1.7684, val loss = 1.9148\n",
            "step 28860: train loss = 1.9224, val loss = 1.9039\n",
            "step 28870: train loss = 1.8921, val loss = 1.8727\n",
            "step 28880: train loss = 1.7350, val loss = 2.0641\n",
            "step 28890: train loss = 2.0314, val loss = 1.9006\n",
            "step 28900: train loss = 1.8838, val loss = 1.9226\n",
            "step 28910: train loss = 1.9241, val loss = 1.9543\n",
            "step 28920: train loss = 1.7947, val loss = 1.8979\n",
            "step 28930: train loss = 1.8980, val loss = 1.9330\n",
            "step 28940: train loss = 1.8575, val loss = 2.0547\n",
            "step 28950: train loss = 1.8313, val loss = 1.8989\n",
            "step 28960: train loss = 1.9361, val loss = 1.9278\n",
            "step 28970: train loss = 1.7941, val loss = 1.9208\n",
            "step 28980: train loss = 1.8719, val loss = 1.8438\n",
            "step 28990: train loss = 1.8747, val loss = 1.9280\n",
            "step 29000: train loss = 1.8344, val loss = 1.9466\n",
            "step 29010: train loss = 1.8736, val loss = 1.7186\n",
            "step 29020: train loss = 2.0003, val loss = 1.9025\n",
            "step 29030: train loss = 1.7938, val loss = 1.8791\n",
            "step 29040: train loss = 1.8799, val loss = 1.8547\n",
            "step 29050: train loss = 2.0268, val loss = 1.9260\n",
            "step 29060: train loss = 1.9031, val loss = 1.8434\n",
            "step 29070: train loss = 1.7948, val loss = 1.8124\n",
            "step 29080: train loss = 1.8825, val loss = 2.0218\n",
            "step 29090: train loss = 1.9010, val loss = 1.8095\n",
            "step 29100: train loss = 1.7651, val loss = 2.0707\n",
            "step 29110: train loss = 1.8285, val loss = 1.8876\n",
            "step 29120: train loss = 1.8455, val loss = 2.0121\n",
            "step 29130: train loss = 1.9197, val loss = 1.8375\n",
            "step 29140: train loss = 1.9367, val loss = 1.9888\n",
            "step 29150: train loss = 2.0065, val loss = 1.9417\n",
            "step 29160: train loss = 1.9625, val loss = 1.8911\n",
            "step 29170: train loss = 1.8119, val loss = 1.9602\n",
            "step 29180: train loss = 1.8615, val loss = 1.9269\n",
            "step 29190: train loss = 2.0266, val loss = 1.8433\n",
            "step 29200: train loss = 2.0178, val loss = 1.8282\n",
            "step 29210: train loss = 2.0408, val loss = 1.8057\n",
            "step 29220: train loss = 2.1093, val loss = 1.8575\n",
            "step 29230: train loss = 1.9969, val loss = 1.9683\n",
            "step 29240: train loss = 1.7850, val loss = 1.8298\n",
            "step 29250: train loss = 1.8511, val loss = 2.0555\n",
            "step 29260: train loss = 2.0044, val loss = 1.9182\n",
            "step 29270: train loss = 1.8164, val loss = 1.9425\n",
            "step 29280: train loss = 1.9600, val loss = 1.8454\n",
            "step 29290: train loss = 1.9245, val loss = 1.8415\n",
            "step 29300: train loss = 2.0224, val loss = 1.8296\n",
            "step 29310: train loss = 1.8018, val loss = 1.9226\n",
            "step 29320: train loss = 1.9634, val loss = 1.9319\n",
            "step 29330: train loss = 1.9394, val loss = 1.8858\n",
            "step 29340: train loss = 1.8373, val loss = 1.9327\n",
            "step 29350: train loss = 1.8488, val loss = 1.9020\n",
            "step 29360: train loss = 1.8993, val loss = 1.9579\n",
            "step 29370: train loss = 1.9172, val loss = 1.8828\n",
            "step 29380: train loss = 1.9197, val loss = 1.8275\n",
            "step 29390: train loss = 1.9295, val loss = 1.9352\n",
            "step 29400: train loss = 1.7968, val loss = 1.8892\n",
            "step 29410: train loss = 1.9121, val loss = 1.8311\n",
            "step 29420: train loss = 1.8188, val loss = 1.8886\n",
            "step 29430: train loss = 1.7292, val loss = 1.8837\n",
            "step 29440: train loss = 1.9065, val loss = 1.9205\n",
            "step 29450: train loss = 1.7783, val loss = 1.7927\n",
            "step 29460: train loss = 1.9210, val loss = 1.7847\n",
            "step 29470: train loss = 2.0142, val loss = 1.8726\n",
            "step 29480: train loss = 1.7625, val loss = 2.0080\n",
            "step 29490: train loss = 1.8004, val loss = 2.0239\n",
            "step 29500: train loss = 1.8786, val loss = 1.9154\n",
            "step 29510: train loss = 1.7503, val loss = 1.8994\n",
            "step 29520: train loss = 1.8721, val loss = 1.9987\n",
            "step 29530: train loss = 1.9811, val loss = 1.8276\n",
            "step 29540: train loss = 2.0174, val loss = 1.8218\n",
            "step 29550: train loss = 1.9049, val loss = 2.0558\n",
            "step 29560: train loss = 1.9648, val loss = 1.8910\n",
            "step 29570: train loss = 1.9148, val loss = 1.8883\n",
            "step 29580: train loss = 1.7804, val loss = 1.9009\n",
            "step 29590: train loss = 1.9440, val loss = 2.0272\n",
            "step 29600: train loss = 2.0449, val loss = 1.9757\n",
            "step 29610: train loss = 2.0028, val loss = 1.8370\n",
            "step 29620: train loss = 1.7892, val loss = 2.0007\n",
            "step 29630: train loss = 1.8435, val loss = 1.9627\n",
            "step 29640: train loss = 1.9859, val loss = 1.9725\n",
            "step 29650: train loss = 1.8422, val loss = 1.8683\n",
            "step 29660: train loss = 1.8664, val loss = 1.7522\n",
            "step 29670: train loss = 1.8538, val loss = 1.9200\n",
            "step 29680: train loss = 1.9293, val loss = 1.9313\n",
            "step 29690: train loss = 1.8946, val loss = 1.9183\n",
            "step 29700: train loss = 1.7961, val loss = 2.0718\n",
            "step 29710: train loss = 1.9571, val loss = 1.8935\n",
            "step 29720: train loss = 1.7821, val loss = 1.9018\n",
            "step 29730: train loss = 1.9605, val loss = 1.9915\n",
            "step 29740: train loss = 1.9700, val loss = 1.8584\n",
            "step 29750: train loss = 1.8579, val loss = 1.9394\n",
            "step 29760: train loss = 1.9228, val loss = 1.7303\n",
            "step 29770: train loss = 1.7900, val loss = 1.8593\n",
            "step 29780: train loss = 1.7677, val loss = 1.8369\n",
            "step 29790: train loss = 1.6876, val loss = 1.8925\n",
            "step 29800: train loss = 1.9810, val loss = 1.9337\n",
            "step 29810: train loss = 1.8947, val loss = 1.9574\n",
            "step 29820: train loss = 1.9340, val loss = 1.9511\n",
            "step 29830: train loss = 1.9037, val loss = 1.8787\n",
            "step 29840: train loss = 1.9881, val loss = 1.9079\n",
            "step 29850: train loss = 1.8497, val loss = 2.0478\n",
            "step 29860: train loss = 1.8873, val loss = 1.8962\n",
            "step 29870: train loss = 1.7305, val loss = 1.8041\n",
            "step 29880: train loss = 1.8140, val loss = 1.8225\n",
            "step 29890: train loss = 1.7584, val loss = 1.9763\n",
            "step 29900: train loss = 2.0986, val loss = 1.8404\n",
            "step 29910: train loss = 1.9028, val loss = 1.9343\n",
            "step 29920: train loss = 1.8425, val loss = 1.8952\n",
            "step 29930: train loss = 1.7946, val loss = 1.8600\n",
            "step 29940: train loss = 1.7746, val loss = 1.8602\n",
            "step 29950: train loss = 1.9517, val loss = 1.9063\n",
            "step 29960: train loss = 1.8534, val loss = 1.8128\n",
            "step 29970: train loss = 1.9268, val loss = 1.8841\n",
            "step 29980: train loss = 1.8094, val loss = 1.7626\n",
            "step 29990: train loss = 1.8510, val loss = 1.8298\n",
            "step 30000: train loss = 1.8080, val loss = 1.8439\n",
            "step 30010: train loss = 1.8021, val loss = 1.8996\n",
            "step 30020: train loss = 1.9045, val loss = 1.9410\n",
            "step 30030: train loss = 1.8599, val loss = 1.8400\n",
            "step 30040: train loss = 1.8111, val loss = 1.8494\n",
            "step 30050: train loss = 1.8208, val loss = 2.1059\n",
            "step 30060: train loss = 1.8454, val loss = 1.8744\n",
            "step 30070: train loss = 1.6980, val loss = 1.8553\n",
            "step 30080: train loss = 1.9693, val loss = 1.9060\n",
            "step 30090: train loss = 1.9211, val loss = 1.8739\n",
            "step 30100: train loss = 1.9161, val loss = 1.9744\n",
            "step 30110: train loss = 1.8019, val loss = 1.9023\n",
            "step 30120: train loss = 1.9263, val loss = 1.9614\n",
            "step 30130: train loss = 1.8836, val loss = 1.9827\n",
            "step 30140: train loss = 1.9487, val loss = 1.8285\n",
            "step 30150: train loss = 1.8863, val loss = 1.9675\n",
            "step 30160: train loss = 2.0688, val loss = 1.9268\n",
            "step 30170: train loss = 1.9307, val loss = 1.8577\n",
            "step 30180: train loss = 1.8515, val loss = 1.8404\n",
            "step 30190: train loss = 1.9419, val loss = 1.9215\n",
            "step 30200: train loss = 1.9470, val loss = 1.9195\n",
            "step 30210: train loss = 1.8855, val loss = 1.7965\n",
            "step 30220: train loss = 1.8391, val loss = 2.0628\n",
            "step 30230: train loss = 1.9077, val loss = 1.9233\n",
            "step 30240: train loss = 1.8309, val loss = 1.7781\n",
            "step 30250: train loss = 1.8917, val loss = 1.7450\n",
            "step 30260: train loss = 1.8899, val loss = 1.8918\n",
            "step 30270: train loss = 1.8604, val loss = 1.9337\n",
            "step 30280: train loss = 1.9517, val loss = 1.9132\n",
            "step 30290: train loss = 1.9301, val loss = 1.8901\n",
            "step 30300: train loss = 1.8082, val loss = 1.9036\n",
            "step 30310: train loss = 1.8488, val loss = 1.9209\n",
            "step 30320: train loss = 1.8554, val loss = 2.0114\n",
            "step 30330: train loss = 1.9343, val loss = 1.8426\n",
            "step 30340: train loss = 1.9956, val loss = 1.9375\n",
            "step 30350: train loss = 1.7860, val loss = 1.8321\n",
            "step 30360: train loss = 2.0080, val loss = 1.9125\n",
            "step 30370: train loss = 1.9438, val loss = 1.9749\n",
            "step 30380: train loss = 1.9275, val loss = 1.9298\n",
            "step 30390: train loss = 1.8045, val loss = 1.9505\n",
            "step 30400: train loss = 2.0569, val loss = 1.9088\n",
            "step 30410: train loss = 1.8137, val loss = 1.7915\n",
            "step 30420: train loss = 1.9245, val loss = 1.9109\n",
            "step 30430: train loss = 2.0255, val loss = 1.8891\n",
            "step 30440: train loss = 1.8722, val loss = 1.8884\n",
            "step 30450: train loss = 1.8999, val loss = 1.7330\n",
            "step 30460: train loss = 1.8519, val loss = 1.8334\n",
            "step 30470: train loss = 1.8566, val loss = 1.8631\n",
            "step 30480: train loss = 1.9163, val loss = 1.8240\n",
            "step 30490: train loss = 1.9281, val loss = 1.9095\n",
            "step 30500: train loss = 1.8425, val loss = 1.9189\n",
            "step 30510: train loss = 1.8992, val loss = 1.8641\n",
            "step 30520: train loss = 1.9372, val loss = 1.9648\n",
            "step 30530: train loss = 1.9013, val loss = 1.9820\n",
            "step 30540: train loss = 1.7380, val loss = 1.8159\n",
            "step 30550: train loss = 1.8658, val loss = 2.1266\n",
            "step 30560: train loss = 1.9335, val loss = 2.0025\n",
            "step 30570: train loss = 1.8171, val loss = 1.8359\n",
            "step 30580: train loss = 1.8808, val loss = 1.8770\n",
            "step 30590: train loss = 1.9218, val loss = 1.8580\n",
            "step 30600: train loss = 1.9559, val loss = 1.9867\n",
            "step 30610: train loss = 1.8688, val loss = 1.8645\n",
            "step 30620: train loss = 1.9076, val loss = 1.8945\n",
            "step 30630: train loss = 1.8076, val loss = 1.9495\n",
            "step 30640: train loss = 1.9748, val loss = 1.9771\n",
            "step 30650: train loss = 1.7282, val loss = 1.9240\n",
            "step 30660: train loss = 2.0339, val loss = 1.8773\n",
            "step 30670: train loss = 1.9659, val loss = 1.9729\n",
            "step 30680: train loss = 1.8669, val loss = 1.9065\n",
            "step 30690: train loss = 2.0998, val loss = 1.8445\n",
            "step 30700: train loss = 1.9020, val loss = 1.9445\n",
            "step 30710: train loss = 1.7443, val loss = 1.9079\n",
            "step 30720: train loss = 1.8325, val loss = 1.9379\n",
            "step 30730: train loss = 1.9265, val loss = 1.8389\n",
            "step 30740: train loss = 1.8083, val loss = 1.7877\n",
            "step 30750: train loss = 1.9456, val loss = 1.8568\n",
            "step 30760: train loss = 1.9897, val loss = 1.8775\n",
            "step 30770: train loss = 1.9404, val loss = 1.9165\n",
            "step 30780: train loss = 2.0278, val loss = 2.0621\n",
            "step 30790: train loss = 1.8642, val loss = 1.9179\n",
            "step 30800: train loss = 1.9915, val loss = 1.7540\n",
            "step 30810: train loss = 2.0388, val loss = 1.7274\n",
            "step 30820: train loss = 1.7077, val loss = 1.9761\n",
            "step 30830: train loss = 1.9128, val loss = 1.9074\n",
            "step 30840: train loss = 1.8992, val loss = 1.9184\n",
            "step 30850: train loss = 1.9713, val loss = 1.9689\n",
            "step 30860: train loss = 1.8031, val loss = 1.8412\n",
            "step 30870: train loss = 1.8908, val loss = 2.0211\n",
            "step 30880: train loss = 2.0223, val loss = 1.9161\n",
            "step 30890: train loss = 1.8936, val loss = 1.8926\n",
            "step 30900: train loss = 1.7971, val loss = 1.8977\n",
            "step 30910: train loss = 1.7720, val loss = 2.0898\n",
            "step 30920: train loss = 1.9131, val loss = 1.8537\n",
            "step 30930: train loss = 1.8257, val loss = 1.8818\n",
            "step 30940: train loss = 1.8641, val loss = 1.8805\n",
            "step 30950: train loss = 1.8671, val loss = 2.0286\n",
            "step 30960: train loss = 1.8024, val loss = 1.8283\n",
            "step 30970: train loss = 1.9437, val loss = 1.7570\n",
            "step 30980: train loss = 1.9251, val loss = 1.8270\n",
            "step 30990: train loss = 1.8909, val loss = 1.8074\n",
            "step 31000: train loss = 2.0111, val loss = 1.9741\n",
            "step 31010: train loss = 1.9410, val loss = 1.9617\n",
            "step 31020: train loss = 1.9727, val loss = 1.8869\n",
            "step 31030: train loss = 1.9404, val loss = 1.8382\n",
            "step 31040: train loss = 1.8145, val loss = 1.9015\n",
            "step 31050: train loss = 1.9687, val loss = 1.9244\n",
            "step 31060: train loss = 1.9239, val loss = 1.9093\n",
            "step 31070: train loss = 1.7322, val loss = 1.7862\n",
            "step 31080: train loss = 1.8900, val loss = 1.9376\n",
            "step 31090: train loss = 1.9425, val loss = 1.9673\n",
            "step 31100: train loss = 1.9165, val loss = 1.9051\n",
            "step 31110: train loss = 2.0309, val loss = 1.8870\n",
            "step 31120: train loss = 1.9890, val loss = 1.9363\n",
            "step 31130: train loss = 1.9064, val loss = 1.8943\n",
            "step 31140: train loss = 1.9316, val loss = 1.8934\n",
            "step 31150: train loss = 1.8170, val loss = 1.8262\n",
            "step 31160: train loss = 1.9016, val loss = 1.9326\n",
            "step 31170: train loss = 1.9037, val loss = 1.8333\n",
            "step 31180: train loss = 1.8946, val loss = 1.9450\n",
            "step 31190: train loss = 1.8660, val loss = 1.8409\n",
            "step 31200: train loss = 2.0238, val loss = 1.8760\n",
            "step 31210: train loss = 1.9184, val loss = 1.7385\n",
            "step 31220: train loss = 1.8402, val loss = 1.8336\n",
            "step 31230: train loss = 1.9039, val loss = 1.9251\n",
            "step 31240: train loss = 1.8765, val loss = 1.9331\n",
            "step 31250: train loss = 1.8956, val loss = 1.8074\n",
            "step 31260: train loss = 1.8216, val loss = 1.8638\n",
            "step 31270: train loss = 1.9282, val loss = 1.8890\n",
            "step 31280: train loss = 1.8418, val loss = 2.0474\n",
            "step 31290: train loss = 1.9486, val loss = 1.9281\n",
            "step 31300: train loss = 1.8475, val loss = 1.9039\n",
            "step 31310: train loss = 1.7521, val loss = 1.9199\n",
            "step 31320: train loss = 2.0131, val loss = 1.9393\n",
            "step 31330: train loss = 1.9873, val loss = 1.7809\n",
            "step 31340: train loss = 1.9161, val loss = 1.7440\n",
            "step 31350: train loss = 1.8162, val loss = 1.8982\n",
            "step 31360: train loss = 1.8685, val loss = 1.9262\n",
            "step 31370: train loss = 1.9044, val loss = 1.8141\n",
            "step 31380: train loss = 1.8322, val loss = 1.9114\n",
            "step 31390: train loss = 1.7772, val loss = 2.0413\n",
            "step 31400: train loss = 1.8445, val loss = 1.9084\n",
            "step 31410: train loss = 1.7406, val loss = 1.7966\n",
            "step 31420: train loss = 1.8731, val loss = 1.9164\n",
            "step 31430: train loss = 2.0585, val loss = 1.8465\n",
            "step 31440: train loss = 1.8298, val loss = 1.8196\n",
            "step 31450: train loss = 1.9443, val loss = 1.8627\n",
            "step 31460: train loss = 1.8406, val loss = 1.7668\n",
            "step 31470: train loss = 1.8309, val loss = 1.8626\n",
            "step 31480: train loss = 1.8241, val loss = 1.8107\n",
            "step 31490: train loss = 1.8687, val loss = 1.8831\n",
            "step 31500: train loss = 1.8885, val loss = 1.9227\n",
            "step 31510: train loss = 1.8215, val loss = 1.8839\n",
            "step 31520: train loss = 1.9154, val loss = 1.9140\n",
            "step 31530: train loss = 1.9185, val loss = 1.9573\n",
            "step 31540: train loss = 1.9788, val loss = 1.8051\n",
            "step 31550: train loss = 1.8927, val loss = 1.9284\n",
            "step 31560: train loss = 1.8104, val loss = 1.9279\n",
            "step 31570: train loss = 1.7549, val loss = 1.8903\n",
            "step 31580: train loss = 1.7911, val loss = 1.7965\n",
            "step 31590: train loss = 1.8497, val loss = 1.8084\n",
            "step 31600: train loss = 1.8228, val loss = 1.9172\n",
            "step 31610: train loss = 1.8150, val loss = 1.8725\n",
            "step 31620: train loss = 1.8025, val loss = 1.9510\n",
            "step 31630: train loss = 1.8415, val loss = 1.8716\n",
            "step 31640: train loss = 1.9177, val loss = 1.7210\n",
            "step 31650: train loss = 1.9056, val loss = 1.6892\n",
            "step 31660: train loss = 1.9879, val loss = 1.9789\n",
            "step 31670: train loss = 1.9021, val loss = 1.9722\n",
            "step 31680: train loss = 1.9851, val loss = 1.8159\n",
            "step 31690: train loss = 1.8115, val loss = 1.7428\n",
            "step 31700: train loss = 1.9146, val loss = 1.8231\n",
            "step 31710: train loss = 1.9583, val loss = 1.9053\n",
            "step 31720: train loss = 1.7627, val loss = 1.8243\n",
            "step 31730: train loss = 1.8763, val loss = 1.9337\n",
            "step 31740: train loss = 1.9028, val loss = 1.9175\n",
            "step 31750: train loss = 1.7732, val loss = 1.9129\n",
            "step 31760: train loss = 1.8106, val loss = 1.9357\n",
            "step 31770: train loss = 2.0506, val loss = 1.8285\n",
            "step 31780: train loss = 2.0111, val loss = 1.9364\n",
            "step 31790: train loss = 2.0313, val loss = 1.9175\n",
            "step 31800: train loss = 1.9096, val loss = 1.8689\n",
            "step 31810: train loss = 1.8672, val loss = 1.9130\n",
            "step 31820: train loss = 1.7858, val loss = 1.8000\n",
            "step 31830: train loss = 1.8702, val loss = 1.8264\n",
            "step 31840: train loss = 1.8362, val loss = 1.8067\n",
            "step 31850: train loss = 1.8615, val loss = 1.8120\n",
            "step 31860: train loss = 1.9507, val loss = 1.8609\n",
            "step 31870: train loss = 1.8186, val loss = 1.8712\n",
            "step 31880: train loss = 1.8989, val loss = 1.8449\n",
            "step 31890: train loss = 1.9427, val loss = 1.8415\n",
            "step 31900: train loss = 1.8534, val loss = 1.8366\n",
            "step 31910: train loss = 1.8455, val loss = 1.9958\n",
            "step 31920: train loss = 2.0454, val loss = 1.7942\n",
            "step 31930: train loss = 1.8509, val loss = 1.9337\n",
            "step 31940: train loss = 1.9307, val loss = 1.9049\n",
            "step 31950: train loss = 1.8835, val loss = 1.8328\n",
            "step 31960: train loss = 1.9593, val loss = 1.9017\n",
            "step 31970: train loss = 1.8584, val loss = 1.7946\n",
            "step 31980: train loss = 1.8399, val loss = 1.9729\n",
            "step 31990: train loss = 1.8548, val loss = 1.8117\n",
            "step 32000: train loss = 1.9826, val loss = 1.9123\n",
            "step 32010: train loss = 1.7932, val loss = 1.7951\n",
            "step 32020: train loss = 1.8147, val loss = 1.9252\n",
            "step 32030: train loss = 1.8308, val loss = 1.7702\n",
            "step 32040: train loss = 1.7817, val loss = 1.9978\n",
            "step 32050: train loss = 1.9716, val loss = 1.8556\n",
            "step 32060: train loss = 2.0614, val loss = 1.9320\n",
            "step 32070: train loss = 1.8773, val loss = 1.8882\n",
            "step 32080: train loss = 1.8678, val loss = 1.8663\n",
            "step 32090: train loss = 1.9248, val loss = 1.8414\n",
            "step 32100: train loss = 2.0231, val loss = 1.8500\n",
            "step 32110: train loss = 1.7945, val loss = 1.9355\n",
            "step 32120: train loss = 1.8907, val loss = 1.8972\n",
            "step 32130: train loss = 1.9510, val loss = 1.8111\n",
            "step 32140: train loss = 1.9469, val loss = 1.9495\n",
            "step 32150: train loss = 1.8343, val loss = 1.7146\n",
            "step 32160: train loss = 1.8966, val loss = 1.9666\n",
            "step 32170: train loss = 1.9720, val loss = 1.7811\n",
            "step 32180: train loss = 1.9678, val loss = 1.8682\n",
            "step 32190: train loss = 1.8408, val loss = 1.8661\n",
            "step 32200: train loss = 1.8654, val loss = 1.9224\n",
            "step 32210: train loss = 1.8253, val loss = 1.9548\n",
            "step 32220: train loss = 1.7902, val loss = 1.8401\n",
            "step 32230: train loss = 1.8441, val loss = 1.7415\n",
            "step 32240: train loss = 1.9408, val loss = 2.0128\n",
            "step 32250: train loss = 1.7909, val loss = 1.9702\n",
            "step 32260: train loss = 1.8072, val loss = 1.7801\n",
            "step 32270: train loss = 1.8581, val loss = 1.8164\n",
            "step 32280: train loss = 1.7996, val loss = 2.0386\n",
            "step 32290: train loss = 1.7150, val loss = 1.8441\n",
            "step 32300: train loss = 1.8965, val loss = 1.7736\n",
            "step 32310: train loss = 1.8602, val loss = 1.8689\n",
            "step 32320: train loss = 1.7721, val loss = 1.9115\n",
            "step 32330: train loss = 1.7232, val loss = 1.8839\n",
            "step 32340: train loss = 1.8420, val loss = 1.8405\n",
            "step 32350: train loss = 1.9313, val loss = 2.0212\n",
            "step 32360: train loss = 1.9098, val loss = 2.1037\n",
            "step 32370: train loss = 1.8680, val loss = 1.8678\n",
            "step 32380: train loss = 1.9192, val loss = 1.8732\n",
            "step 32390: train loss = 1.8484, val loss = 1.9405\n",
            "step 32400: train loss = 1.9229, val loss = 1.9268\n",
            "step 32410: train loss = 1.7388, val loss = 1.7767\n",
            "step 32420: train loss = 1.7610, val loss = 1.9248\n",
            "step 32430: train loss = 1.8842, val loss = 1.8562\n",
            "step 32440: train loss = 1.9988, val loss = 1.8883\n",
            "step 32450: train loss = 1.8741, val loss = 1.8741\n",
            "step 32460: train loss = 1.9157, val loss = 1.9703\n",
            "step 32470: train loss = 1.8955, val loss = 1.9539\n",
            "step 32480: train loss = 2.0867, val loss = 1.8359\n",
            "step 32490: train loss = 1.9137, val loss = 1.8097\n",
            "step 32500: train loss = 1.9307, val loss = 1.9158\n",
            "step 32510: train loss = 1.8573, val loss = 1.8425\n",
            "step 32520: train loss = 1.8789, val loss = 2.0260\n",
            "step 32530: train loss = 1.8317, val loss = 1.8694\n",
            "step 32540: train loss = 2.0277, val loss = 1.9456\n",
            "step 32550: train loss = 1.8693, val loss = 1.7667\n",
            "step 32560: train loss = 1.7517, val loss = 1.8549\n",
            "step 32570: train loss = 1.7747, val loss = 1.8156\n",
            "step 32580: train loss = 1.8592, val loss = 1.8083\n",
            "step 32590: train loss = 1.8854, val loss = 1.9108\n",
            "step 32600: train loss = 1.9381, val loss = 1.9572\n",
            "step 32610: train loss = 1.7927, val loss = 1.9900\n",
            "step 32620: train loss = 1.8850, val loss = 2.0766\n",
            "step 32630: train loss = 1.8845, val loss = 1.7877\n",
            "step 32640: train loss = 1.8734, val loss = 1.8959\n",
            "step 32650: train loss = 1.7620, val loss = 1.9029\n",
            "step 32660: train loss = 1.8957, val loss = 1.9097\n",
            "step 32670: train loss = 1.9384, val loss = 1.7219\n",
            "step 32680: train loss = 1.8938, val loss = 1.6808\n",
            "step 32690: train loss = 1.8529, val loss = 1.9491\n",
            "step 32700: train loss = 1.8676, val loss = 1.8842\n",
            "step 32710: train loss = 1.6893, val loss = 1.9990\n",
            "step 32720: train loss = 1.9361, val loss = 1.9090\n",
            "step 32730: train loss = 1.9522, val loss = 1.8394\n",
            "step 32740: train loss = 1.8496, val loss = 1.8838\n",
            "step 32750: train loss = 1.8013, val loss = 1.9574\n",
            "step 32760: train loss = 1.9069, val loss = 1.8464\n",
            "step 32770: train loss = 1.9678, val loss = 1.8798\n",
            "step 32780: train loss = 1.8298, val loss = 1.7921\n",
            "step 32790: train loss = 1.9210, val loss = 1.9029\n",
            "step 32800: train loss = 1.8188, val loss = 1.9626\n",
            "step 32810: train loss = 1.8507, val loss = 1.9089\n",
            "step 32820: train loss = 1.9514, val loss = 1.7669\n",
            "step 32830: train loss = 1.8399, val loss = 1.8906\n",
            "step 32840: train loss = 1.8767, val loss = 1.8181\n",
            "step 32850: train loss = 1.8641, val loss = 2.0030\n",
            "step 32860: train loss = 1.8429, val loss = 1.7890\n",
            "step 32870: train loss = 1.9494, val loss = 1.8548\n",
            "step 32880: train loss = 1.7700, val loss = 1.8969\n",
            "step 32890: train loss = 1.9000, val loss = 1.8414\n",
            "step 32900: train loss = 1.8595, val loss = 1.8912\n",
            "step 32910: train loss = 1.8690, val loss = 1.9968\n",
            "step 32920: train loss = 1.8670, val loss = 1.8548\n",
            "step 32930: train loss = 1.8288, val loss = 1.7789\n",
            "step 32940: train loss = 1.8546, val loss = 1.7233\n",
            "step 32950: train loss = 1.8955, val loss = 1.9615\n",
            "step 32960: train loss = 1.9361, val loss = 1.9007\n",
            "step 32970: train loss = 1.8488, val loss = 1.8309\n",
            "step 32980: train loss = 1.9067, val loss = 1.9386\n",
            "step 32990: train loss = 1.6462, val loss = 1.8735\n",
            "step 33000: train loss = 1.8644, val loss = 1.8242\n",
            "step 33010: train loss = 1.7400, val loss = 2.0099\n",
            "step 33020: train loss = 1.7683, val loss = 1.8829\n",
            "step 33030: train loss = 2.0153, val loss = 1.9206\n",
            "step 33040: train loss = 1.8536, val loss = 1.7198\n",
            "step 33050: train loss = 1.8687, val loss = 1.8377\n",
            "step 33060: train loss = 1.9124, val loss = 1.9416\n",
            "step 33070: train loss = 1.8435, val loss = 1.6974\n",
            "step 33080: train loss = 1.8906, val loss = 1.7856\n",
            "step 33090: train loss = 1.8043, val loss = 1.7648\n",
            "step 33100: train loss = 1.9423, val loss = 1.7991\n",
            "step 33110: train loss = 2.0547, val loss = 1.8477\n",
            "step 33120: train loss = 1.8948, val loss = 1.7771\n",
            "step 33130: train loss = 1.8355, val loss = 1.7671\n",
            "step 33140: train loss = 1.9834, val loss = 1.8274\n",
            "step 33150: train loss = 1.8911, val loss = 1.8109\n",
            "step 33160: train loss = 1.8877, val loss = 1.8203\n",
            "step 33170: train loss = 1.8223, val loss = 1.9014\n",
            "step 33180: train loss = 2.0992, val loss = 1.7902\n",
            "step 33190: train loss = 1.8260, val loss = 1.7071\n",
            "step 33200: train loss = 1.7537, val loss = 1.7689\n",
            "step 33210: train loss = 1.7957, val loss = 1.7253\n",
            "step 33220: train loss = 1.8009, val loss = 1.9318\n",
            "step 33230: train loss = 1.9259, val loss = 1.9679\n",
            "step 33240: train loss = 1.8519, val loss = 1.8445\n",
            "step 33250: train loss = 1.8325, val loss = 1.7972\n",
            "step 33260: train loss = 1.8543, val loss = 1.8390\n",
            "step 33270: train loss = 1.9477, val loss = 1.8025\n",
            "step 33280: train loss = 1.8914, val loss = 1.9018\n",
            "step 33290: train loss = 1.8478, val loss = 1.9171\n",
            "step 33300: train loss = 2.0515, val loss = 1.9274\n",
            "step 33310: train loss = 1.7563, val loss = 1.8454\n",
            "step 33320: train loss = 1.8470, val loss = 1.8794\n",
            "step 33330: train loss = 1.8559, val loss = 1.8677\n",
            "step 33340: train loss = 1.8683, val loss = 1.8993\n",
            "step 33350: train loss = 1.8306, val loss = 1.8204\n",
            "step 33360: train loss = 1.8668, val loss = 1.7671\n",
            "step 33370: train loss = 1.7736, val loss = 1.9564\n",
            "step 33380: train loss = 1.8777, val loss = 1.7914\n",
            "step 33390: train loss = 1.8094, val loss = 1.9166\n",
            "step 33400: train loss = 1.8147, val loss = 1.8896\n",
            "step 33410: train loss = 1.7118, val loss = 1.7980\n",
            "step 33420: train loss = 1.8441, val loss = 2.0461\n",
            "step 33430: train loss = 1.8339, val loss = 1.9225\n",
            "step 33440: train loss = 1.9658, val loss = 1.8524\n",
            "step 33450: train loss = 1.9076, val loss = 1.9699\n",
            "step 33460: train loss = 1.7622, val loss = 1.8538\n",
            "step 33470: train loss = 1.8174, val loss = 1.8237\n",
            "step 33480: train loss = 1.9930, val loss = 1.7632\n",
            "step 33490: train loss = 1.8871, val loss = 1.9668\n",
            "step 33500: train loss = 1.8429, val loss = 1.9584\n",
            "step 33510: train loss = 1.9244, val loss = 1.8194\n",
            "step 33520: train loss = 1.9301, val loss = 1.9533\n",
            "step 33530: train loss = 1.8811, val loss = 1.9547\n",
            "step 33540: train loss = 1.8214, val loss = 1.8852\n",
            "step 33550: train loss = 1.9126, val loss = 1.8677\n",
            "step 33560: train loss = 1.7506, val loss = 1.9631\n",
            "step 33570: train loss = 1.8547, val loss = 1.8780\n",
            "step 33580: train loss = 1.8976, val loss = 1.8696\n",
            "step 33590: train loss = 1.8142, val loss = 1.7511\n",
            "step 33600: train loss = 1.8766, val loss = 1.8081\n",
            "step 33610: train loss = 1.8252, val loss = 1.9941\n",
            "step 33620: train loss = 1.8461, val loss = 1.8922\n",
            "step 33630: train loss = 1.7906, val loss = 1.8102\n",
            "step 33640: train loss = 1.9886, val loss = 1.9785\n",
            "step 33650: train loss = 1.9178, val loss = 1.9175\n",
            "step 33660: train loss = 1.8971, val loss = 1.9390\n",
            "step 33670: train loss = 1.6958, val loss = 1.8927\n",
            "step 33680: train loss = 1.9522, val loss = 1.8896\n",
            "step 33690: train loss = 1.9921, val loss = 1.8401\n",
            "step 33700: train loss = 1.7863, val loss = 1.8886\n",
            "step 33710: train loss = 1.9336, val loss = 1.7189\n",
            "step 33720: train loss = 2.0092, val loss = 1.9291\n",
            "step 33730: train loss = 1.7533, val loss = 1.9139\n",
            "step 33740: train loss = 1.8327, val loss = 1.7398\n",
            "step 33750: train loss = 1.9233, val loss = 2.0906\n",
            "step 33760: train loss = 1.8784, val loss = 2.0184\n",
            "step 33770: train loss = 1.9000, val loss = 1.9084\n",
            "step 33780: train loss = 1.8599, val loss = 1.8881\n",
            "step 33790: train loss = 1.9883, val loss = 1.8948\n",
            "step 33800: train loss = 1.8596, val loss = 1.9502\n",
            "step 33810: train loss = 1.8751, val loss = 1.7471\n",
            "step 33820: train loss = 2.0368, val loss = 2.0160\n",
            "step 33830: train loss = 1.8262, val loss = 1.9315\n",
            "step 33840: train loss = 1.9296, val loss = 1.9757\n",
            "step 33850: train loss = 1.9291, val loss = 1.9685\n",
            "step 33860: train loss = 1.9467, val loss = 1.9071\n",
            "step 33870: train loss = 1.9889, val loss = 1.9298\n",
            "step 33880: train loss = 1.8242, val loss = 1.9165\n",
            "step 33890: train loss = 1.9373, val loss = 1.6625\n",
            "step 33900: train loss = 1.8186, val loss = 1.9407\n",
            "step 33910: train loss = 2.0190, val loss = 1.8163\n",
            "step 33920: train loss = 1.8297, val loss = 1.8725\n",
            "step 33930: train loss = 1.7448, val loss = 1.9557\n",
            "step 33940: train loss = 1.9450, val loss = 1.8130\n",
            "step 33950: train loss = 1.8981, val loss = 1.9275\n",
            "step 33960: train loss = 1.8341, val loss = 1.7552\n",
            "step 33970: train loss = 1.7178, val loss = 1.7380\n",
            "step 33980: train loss = 1.8931, val loss = 1.9162\n",
            "step 33990: train loss = 1.8984, val loss = 1.7739\n",
            "step 34000: train loss = 1.7081, val loss = 1.9052\n",
            "step 34010: train loss = 1.8343, val loss = 1.9588\n",
            "step 34020: train loss = 1.8428, val loss = 1.7828\n",
            "step 34030: train loss = 2.0387, val loss = 1.7603\n",
            "step 34040: train loss = 1.8691, val loss = 1.9279\n",
            "step 34050: train loss = 1.8620, val loss = 1.8770\n",
            "step 34060: train loss = 1.8332, val loss = 1.9176\n",
            "step 34070: train loss = 1.7604, val loss = 2.0297\n",
            "step 34080: train loss = 1.7347, val loss = 1.8140\n",
            "step 34090: train loss = 1.9888, val loss = 1.8588\n",
            "step 34100: train loss = 1.9736, val loss = 1.9857\n",
            "step 34110: train loss = 1.8494, val loss = 1.7306\n",
            "step 34120: train loss = 1.8466, val loss = 1.8886\n",
            "step 34130: train loss = 1.9936, val loss = 1.8342\n",
            "step 34140: train loss = 1.7847, val loss = 1.8657\n",
            "step 34150: train loss = 1.8314, val loss = 1.9201\n",
            "step 34160: train loss = 1.8792, val loss = 1.9067\n",
            "step 34170: train loss = 1.8850, val loss = 1.9345\n",
            "step 34180: train loss = 1.8241, val loss = 1.9690\n",
            "step 34190: train loss = 1.8608, val loss = 1.9692\n",
            "step 34200: train loss = 1.7478, val loss = 1.8753\n",
            "step 34210: train loss = 1.7855, val loss = 1.8587\n",
            "step 34220: train loss = 1.8181, val loss = 1.9354\n",
            "step 34230: train loss = 1.8397, val loss = 1.9063\n",
            "step 34240: train loss = 1.8683, val loss = 1.9880\n",
            "step 34250: train loss = 1.7385, val loss = 1.7126\n",
            "step 34260: train loss = 1.8471, val loss = 2.0253\n",
            "step 34270: train loss = 1.8593, val loss = 1.8006\n",
            "step 34280: train loss = 1.6870, val loss = 1.8196\n",
            "step 34290: train loss = 1.8986, val loss = 1.9414\n",
            "step 34300: train loss = 2.0440, val loss = 1.9527\n",
            "step 34310: train loss = 1.8301, val loss = 1.8352\n",
            "step 34320: train loss = 1.9102, val loss = 1.7707\n",
            "step 34330: train loss = 1.9347, val loss = 1.7912\n",
            "step 34340: train loss = 1.8318, val loss = 1.8034\n",
            "step 34350: train loss = 1.9349, val loss = 1.8514\n",
            "step 34360: train loss = 1.7606, val loss = 1.8586\n",
            "step 34370: train loss = 1.8622, val loss = 2.0484\n",
            "step 34380: train loss = 1.9355, val loss = 1.9815\n",
            "step 34390: train loss = 1.8269, val loss = 1.8503\n",
            "step 34400: train loss = 1.9191, val loss = 1.8567\n",
            "step 34410: train loss = 1.8345, val loss = 1.8456\n",
            "step 34420: train loss = 1.9328, val loss = 1.8504\n",
            "step 34430: train loss = 1.9058, val loss = 1.9454\n",
            "step 34440: train loss = 1.9797, val loss = 1.9679\n",
            "step 34450: train loss = 1.9513, val loss = 1.9430\n",
            "step 34460: train loss = 1.8393, val loss = 1.8796\n",
            "step 34470: train loss = 1.8384, val loss = 1.9569\n",
            "step 34480: train loss = 1.8452, val loss = 1.8949\n",
            "step 34490: train loss = 1.9167, val loss = 1.8770\n",
            "step 34500: train loss = 1.7471, val loss = 1.9693\n",
            "step 34510: train loss = 1.9538, val loss = 1.8601\n",
            "step 34520: train loss = 1.9381, val loss = 2.0124\n",
            "step 34530: train loss = 1.9209, val loss = 2.0087\n",
            "step 34540: train loss = 1.9194, val loss = 1.8932\n",
            "step 34550: train loss = 1.9134, val loss = 2.0865\n",
            "step 34560: train loss = 1.9586, val loss = 1.9686\n",
            "step 34570: train loss = 1.8723, val loss = 1.9886\n",
            "step 34580: train loss = 1.7834, val loss = 1.7796\n",
            "step 34590: train loss = 1.8915, val loss = 1.8911\n",
            "step 34600: train loss = 1.9918, val loss = 1.9253\n",
            "step 34610: train loss = 2.1302, val loss = 2.0531\n",
            "step 34620: train loss = 1.8022, val loss = 1.8694\n",
            "step 34630: train loss = 1.9157, val loss = 1.9637\n",
            "step 34640: train loss = 1.9263, val loss = 1.9197\n",
            "step 34650: train loss = 1.9618, val loss = 1.8983\n",
            "step 34660: train loss = 1.8861, val loss = 1.8849\n",
            "step 34670: train loss = 1.8841, val loss = 1.8350\n",
            "step 34680: train loss = 1.7574, val loss = 1.7428\n",
            "step 34690: train loss = 1.9599, val loss = 1.9447\n",
            "step 34700: train loss = 1.8370, val loss = 1.9253\n",
            "step 34710: train loss = 1.9484, val loss = 1.9140\n",
            "step 34720: train loss = 1.7834, val loss = 1.7752\n",
            "step 34730: train loss = 2.0108, val loss = 2.0040\n",
            "step 34740: train loss = 1.8459, val loss = 1.8664\n",
            "step 34750: train loss = 1.9292, val loss = 1.9709\n",
            "step 34760: train loss = 1.8710, val loss = 1.8625\n",
            "step 34770: train loss = 1.8417, val loss = 1.8975\n",
            "step 34780: train loss = 1.8874, val loss = 1.9705\n",
            "step 34790: train loss = 1.8266, val loss = 1.8540\n",
            "step 34800: train loss = 1.7283, val loss = 2.0136\n",
            "step 34810: train loss = 1.9599, val loss = 2.0215\n",
            "step 34820: train loss = 1.9323, val loss = 1.9519\n",
            "step 34830: train loss = 1.9574, val loss = 1.9522\n",
            "step 34840: train loss = 1.9899, val loss = 1.9437\n",
            "step 34850: train loss = 1.8059, val loss = 1.7943\n",
            "step 34860: train loss = 1.8890, val loss = 1.9104\n",
            "step 34870: train loss = 1.8729, val loss = 1.9146\n",
            "step 34880: train loss = 1.9543, val loss = 1.9612\n",
            "step 34890: train loss = 1.8395, val loss = 1.9639\n",
            "step 34900: train loss = 1.9001, val loss = 1.9131\n",
            "step 34910: train loss = 1.8185, val loss = 1.8873\n",
            "step 34920: train loss = 1.7647, val loss = 1.9027\n",
            "step 34930: train loss = 1.7798, val loss = 1.8260\n",
            "step 34940: train loss = 1.8004, val loss = 1.9247\n",
            "step 34950: train loss = 1.8844, val loss = 1.8683\n",
            "step 34960: train loss = 1.8466, val loss = 2.0026\n",
            "step 34970: train loss = 1.9049, val loss = 1.9055\n",
            "step 34980: train loss = 1.9360, val loss = 2.0243\n",
            "step 34990: train loss = 1.9895, val loss = 1.8276\n",
            "step 35000: train loss = 1.8502, val loss = 1.8796\n",
            "step 35010: train loss = 1.8556, val loss = 1.7935\n",
            "step 35020: train loss = 1.7417, val loss = 1.9694\n",
            "step 35030: train loss = 1.7854, val loss = 1.8904\n",
            "step 35040: train loss = 1.9104, val loss = 1.9586\n",
            "step 35050: train loss = 1.9097, val loss = 1.7707\n",
            "step 35060: train loss = 1.9320, val loss = 1.9467\n",
            "step 35070: train loss = 1.8588, val loss = 1.9273\n",
            "step 35080: train loss = 1.8743, val loss = 1.7216\n",
            "step 35090: train loss = 1.9983, val loss = 1.7306\n",
            "step 35100: train loss = 1.8288, val loss = 2.0651\n",
            "step 35110: train loss = 1.9673, val loss = 1.8009\n",
            "step 35120: train loss = 1.9945, val loss = 2.0281\n",
            "step 35130: train loss = 1.9173, val loss = 1.9247\n",
            "step 35140: train loss = 1.7647, val loss = 2.0075\n",
            "step 35150: train loss = 1.7859, val loss = 1.9178\n",
            "step 35160: train loss = 1.8165, val loss = 1.9717\n",
            "step 35170: train loss = 1.8516, val loss = 1.7964\n",
            "step 35180: train loss = 1.7314, val loss = 1.8958\n",
            "step 35190: train loss = 1.8256, val loss = 1.8272\n",
            "step 35200: train loss = 1.8082, val loss = 1.9529\n",
            "step 35210: train loss = 1.7220, val loss = 1.9322\n",
            "step 35220: train loss = 1.8721, val loss = 1.8156\n",
            "step 35230: train loss = 1.8993, val loss = 2.0310\n",
            "step 35240: train loss = 1.9077, val loss = 1.8395\n",
            "step 35250: train loss = 1.9751, val loss = 1.9007\n",
            "step 35260: train loss = 1.7383, val loss = 2.0353\n",
            "step 35270: train loss = 1.7593, val loss = 1.9450\n",
            "step 35280: train loss = 2.0315, val loss = 1.8571\n",
            "step 35290: train loss = 1.7937, val loss = 1.7972\n",
            "step 35300: train loss = 1.9346, val loss = 2.0320\n",
            "step 35310: train loss = 1.9036, val loss = 2.0061\n",
            "step 35320: train loss = 1.8369, val loss = 2.0213\n",
            "step 35330: train loss = 2.0543, val loss = 2.1368\n",
            "step 35340: train loss = 2.0482, val loss = 1.8516\n",
            "step 35350: train loss = 1.8167, val loss = 1.8246\n",
            "step 35360: train loss = 2.0385, val loss = 1.8319\n",
            "step 35370: train loss = 1.8840, val loss = 1.9309\n",
            "step 35380: train loss = 1.8553, val loss = 1.9513\n",
            "step 35390: train loss = 1.7710, val loss = 1.8829\n",
            "step 35400: train loss = 1.9663, val loss = 1.7685\n",
            "step 35410: train loss = 1.9258, val loss = 1.9312\n",
            "step 35420: train loss = 1.9383, val loss = 1.9263\n",
            "step 35430: train loss = 1.9749, val loss = 1.8335\n",
            "step 35440: train loss = 1.6382, val loss = 2.0204\n",
            "step 35450: train loss = 1.7992, val loss = 1.9443\n",
            "step 35460: train loss = 1.7648, val loss = 1.8938\n",
            "step 35470: train loss = 2.0001, val loss = 2.0510\n",
            "step 35480: train loss = 1.9232, val loss = 1.8444\n",
            "step 35490: train loss = 1.9093, val loss = 1.8299\n",
            "step 35500: train loss = 1.8751, val loss = 1.9193\n",
            "step 35510: train loss = 1.8511, val loss = 2.0760\n",
            "step 35520: train loss = 1.9003, val loss = 1.9756\n",
            "step 35530: train loss = 2.0048, val loss = 2.0765\n",
            "step 35540: train loss = 1.9057, val loss = 1.8749\n",
            "step 35550: train loss = 1.9511, val loss = 1.9657\n",
            "step 35560: train loss = 1.7748, val loss = 1.7245\n",
            "step 35570: train loss = 1.7939, val loss = 2.0228\n",
            "step 35580: train loss = 2.0507, val loss = 1.9981\n",
            "step 35590: train loss = 1.7830, val loss = 1.8374\n",
            "step 35600: train loss = 1.8739, val loss = 1.9093\n",
            "step 35610: train loss = 1.7800, val loss = 1.9409\n",
            "step 35620: train loss = 1.7129, val loss = 1.8183\n",
            "step 35630: train loss = 1.9270, val loss = 1.9530\n",
            "step 35640: train loss = 1.8987, val loss = 1.9172\n",
            "step 35650: train loss = 1.9498, val loss = 1.9290\n",
            "step 35660: train loss = 1.9413, val loss = 1.8329\n",
            "step 35670: train loss = 1.9670, val loss = 1.7374\n",
            "step 35680: train loss = 1.9455, val loss = 2.0060\n",
            "step 35690: train loss = 1.9266, val loss = 1.9564\n",
            "step 35700: train loss = 1.9752, val loss = 1.8704\n",
            "step 35710: train loss = 1.9542, val loss = 1.9660\n",
            "step 35720: train loss = 2.0330, val loss = 2.0016\n",
            "step 35730: train loss = 1.9442, val loss = 1.7814\n",
            "step 35740: train loss = 1.8356, val loss = 1.7467\n",
            "step 35750: train loss = 1.9800, val loss = 1.6785\n",
            "step 35760: train loss = 1.8773, val loss = 1.8192\n",
            "step 35770: train loss = 1.9633, val loss = 1.7866\n",
            "step 35780: train loss = 1.8858, val loss = 1.9677\n",
            "step 35790: train loss = 1.8396, val loss = 1.8328\n",
            "step 35800: train loss = 1.9058, val loss = 1.9525\n",
            "step 35810: train loss = 1.7798, val loss = 1.8902\n",
            "step 35820: train loss = 1.9571, val loss = 1.9054\n",
            "step 35830: train loss = 1.8609, val loss = 1.8006\n",
            "step 35840: train loss = 1.8797, val loss = 1.7902\n",
            "step 35850: train loss = 1.8773, val loss = 1.7379\n",
            "step 35860: train loss = 1.7938, val loss = 1.6802\n",
            "step 35870: train loss = 1.9033, val loss = 1.9755\n",
            "step 35880: train loss = 1.9670, val loss = 1.9515\n",
            "step 35890: train loss = 1.8172, val loss = 1.9549\n",
            "step 35900: train loss = 1.8039, val loss = 1.7717\n",
            "step 35910: train loss = 1.7884, val loss = 1.9188\n",
            "step 35920: train loss = 2.0331, val loss = 1.8539\n",
            "step 35930: train loss = 1.7916, val loss = 1.8672\n",
            "step 35940: train loss = 1.9987, val loss = 1.8918\n",
            "step 35950: train loss = 1.8567, val loss = 1.6885\n",
            "step 35960: train loss = 1.8159, val loss = 1.8230\n",
            "step 35970: train loss = 1.9005, val loss = 2.0475\n",
            "step 35980: train loss = 2.1041, val loss = 1.8283\n",
            "step 35990: train loss = 1.8652, val loss = 1.9446\n",
            "step 36000: train loss = 1.9689, val loss = 1.9603\n",
            "step 36010: train loss = 1.8164, val loss = 1.9182\n",
            "step 36020: train loss = 2.0259, val loss = 1.7678\n",
            "step 36030: train loss = 1.9879, val loss = 1.8863\n",
            "step 36040: train loss = 1.7576, val loss = 1.8116\n",
            "step 36050: train loss = 1.8146, val loss = 1.6938\n",
            "step 36060: train loss = 1.9350, val loss = 1.8716\n",
            "step 36070: train loss = 1.8944, val loss = 1.6691\n",
            "step 36080: train loss = 1.8665, val loss = 1.9188\n",
            "step 36090: train loss = 1.9266, val loss = 1.8641\n",
            "step 36100: train loss = 1.8902, val loss = 1.8574\n",
            "step 36110: train loss = 1.9205, val loss = 2.0199\n",
            "step 36120: train loss = 1.8293, val loss = 1.9420\n",
            "step 36130: train loss = 1.7705, val loss = 1.8923\n",
            "step 36140: train loss = 1.8509, val loss = 1.9490\n",
            "step 36150: train loss = 1.8148, val loss = 1.8797\n",
            "step 36160: train loss = 1.7280, val loss = 1.9041\n",
            "step 36170: train loss = 1.9027, val loss = 1.8639\n",
            "step 36180: train loss = 1.7979, val loss = 1.8695\n",
            "step 36190: train loss = 1.9384, val loss = 2.0264\n",
            "step 36200: train loss = 1.8876, val loss = 1.7139\n",
            "step 36210: train loss = 1.9116, val loss = 1.8689\n",
            "step 36220: train loss = 1.9677, val loss = 1.9583\n",
            "step 36230: train loss = 1.9836, val loss = 1.9120\n",
            "step 36240: train loss = 1.8881, val loss = 1.9312\n",
            "step 36250: train loss = 1.8177, val loss = 1.7256\n",
            "step 36260: train loss = 1.9558, val loss = 1.8596\n",
            "step 36270: train loss = 1.8851, val loss = 1.8673\n",
            "step 36280: train loss = 1.9787, val loss = 1.9703\n",
            "step 36290: train loss = 1.8323, val loss = 2.0722\n",
            "step 36300: train loss = 1.8484, val loss = 1.8338\n",
            "step 36310: train loss = 1.7866, val loss = 1.8502\n",
            "step 36320: train loss = 1.6760, val loss = 1.9437\n",
            "step 36330: train loss = 1.8875, val loss = 1.8758\n",
            "step 36340: train loss = 1.9747, val loss = 1.9412\n",
            "step 36350: train loss = 1.8727, val loss = 1.8796\n",
            "step 36360: train loss = 1.8888, val loss = 1.8537\n",
            "step 36370: train loss = 1.8098, val loss = 1.9283\n",
            "step 36380: train loss = 1.9586, val loss = 1.9515\n",
            "step 36390: train loss = 1.8582, val loss = 1.8180\n",
            "step 36400: train loss = 1.7831, val loss = 2.0619\n",
            "step 36410: train loss = 1.8865, val loss = 1.8590\n",
            "step 36420: train loss = 1.9518, val loss = 1.8455\n",
            "step 36430: train loss = 1.7758, val loss = 1.7046\n",
            "step 36440: train loss = 1.8779, val loss = 2.0640\n",
            "step 36450: train loss = 1.7818, val loss = 1.8446\n",
            "step 36460: train loss = 2.0417, val loss = 1.8780\n",
            "step 36470: train loss = 1.9092, val loss = 1.8375\n",
            "step 36480: train loss = 1.8691, val loss = 1.8358\n",
            "step 36490: train loss = 1.9854, val loss = 2.0207\n",
            "step 36500: train loss = 1.8788, val loss = 1.7618\n",
            "step 36510: train loss = 1.7944, val loss = 1.8110\n",
            "step 36520: train loss = 2.0096, val loss = 1.9361\n",
            "step 36530: train loss = 2.0135, val loss = 2.0632\n",
            "step 36540: train loss = 1.7454, val loss = 1.8774\n",
            "step 36550: train loss = 1.8059, val loss = 1.8639\n",
            "step 36560: train loss = 1.8879, val loss = 1.9772\n",
            "step 36570: train loss = 1.9015, val loss = 1.8635\n",
            "step 36580: train loss = 1.8861, val loss = 1.9252\n",
            "step 36590: train loss = 1.8384, val loss = 1.9875\n",
            "step 36600: train loss = 1.8426, val loss = 1.9610\n",
            "step 36610: train loss = 1.8531, val loss = 2.0205\n",
            "step 36620: train loss = 1.9116, val loss = 1.7616\n",
            "step 36630: train loss = 1.9796, val loss = 1.9020\n",
            "step 36640: train loss = 1.8303, val loss = 1.9978\n",
            "step 36650: train loss = 1.8840, val loss = 1.9734\n",
            "step 36660: train loss = 1.9838, val loss = 1.9443\n",
            "step 36670: train loss = 1.9050, val loss = 2.0005\n",
            "step 36680: train loss = 1.7243, val loss = 1.7929\n",
            "step 36690: train loss = 1.8408, val loss = 1.9485\n",
            "step 36700: train loss = 1.8999, val loss = 1.9396\n",
            "step 36710: train loss = 1.8254, val loss = 1.9397\n",
            "step 36720: train loss = 1.8872, val loss = 1.9351\n",
            "step 36730: train loss = 1.7927, val loss = 1.8673\n",
            "step 36740: train loss = 1.8261, val loss = 1.9779\n",
            "step 36750: train loss = 1.7287, val loss = 1.8636\n",
            "step 36760: train loss = 1.8784, val loss = 1.9759\n",
            "step 36770: train loss = 2.0197, val loss = 1.7503\n",
            "step 36780: train loss = 1.8839, val loss = 1.8898\n",
            "step 36790: train loss = 1.7568, val loss = 1.9402\n",
            "step 36800: train loss = 1.8937, val loss = 2.0472\n",
            "step 36810: train loss = 1.8785, val loss = 1.8534\n",
            "step 36820: train loss = 1.8061, val loss = 1.8750\n",
            "step 36830: train loss = 1.8497, val loss = 1.8410\n",
            "step 36840: train loss = 1.8952, val loss = 1.9552\n",
            "step 36850: train loss = 1.7612, val loss = 1.8743\n",
            "step 36860: train loss = 1.8787, val loss = 1.9197\n",
            "step 36870: train loss = 1.8202, val loss = 1.8400\n",
            "step 36880: train loss = 1.8741, val loss = 1.8836\n",
            "step 36890: train loss = 1.7190, val loss = 1.7980\n",
            "step 36900: train loss = 1.8931, val loss = 1.7971\n",
            "step 36910: train loss = 1.7489, val loss = 1.7535\n",
            "step 36920: train loss = 1.8025, val loss = 1.8377\n",
            "step 36930: train loss = 1.7771, val loss = 1.8315\n",
            "step 36940: train loss = 1.8104, val loss = 1.8902\n",
            "step 36950: train loss = 1.8592, val loss = 1.8785\n",
            "step 36960: train loss = 1.8517, val loss = 1.7827\n",
            "step 36970: train loss = 1.7884, val loss = 1.8066\n",
            "step 36980: train loss = 1.6835, val loss = 1.9583\n",
            "step 36990: train loss = 1.8192, val loss = 1.8145\n",
            "step 37000: train loss = 1.6507, val loss = 1.8395\n",
            "step 37010: train loss = 1.8444, val loss = 1.8843\n",
            "step 37020: train loss = 1.9698, val loss = 1.9248\n",
            "step 37030: train loss = 1.7480, val loss = 1.8485\n",
            "step 37040: train loss = 1.9006, val loss = 1.9353\n",
            "step 37050: train loss = 2.0170, val loss = 1.9315\n",
            "step 37060: train loss = 1.9012, val loss = 1.9040\n",
            "step 37070: train loss = 1.7868, val loss = 1.7838\n",
            "step 37080: train loss = 1.9160, val loss = 1.9199\n",
            "step 37090: train loss = 1.7962, val loss = 1.8065\n",
            "step 37100: train loss = 1.8894, val loss = 1.8454\n",
            "step 37110: train loss = 1.7444, val loss = 1.7087\n",
            "step 37120: train loss = 1.7496, val loss = 1.8934\n",
            "step 37130: train loss = 1.6690, val loss = 1.8294\n",
            "step 37140: train loss = 1.8365, val loss = 2.1051\n",
            "step 37150: train loss = 1.9894, val loss = 1.8593\n",
            "step 37160: train loss = 1.9873, val loss = 1.8564\n",
            "step 37170: train loss = 1.8851, val loss = 1.8353\n",
            "step 37180: train loss = 1.8512, val loss = 1.8435\n",
            "step 37190: train loss = 1.8108, val loss = 1.8307\n",
            "step 37200: train loss = 1.8056, val loss = 1.9376\n",
            "step 37210: train loss = 1.8286, val loss = 1.7774\n",
            "step 37220: train loss = 1.7639, val loss = 1.7717\n",
            "step 37230: train loss = 1.9596, val loss = 1.7594\n",
            "step 37240: train loss = 1.8273, val loss = 1.8616\n",
            "step 37250: train loss = 1.8166, val loss = 1.8012\n",
            "step 37260: train loss = 1.7587, val loss = 1.9598\n",
            "step 37270: train loss = 1.7849, val loss = 1.8466\n",
            "step 37280: train loss = 1.8820, val loss = 2.0160\n",
            "step 37290: train loss = 1.8981, val loss = 1.8190\n",
            "step 37300: train loss = 1.8003, val loss = 1.8926\n",
            "step 37310: train loss = 1.8691, val loss = 1.8428\n",
            "step 37320: train loss = 1.9285, val loss = 1.8242\n",
            "step 37330: train loss = 1.9191, val loss = 1.8612\n",
            "step 37340: train loss = 1.9643, val loss = 1.7917\n",
            "step 37350: train loss = 1.8253, val loss = 1.8619\n",
            "step 37360: train loss = 1.8029, val loss = 1.7861\n",
            "step 37370: train loss = 1.7752, val loss = 1.9278\n",
            "step 37380: train loss = 1.8673, val loss = 1.6975\n",
            "step 37390: train loss = 1.8559, val loss = 1.9696\n",
            "step 37400: train loss = 1.8656, val loss = 1.9060\n",
            "step 37410: train loss = 1.7617, val loss = 2.0498\n",
            "step 37420: train loss = 1.8644, val loss = 1.8009\n",
            "step 37430: train loss = 1.8558, val loss = 1.8618\n",
            "step 37440: train loss = 1.7135, val loss = 1.9099\n",
            "step 37450: train loss = 1.7270, val loss = 1.8353\n",
            "step 37460: train loss = 1.9088, val loss = 1.8519\n",
            "step 37470: train loss = 1.9467, val loss = 1.8369\n",
            "step 37480: train loss = 1.9190, val loss = 1.8645\n",
            "step 37490: train loss = 1.8964, val loss = 1.7689\n",
            "step 37500: train loss = 1.7046, val loss = 1.9004\n",
            "step 37510: train loss = 1.8216, val loss = 1.7786\n",
            "step 37520: train loss = 1.7347, val loss = 1.8399\n",
            "step 37530: train loss = 1.8747, val loss = 1.7998\n",
            "step 37540: train loss = 1.7558, val loss = 1.8427\n",
            "step 37550: train loss = 1.8079, val loss = 1.8136\n",
            "step 37560: train loss = 1.9261, val loss = 1.8438\n",
            "step 37570: train loss = 1.8820, val loss = 1.8452\n",
            "step 37580: train loss = 1.9292, val loss = 1.7818\n",
            "step 37590: train loss = 1.8885, val loss = 1.9856\n",
            "step 37600: train loss = 1.7357, val loss = 1.8255\n",
            "step 37610: train loss = 1.7110, val loss = 1.8933\n",
            "step 37620: train loss = 1.8987, val loss = 2.0645\n",
            "step 37630: train loss = 1.8814, val loss = 1.9528\n",
            "step 37640: train loss = 1.9718, val loss = 1.7740\n",
            "step 37650: train loss = 1.8876, val loss = 1.8313\n",
            "step 37660: train loss = 1.7639, val loss = 1.9262\n",
            "step 37670: train loss = 1.9128, val loss = 1.9136\n",
            "step 37680: train loss = 1.8392, val loss = 1.8384\n",
            "step 37690: train loss = 1.8608, val loss = 1.8573\n",
            "step 37700: train loss = 1.9690, val loss = 1.8798\n",
            "step 37710: train loss = 1.8834, val loss = 1.8490\n",
            "step 37720: train loss = 1.8000, val loss = 1.9133\n",
            "step 37730: train loss = 1.8414, val loss = 1.8106\n",
            "step 37740: train loss = 1.7785, val loss = 1.8539\n",
            "step 37750: train loss = 1.8596, val loss = 1.9445\n",
            "step 37760: train loss = 1.9106, val loss = 1.6806\n",
            "step 37770: train loss = 1.9899, val loss = 1.8869\n",
            "step 37780: train loss = 1.9373, val loss = 1.7863\n",
            "step 37790: train loss = 1.8837, val loss = 1.9089\n",
            "step 37800: train loss = 1.8310, val loss = 1.8577\n",
            "step 37810: train loss = 1.9093, val loss = 1.9282\n",
            "step 37820: train loss = 1.9818, val loss = 1.9311\n",
            "step 37830: train loss = 1.9448, val loss = 1.8771\n",
            "step 37840: train loss = 1.8714, val loss = 1.9028\n",
            "step 37850: train loss = 1.8093, val loss = 1.9563\n",
            "step 37860: train loss = 1.9269, val loss = 1.9099\n",
            "step 37870: train loss = 1.8626, val loss = 1.9431\n",
            "step 37880: train loss = 1.9577, val loss = 1.7140\n",
            "step 37890: train loss = 1.8779, val loss = 1.9266\n",
            "step 37900: train loss = 1.8451, val loss = 1.9974\n",
            "step 37910: train loss = 1.8378, val loss = 1.7871\n",
            "step 37920: train loss = 1.7895, val loss = 1.8663\n",
            "step 37930: train loss = 1.9788, val loss = 1.9205\n",
            "step 37940: train loss = 1.8397, val loss = 1.6870\n",
            "step 37950: train loss = 1.9238, val loss = 1.7840\n",
            "step 37960: train loss = 1.9324, val loss = 1.8550\n",
            "step 37970: train loss = 1.8375, val loss = 1.8382\n",
            "step 37980: train loss = 1.7546, val loss = 1.6751\n",
            "step 37990: train loss = 1.8244, val loss = 1.8625\n",
            "step 38000: train loss = 1.7352, val loss = 1.7691\n",
            "step 38010: train loss = 1.7275, val loss = 1.9221\n",
            "step 38020: train loss = 1.7894, val loss = 1.8915\n",
            "step 38030: train loss = 1.8547, val loss = 1.8511\n",
            "step 38040: train loss = 2.0070, val loss = 1.8327\n",
            "step 38050: train loss = 1.9948, val loss = 1.8991\n",
            "step 38060: train loss = 1.9350, val loss = 1.9851\n",
            "step 38070: train loss = 1.7421, val loss = 1.8789\n",
            "step 38080: train loss = 1.8928, val loss = 1.8122\n",
            "step 38090: train loss = 1.8902, val loss = 1.7094\n",
            "step 38100: train loss = 1.8506, val loss = 1.7659\n",
            "step 38110: train loss = 1.8026, val loss = 1.8370\n",
            "step 38120: train loss = 1.9303, val loss = 1.8084\n",
            "step 38130: train loss = 1.8072, val loss = 1.8004\n",
            "step 38140: train loss = 1.7059, val loss = 1.9049\n",
            "step 38150: train loss = 1.8021, val loss = 1.8296\n",
            "step 38160: train loss = 1.7234, val loss = 1.8161\n",
            "step 38170: train loss = 2.0656, val loss = 2.0845\n",
            "step 38180: train loss = 2.0063, val loss = 1.9084\n",
            "step 38190: train loss = 1.7919, val loss = 1.8532\n",
            "step 38200: train loss = 1.8487, val loss = 1.9660\n",
            "step 38210: train loss = 1.7074, val loss = 1.9096\n",
            "step 38220: train loss = 1.7373, val loss = 1.9494\n",
            "step 38230: train loss = 1.7233, val loss = 1.9121\n",
            "step 38240: train loss = 1.8189, val loss = 1.8485\n",
            "step 38250: train loss = 1.9027, val loss = 1.8234\n",
            "step 38260: train loss = 1.9110, val loss = 1.8526\n",
            "step 38270: train loss = 1.8101, val loss = 1.9214\n",
            "step 38280: train loss = 1.8749, val loss = 1.7657\n",
            "step 38290: train loss = 1.7744, val loss = 1.8849\n",
            "step 38300: train loss = 1.8845, val loss = 1.9708\n",
            "step 38310: train loss = 1.8097, val loss = 1.9052\n",
            "step 38320: train loss = 2.0076, val loss = 1.8517\n",
            "step 38330: train loss = 1.8498, val loss = 1.9121\n",
            "step 38340: train loss = 1.9247, val loss = 1.8504\n",
            "step 38350: train loss = 1.8887, val loss = 1.8460\n",
            "step 38360: train loss = 1.9443, val loss = 1.9396\n",
            "step 38370: train loss = 1.9431, val loss = 1.9811\n",
            "step 38380: train loss = 1.8399, val loss = 1.9538\n",
            "step 38390: train loss = 1.8307, val loss = 1.9037\n",
            "step 38400: train loss = 1.8360, val loss = 1.9930\n",
            "step 38410: train loss = 1.9090, val loss = 1.8201\n",
            "step 38420: train loss = 1.8514, val loss = 1.8473\n",
            "step 38430: train loss = 1.9737, val loss = 1.7550\n",
            "step 38440: train loss = 1.8818, val loss = 1.8600\n",
            "step 38450: train loss = 1.8718, val loss = 1.8758\n",
            "step 38460: train loss = 1.9690, val loss = 1.7717\n",
            "step 38470: train loss = 1.7626, val loss = 1.8833\n",
            "step 38480: train loss = 1.8850, val loss = 1.8196\n",
            "step 38490: train loss = 1.9637, val loss = 1.8794\n",
            "step 38500: train loss = 1.9091, val loss = 1.7488\n",
            "step 38510: train loss = 1.8566, val loss = 1.8522\n",
            "step 38520: train loss = 1.9299, val loss = 1.9885\n",
            "step 38530: train loss = 1.9190, val loss = 2.0413\n",
            "step 38540: train loss = 1.8391, val loss = 1.7696\n",
            "step 38550: train loss = 1.8466, val loss = 2.0768\n",
            "step 38560: train loss = 1.8450, val loss = 1.8716\n",
            "step 38570: train loss = 1.8133, val loss = 1.8434\n",
            "step 38580: train loss = 1.7411, val loss = 1.8062\n",
            "step 38590: train loss = 1.7060, val loss = 1.9383\n",
            "step 38600: train loss = 1.8325, val loss = 1.9232\n",
            "step 38610: train loss = 1.8078, val loss = 2.0044\n",
            "step 38620: train loss = 1.8874, val loss = 1.8509\n",
            "step 38630: train loss = 1.7890, val loss = 1.7981\n",
            "step 38640: train loss = 1.9540, val loss = 1.9009\n",
            "step 38650: train loss = 1.8349, val loss = 1.8895\n",
            "step 38660: train loss = 1.7283, val loss = 2.0101\n",
            "step 38670: train loss = 1.7940, val loss = 1.9176\n",
            "step 38680: train loss = 1.8961, val loss = 1.8542\n",
            "step 38690: train loss = 1.8003, val loss = 1.8519\n",
            "step 38700: train loss = 1.7908, val loss = 1.8317\n",
            "step 38710: train loss = 1.8508, val loss = 1.9730\n",
            "step 38720: train loss = 1.9062, val loss = 1.9120\n",
            "step 38730: train loss = 1.9299, val loss = 1.9074\n",
            "step 38740: train loss = 1.8336, val loss = 1.8751\n",
            "step 38750: train loss = 1.9226, val loss = 1.9608\n",
            "step 38760: train loss = 1.9185, val loss = 1.8404\n",
            "step 38770: train loss = 1.7591, val loss = 1.6718\n",
            "step 38780: train loss = 1.8636, val loss = 1.8303\n",
            "step 38790: train loss = 1.8903, val loss = 1.8706\n",
            "step 38800: train loss = 1.8318, val loss = 1.6547\n",
            "step 38810: train loss = 1.8432, val loss = 1.8695\n",
            "step 38820: train loss = 1.8511, val loss = 1.7482\n",
            "step 38830: train loss = 1.8302, val loss = 2.0044\n",
            "step 38840: train loss = 1.8312, val loss = 1.9312\n",
            "step 38850: train loss = 1.8798, val loss = 1.8632\n",
            "step 38860: train loss = 2.0651, val loss = 1.8489\n",
            "step 38870: train loss = 1.7555, val loss = 1.8172\n",
            "step 38880: train loss = 1.8806, val loss = 1.8390\n",
            "step 38890: train loss = 1.7411, val loss = 1.7765\n",
            "step 38900: train loss = 1.6603, val loss = 1.7009\n",
            "step 38910: train loss = 1.8949, val loss = 1.8600\n",
            "step 38920: train loss = 1.9545, val loss = 1.8610\n",
            "step 38930: train loss = 1.7640, val loss = 1.8677\n",
            "step 38940: train loss = 1.8813, val loss = 1.8885\n",
            "step 38950: train loss = 1.8830, val loss = 1.9628\n",
            "step 38960: train loss = 1.8201, val loss = 1.8562\n",
            "step 38970: train loss = 1.8576, val loss = 1.8013\n",
            "step 38980: train loss = 1.7273, val loss = 1.9702\n",
            "step 38990: train loss = 2.0011, val loss = 1.9868\n",
            "step 39000: train loss = 1.8417, val loss = 1.9334\n",
            "step 39010: train loss = 1.8995, val loss = 1.9098\n",
            "step 39020: train loss = 1.8655, val loss = 1.7327\n",
            "step 39030: train loss = 1.7977, val loss = 1.8850\n",
            "step 39040: train loss = 1.7786, val loss = 1.9148\n",
            "step 39050: train loss = 1.8301, val loss = 1.9489\n",
            "step 39060: train loss = 1.9970, val loss = 1.9053\n",
            "step 39070: train loss = 1.7036, val loss = 1.8732\n",
            "step 39080: train loss = 1.6952, val loss = 1.8583\n",
            "step 39090: train loss = 1.8075, val loss = 1.9366\n",
            "step 39100: train loss = 1.8254, val loss = 1.8003\n",
            "step 39110: train loss = 1.8848, val loss = 1.8265\n",
            "step 39120: train loss = 1.8475, val loss = 1.7754\n",
            "step 39130: train loss = 1.8909, val loss = 1.7925\n",
            "step 39140: train loss = 1.9349, val loss = 1.9421\n",
            "step 39150: train loss = 1.9816, val loss = 1.8723\n",
            "step 39160: train loss = 1.8414, val loss = 1.8455\n",
            "step 39170: train loss = 1.8490, val loss = 1.8782\n",
            "step 39180: train loss = 1.8509, val loss = 2.0143\n",
            "step 39190: train loss = 1.7981, val loss = 1.7419\n",
            "step 39200: train loss = 1.9099, val loss = 1.9437\n",
            "step 39210: train loss = 1.7613, val loss = 1.9070\n",
            "step 39220: train loss = 1.7630, val loss = 2.0191\n",
            "step 39230: train loss = 1.8305, val loss = 1.9725\n",
            "step 39240: train loss = 1.8845, val loss = 1.9835\n",
            "step 39250: train loss = 2.0632, val loss = 1.8444\n",
            "step 39260: train loss = 1.8914, val loss = 1.7750\n",
            "step 39270: train loss = 1.8613, val loss = 1.9263\n",
            "step 39280: train loss = 1.6264, val loss = 2.0153\n",
            "step 39290: train loss = 1.7809, val loss = 1.9861\n",
            "step 39300: train loss = 1.8163, val loss = 1.8716\n",
            "step 39310: train loss = 1.8361, val loss = 1.8428\n",
            "step 39320: train loss = 1.7627, val loss = 1.8146\n",
            "step 39330: train loss = 1.7867, val loss = 1.8044\n",
            "step 39340: train loss = 1.8449, val loss = 1.8240\n",
            "step 39350: train loss = 1.6012, val loss = 1.9228\n",
            "step 39360: train loss = 1.7461, val loss = 1.9304\n",
            "step 39370: train loss = 1.7719, val loss = 1.8850\n",
            "step 39380: train loss = 1.9145, val loss = 1.8508\n",
            "step 39390: train loss = 1.8775, val loss = 1.8552\n",
            "step 39400: train loss = 1.7100, val loss = 1.8850\n",
            "step 39410: train loss = 1.7683, val loss = 1.8890\n",
            "step 39420: train loss = 1.8232, val loss = 1.7908\n",
            "step 39430: train loss = 1.9026, val loss = 1.7754\n",
            "step 39440: train loss = 1.6847, val loss = 1.7286\n",
            "step 39450: train loss = 1.9500, val loss = 1.7874\n",
            "step 39460: train loss = 1.8539, val loss = 1.8424\n",
            "step 39470: train loss = 1.6474, val loss = 1.8140\n",
            "step 39480: train loss = 1.8340, val loss = 1.9892\n",
            "step 39490: train loss = 1.8541, val loss = 1.8374\n",
            "step 39500: train loss = 1.7805, val loss = 1.8220\n",
            "step 39510: train loss = 1.9833, val loss = 2.0825\n",
            "step 39520: train loss = 1.8485, val loss = 1.8769\n",
            "step 39530: train loss = 1.8431, val loss = 1.7776\n",
            "step 39540: train loss = 1.7492, val loss = 1.9125\n",
            "step 39550: train loss = 1.7833, val loss = 1.9123\n",
            "step 39560: train loss = 1.9082, val loss = 1.8164\n",
            "step 39570: train loss = 1.7927, val loss = 1.9267\n",
            "step 39580: train loss = 1.8322, val loss = 1.7129\n",
            "step 39590: train loss = 1.9400, val loss = 1.8044\n",
            "step 39600: train loss = 1.7312, val loss = 1.8625\n",
            "step 39610: train loss = 1.8174, val loss = 1.8464\n",
            "step 39620: train loss = 1.9049, val loss = 1.8351\n",
            "step 39630: train loss = 1.8886, val loss = 2.0047\n",
            "step 39640: train loss = 1.8183, val loss = 1.8092\n",
            "step 39650: train loss = 1.8525, val loss = 1.7767\n",
            "step 39660: train loss = 1.8795, val loss = 1.9033\n",
            "step 39670: train loss = 1.8224, val loss = 1.8132\n",
            "step 39680: train loss = 1.8657, val loss = 1.7851\n",
            "step 39690: train loss = 1.8878, val loss = 1.7911\n",
            "step 39700: train loss = 1.8691, val loss = 2.0118\n",
            "step 39710: train loss = 1.9211, val loss = 1.9282\n",
            "step 39720: train loss = 2.0316, val loss = 1.9619\n",
            "step 39730: train loss = 1.9070, val loss = 1.9761\n",
            "step 39740: train loss = 1.9460, val loss = 1.9884\n",
            "step 39750: train loss = 1.8355, val loss = 1.8846\n",
            "step 39760: train loss = 1.7583, val loss = 1.6272\n",
            "step 39770: train loss = 1.9742, val loss = 1.7691\n",
            "step 39780: train loss = 1.9475, val loss = 1.8896\n",
            "step 39790: train loss = 1.9487, val loss = 1.8457\n",
            "step 39800: train loss = 1.9830, val loss = 2.0075\n",
            "step 39810: train loss = 1.9192, val loss = 1.8922\n",
            "step 39820: train loss = 1.7941, val loss = 1.8583\n",
            "step 39830: train loss = 1.8519, val loss = 1.9363\n",
            "step 39840: train loss = 1.8960, val loss = 1.9509\n",
            "step 39850: train loss = 1.7801, val loss = 1.7292\n",
            "step 39860: train loss = 1.9202, val loss = 1.9314\n",
            "step 39870: train loss = 1.9018, val loss = 1.9616\n",
            "step 39880: train loss = 1.7602, val loss = 1.8763\n",
            "step 39890: train loss = 1.9428, val loss = 1.8833\n",
            "step 39900: train loss = 1.8380, val loss = 1.8606\n",
            "step 39910: train loss = 1.9329, val loss = 1.9377\n",
            "step 39920: train loss = 1.8081, val loss = 1.7123\n",
            "step 39930: train loss = 1.8821, val loss = 1.9968\n",
            "step 39940: train loss = 1.9391, val loss = 1.8678\n",
            "step 39950: train loss = 1.9081, val loss = 1.8579\n",
            "step 39960: train loss = 1.8527, val loss = 1.9060\n",
            "step 39970: train loss = 1.7712, val loss = 1.8368\n",
            "step 39980: train loss = 1.8134, val loss = 1.8485\n",
            "step 39990: train loss = 1.8180, val loss = 1.7340\n",
            "step 40000: train loss = 1.9409, val loss = 1.9256\n",
            "step 40010: train loss = 1.8678, val loss = 1.9866\n",
            "step 40020: train loss = 1.7769, val loss = 1.7421\n",
            "step 40030: train loss = 2.0511, val loss = 1.7389\n",
            "step 40040: train loss = 1.9842, val loss = 1.8304\n",
            "step 40050: train loss = 1.7696, val loss = 1.8726\n",
            "step 40060: train loss = 1.7789, val loss = 1.9555\n",
            "step 40070: train loss = 1.8982, val loss = 1.8128\n",
            "step 40080: train loss = 1.9426, val loss = 1.9577\n",
            "step 40090: train loss = 1.7134, val loss = 1.8899\n",
            "step 40100: train loss = 1.8151, val loss = 1.8670\n",
            "step 40110: train loss = 1.8275, val loss = 1.7948\n",
            "step 40120: train loss = 1.7425, val loss = 1.7369\n",
            "step 40130: train loss = 1.7958, val loss = 1.8409\n",
            "step 40140: train loss = 1.8316, val loss = 2.0609\n",
            "step 40150: train loss = 1.8599, val loss = 1.8486\n",
            "step 40160: train loss = 1.9326, val loss = 2.0020\n",
            "step 40170: train loss = 2.0013, val loss = 2.0444\n",
            "step 40180: train loss = 2.0376, val loss = 2.0066\n",
            "step 40190: train loss = 1.9349, val loss = 1.8459\n",
            "step 40200: train loss = 1.9578, val loss = 2.0820\n",
            "step 40210: train loss = 1.9378, val loss = 1.9408\n",
            "step 40220: train loss = 2.0346, val loss = 2.0973\n",
            "step 40230: train loss = 1.9601, val loss = 1.9790\n",
            "step 40240: train loss = 1.9859, val loss = 1.8617\n",
            "step 40250: train loss = 1.9962, val loss = 2.0541\n",
            "step 40260: train loss = 1.9488, val loss = 2.1203\n",
            "step 40270: train loss = 1.8875, val loss = 1.8475\n",
            "step 40280: train loss = 1.8657, val loss = 1.8907\n",
            "step 40290: train loss = 1.8500, val loss = 1.9086\n",
            "step 40300: train loss = 1.9862, val loss = 1.7982\n",
            "step 40310: train loss = 1.9656, val loss = 1.9472\n",
            "step 40320: train loss = 1.9068, val loss = 1.9211\n",
            "step 40330: train loss = 1.8986, val loss = 1.8163\n",
            "step 40340: train loss = 1.8803, val loss = 1.9104\n",
            "step 40350: train loss = 1.8457, val loss = 1.9284\n",
            "step 40360: train loss = 1.7974, val loss = 1.8291\n",
            "step 40370: train loss = 1.8843, val loss = 1.8503\n",
            "step 40380: train loss = 1.8778, val loss = 1.9110\n",
            "step 40390: train loss = 1.8305, val loss = 2.0161\n",
            "step 40400: train loss = 1.8754, val loss = 1.7789\n",
            "step 40410: train loss = 1.7320, val loss = 1.9215\n",
            "step 40420: train loss = 1.8712, val loss = 1.9043\n",
            "step 40430: train loss = 1.8604, val loss = 1.7436\n",
            "step 40440: train loss = 1.9429, val loss = 1.8852\n",
            "step 40450: train loss = 1.6383, val loss = 1.9854\n",
            "step 40460: train loss = 1.9083, val loss = 1.9035\n",
            "step 40470: train loss = 1.9185, val loss = 1.7853\n",
            "step 40480: train loss = 1.7397, val loss = 1.7930\n",
            "step 40490: train loss = 1.9338, val loss = 1.9326\n",
            "step 40500: train loss = 1.8527, val loss = 1.7175\n",
            "step 40510: train loss = 1.8387, val loss = 1.7990\n",
            "step 40520: train loss = 1.8031, val loss = 1.8050\n",
            "step 40530: train loss = 1.8805, val loss = 1.8640\n",
            "step 40540: train loss = 1.9604, val loss = 1.9383\n",
            "step 40550: train loss = 1.8405, val loss = 1.8827\n",
            "step 40560: train loss = 1.8478, val loss = 1.8894\n",
            "step 40570: train loss = 1.9370, val loss = 1.7928\n",
            "step 40580: train loss = 1.8930, val loss = 1.8206\n",
            "step 40590: train loss = 1.7885, val loss = 1.8919\n",
            "step 40600: train loss = 1.9136, val loss = 1.8100\n",
            "step 40610: train loss = 1.8227, val loss = 1.8601\n",
            "step 40620: train loss = 1.8578, val loss = 1.8001\n",
            "step 40630: train loss = 1.9403, val loss = 1.8430\n",
            "step 40640: train loss = 1.7723, val loss = 1.8737\n",
            "step 40650: train loss = 1.9668, val loss = 1.7669\n",
            "step 40660: train loss = 1.8511, val loss = 1.8465\n",
            "step 40670: train loss = 2.0001, val loss = 1.7695\n",
            "step 40680: train loss = 1.9741, val loss = 1.9736\n",
            "step 40690: train loss = 1.8650, val loss = 1.9234\n",
            "step 40700: train loss = 1.7964, val loss = 1.9306\n",
            "step 40710: train loss = 1.8816, val loss = 1.8359\n",
            "step 40720: train loss = 1.9399, val loss = 1.9056\n",
            "step 40730: train loss = 1.8628, val loss = 1.7749\n",
            "step 40740: train loss = 1.9594, val loss = 2.0364\n",
            "step 40750: train loss = 1.8902, val loss = 2.0434\n",
            "step 40760: train loss = 1.8352, val loss = 1.9492\n",
            "step 40770: train loss = 2.0266, val loss = 1.9318\n",
            "step 40780: train loss = 1.8324, val loss = 1.8140\n",
            "step 40790: train loss = 1.7480, val loss = 1.8817\n",
            "step 40800: train loss = 1.7674, val loss = 1.8538\n",
            "step 40810: train loss = 1.7932, val loss = 1.9071\n",
            "step 40820: train loss = 1.9827, val loss = 1.8559\n",
            "step 40830: train loss = 1.7629, val loss = 1.9501\n",
            "step 40840: train loss = 1.8098, val loss = 1.9981\n",
            "step 40850: train loss = 1.8128, val loss = 1.7917\n",
            "step 40860: train loss = 1.8396, val loss = 1.8056\n",
            "step 40870: train loss = 1.9642, val loss = 1.8670\n",
            "step 40880: train loss = 1.8808, val loss = 1.8984\n",
            "step 40890: train loss = 1.8765, val loss = 1.8835\n",
            "step 40900: train loss = 1.8098, val loss = 1.9160\n",
            "step 40910: train loss = 1.8726, val loss = 1.9335\n",
            "step 40920: train loss = 2.0310, val loss = 1.9882\n",
            "step 40930: train loss = 1.6996, val loss = 1.8423\n",
            "step 40940: train loss = 1.7802, val loss = 1.8239\n",
            "step 40950: train loss = 1.8376, val loss = 1.8705\n",
            "step 40960: train loss = 1.7755, val loss = 2.0022\n",
            "step 40970: train loss = 1.8731, val loss = 1.7571\n",
            "step 40980: train loss = 1.8819, val loss = 1.8709\n",
            "step 40990: train loss = 1.8300, val loss = 1.8251\n",
            "step 41000: train loss = 1.8441, val loss = 1.9399\n",
            "step 41010: train loss = 1.7976, val loss = 1.9268\n",
            "step 41020: train loss = 1.7718, val loss = 1.7139\n",
            "step 41030: train loss = 1.9785, val loss = 1.9848\n",
            "step 41040: train loss = 1.7957, val loss = 1.9268\n",
            "step 41050: train loss = 1.9119, val loss = 1.9252\n",
            "step 41060: train loss = 1.9286, val loss = 1.9769\n",
            "step 41070: train loss = 2.0012, val loss = 1.9415\n",
            "step 41080: train loss = 2.0359, val loss = 1.8798\n",
            "step 41090: train loss = 1.8045, val loss = 1.8839\n",
            "step 41100: train loss = 1.8204, val loss = 1.8552\n",
            "step 41110: train loss = 1.9002, val loss = 1.8656\n",
            "step 41120: train loss = 1.9174, val loss = 1.8263\n",
            "step 41130: train loss = 1.8169, val loss = 1.9650\n",
            "step 41140: train loss = 1.9714, val loss = 1.9188\n",
            "step 41150: train loss = 2.0241, val loss = 1.8380\n",
            "step 41160: train loss = 1.8413, val loss = 1.7973\n",
            "step 41170: train loss = 1.8814, val loss = 1.7737\n",
            "step 41180: train loss = 1.8720, val loss = 1.9549\n",
            "step 41190: train loss = 1.7185, val loss = 1.9573\n",
            "step 41200: train loss = 1.8539, val loss = 2.0004\n",
            "step 41210: train loss = 1.8050, val loss = 1.9121\n",
            "step 41220: train loss = 1.7895, val loss = 1.9368\n",
            "step 41230: train loss = 1.9047, val loss = 1.8823\n",
            "step 41240: train loss = 1.7698, val loss = 1.9435\n",
            "step 41250: train loss = 1.9063, val loss = 1.9370\n",
            "step 41260: train loss = 2.0258, val loss = 1.9816\n",
            "step 41270: train loss = 2.0575, val loss = 1.9124\n",
            "step 41280: train loss = 1.9853, val loss = 1.7370\n",
            "step 41290: train loss = 1.7154, val loss = 1.9390\n",
            "step 41300: train loss = 1.9565, val loss = 1.8522\n",
            "step 41310: train loss = 2.0715, val loss = 1.8657\n",
            "step 41320: train loss = 1.9132, val loss = 1.9411\n",
            "step 41330: train loss = 1.8627, val loss = 1.8141\n",
            "step 41340: train loss = 2.0272, val loss = 1.8656\n",
            "step 41350: train loss = 1.9040, val loss = 1.9474\n",
            "step 41360: train loss = 1.9993, val loss = 1.7873\n",
            "step 41370: train loss = 1.8804, val loss = 1.8672\n",
            "step 41380: train loss = 1.8402, val loss = 1.8688\n",
            "step 41390: train loss = 1.9958, val loss = 1.9242\n",
            "step 41400: train loss = 1.7820, val loss = 1.8015\n",
            "step 41410: train loss = 1.8508, val loss = 1.9170\n",
            "step 41420: train loss = 1.9341, val loss = 1.8342\n",
            "step 41430: train loss = 1.7656, val loss = 1.8726\n",
            "step 41440: train loss = 1.7097, val loss = 1.9125\n",
            "step 41450: train loss = 1.7614, val loss = 1.9152\n",
            "step 41460: train loss = 1.9669, val loss = 1.9234\n",
            "step 41470: train loss = 1.9018, val loss = 1.8596\n",
            "step 41480: train loss = 1.7132, val loss = 1.9324\n",
            "step 41490: train loss = 1.8343, val loss = 1.7900\n",
            "step 41500: train loss = 1.9710, val loss = 1.8838\n",
            "step 41510: train loss = 1.9266, val loss = 1.9525\n",
            "step 41520: train loss = 1.9554, val loss = 1.7965\n",
            "step 41530: train loss = 1.9218, val loss = 1.9443\n",
            "step 41540: train loss = 1.8319, val loss = 1.8113\n",
            "step 41550: train loss = 1.8995, val loss = 1.7892\n",
            "step 41560: train loss = 1.8907, val loss = 1.9431\n",
            "step 41570: train loss = 1.8758, val loss = 1.9326\n",
            "step 41580: train loss = 1.8149, val loss = 1.9760\n",
            "step 41590: train loss = 1.9236, val loss = 2.0209\n",
            "step 41600: train loss = 1.8183, val loss = 1.9510\n",
            "step 41610: train loss = 1.8933, val loss = 1.8753\n",
            "step 41620: train loss = 1.9806, val loss = 1.8008\n",
            "step 41630: train loss = 1.8867, val loss = 1.7874\n",
            "step 41640: train loss = 1.9091, val loss = 1.8437\n",
            "step 41650: train loss = 1.9116, val loss = 1.9439\n",
            "step 41660: train loss = 1.8127, val loss = 1.9787\n",
            "step 41670: train loss = 1.9041, val loss = 1.8746\n",
            "step 41680: train loss = 1.9019, val loss = 1.9369\n",
            "step 41690: train loss = 1.7066, val loss = 1.8434\n",
            "step 41700: train loss = 1.9343, val loss = 1.8712\n",
            "step 41710: train loss = 1.9491, val loss = 1.8489\n",
            "step 41720: train loss = 1.9671, val loss = 1.8855\n",
            "step 41730: train loss = 1.8068, val loss = 1.8145\n",
            "step 41740: train loss = 1.8464, val loss = 1.9252\n",
            "step 41750: train loss = 1.8779, val loss = 1.7685\n",
            "step 41760: train loss = 1.9457, val loss = 1.8157\n",
            "step 41770: train loss = 1.9513, val loss = 1.9590\n",
            "step 41780: train loss = 1.8835, val loss = 1.8997\n",
            "step 41790: train loss = 1.8289, val loss = 1.7213\n",
            "step 41800: train loss = 1.9705, val loss = 1.8166\n",
            "step 41810: train loss = 1.9013, val loss = 1.8349\n",
            "step 41820: train loss = 1.8115, val loss = 1.8585\n",
            "step 41830: train loss = 1.9862, val loss = 1.9293\n",
            "step 41840: train loss = 1.8285, val loss = 1.9552\n",
            "step 41850: train loss = 1.8454, val loss = 1.8794\n",
            "step 41860: train loss = 1.9105, val loss = 2.0432\n",
            "step 41870: train loss = 1.9120, val loss = 1.8858\n",
            "step 41880: train loss = 1.9746, val loss = 1.8709\n",
            "step 41890: train loss = 1.8717, val loss = 1.8885\n",
            "step 41900: train loss = 1.7790, val loss = 1.7395\n",
            "step 41910: train loss = 1.9308, val loss = 1.7755\n",
            "step 41920: train loss = 1.9742, val loss = 1.7928\n",
            "step 41930: train loss = 1.9120, val loss = 1.9563\n",
            "step 41940: train loss = 1.8774, val loss = 1.7530\n",
            "step 41950: train loss = 1.9290, val loss = 1.8132\n",
            "step 41960: train loss = 1.8217, val loss = 2.0082\n",
            "step 41970: train loss = 1.7791, val loss = 1.9655\n",
            "step 41980: train loss = 1.9860, val loss = 1.9386\n",
            "step 41990: train loss = 1.7246, val loss = 1.7782\n",
            "step 42000: train loss = 1.9250, val loss = 1.9243\n",
            "step 42010: train loss = 1.8137, val loss = 1.7515\n",
            "step 42020: train loss = 1.6504, val loss = 1.9423\n",
            "step 42030: train loss = 1.6719, val loss = 1.9069\n",
            "step 42040: train loss = 1.6679, val loss = 1.8639\n",
            "step 42050: train loss = 1.8955, val loss = 1.9797\n",
            "step 42060: train loss = 1.8352, val loss = 1.9094\n",
            "step 42070: train loss = 1.8065, val loss = 1.8108\n",
            "step 42080: train loss = 1.6912, val loss = 1.9153\n",
            "step 42090: train loss = 1.8966, val loss = 1.8383\n",
            "step 42100: train loss = 1.9636, val loss = 1.8272\n",
            "step 42110: train loss = 1.8665, val loss = 1.8633\n",
            "step 42120: train loss = 1.8714, val loss = 1.7625\n",
            "step 42130: train loss = 1.9923, val loss = 1.8300\n",
            "step 42140: train loss = 1.8389, val loss = 1.8083\n",
            "step 42150: train loss = 1.6924, val loss = 1.9121\n",
            "step 42160: train loss = 2.0111, val loss = 1.7421\n",
            "step 42170: train loss = 1.7201, val loss = 1.8119\n",
            "step 42180: train loss = 1.8188, val loss = 2.0469\n",
            "step 42190: train loss = 1.6938, val loss = 1.9579\n",
            "step 42200: train loss = 1.8665, val loss = 1.9076\n",
            "step 42210: train loss = 1.9116, val loss = 1.7916\n",
            "step 42220: train loss = 1.8528, val loss = 2.0065\n",
            "step 42230: train loss = 1.8318, val loss = 1.8974\n",
            "step 42240: train loss = 1.9627, val loss = 1.9280\n",
            "step 42250: train loss = 1.9141, val loss = 1.8639\n",
            "step 42260: train loss = 1.7805, val loss = 1.7800\n",
            "step 42270: train loss = 1.8283, val loss = 1.6772\n",
            "step 42280: train loss = 1.8159, val loss = 1.9280\n",
            "step 42290: train loss = 1.8691, val loss = 1.7987\n",
            "step 42300: train loss = 1.8247, val loss = 1.7812\n",
            "step 42310: train loss = 1.8909, val loss = 1.9117\n",
            "step 42320: train loss = 1.8663, val loss = 1.7337\n",
            "step 42330: train loss = 1.6291, val loss = 1.8228\n",
            "step 42340: train loss = 1.8230, val loss = 1.7814\n",
            "step 42350: train loss = 1.8202, val loss = 1.6971\n",
            "step 42360: train loss = 1.8669, val loss = 1.7418\n",
            "step 42370: train loss = 1.7914, val loss = 1.8301\n",
            "step 42380: train loss = 1.9126, val loss = 1.9710\n",
            "step 42390: train loss = 2.0267, val loss = 1.9287\n",
            "step 42400: train loss = 1.8027, val loss = 1.8245\n",
            "step 42410: train loss = 1.8649, val loss = 1.7095\n",
            "step 42420: train loss = 1.8279, val loss = 2.0078\n",
            "step 42430: train loss = 1.8161, val loss = 1.7622\n",
            "step 42440: train loss = 1.7215, val loss = 1.9264\n",
            "step 42450: train loss = 1.9690, val loss = 1.9464\n",
            "step 42460: train loss = 1.9325, val loss = 1.8418\n",
            "step 42470: train loss = 2.0495, val loss = 1.8824\n",
            "step 42480: train loss = 1.8326, val loss = 1.8340\n",
            "step 42490: train loss = 1.7795, val loss = 1.9244\n",
            "step 42500: train loss = 1.9007, val loss = 1.7990\n",
            "step 42510: train loss = 1.9249, val loss = 1.9071\n",
            "step 42520: train loss = 1.9205, val loss = 1.9406\n",
            "step 42530: train loss = 1.8706, val loss = 1.9581\n",
            "step 42540: train loss = 1.9029, val loss = 2.0035\n",
            "step 42550: train loss = 1.9290, val loss = 1.8017\n",
            "step 42560: train loss = 1.8895, val loss = 1.8201\n",
            "step 42570: train loss = 1.7300, val loss = 1.8424\n",
            "step 42580: train loss = 1.8825, val loss = 1.7645\n",
            "step 42590: train loss = 1.6953, val loss = 1.8499\n",
            "step 42600: train loss = 1.8138, val loss = 1.7331\n",
            "step 42610: train loss = 1.8971, val loss = 1.8967\n",
            "step 42620: train loss = 1.7646, val loss = 1.7897\n",
            "step 42630: train loss = 1.9230, val loss = 1.7741\n",
            "step 42640: train loss = 1.9086, val loss = 1.7746\n",
            "step 42650: train loss = 1.8937, val loss = 1.8596\n",
            "step 42660: train loss = 1.9584, val loss = 1.8614\n",
            "step 42670: train loss = 1.9803, val loss = 1.8616\n",
            "step 42680: train loss = 1.8077, val loss = 1.7100\n",
            "step 42690: train loss = 1.8909, val loss = 1.9693\n",
            "step 42700: train loss = 1.7655, val loss = 1.8689\n",
            "step 42710: train loss = 1.7357, val loss = 1.8731\n",
            "step 42720: train loss = 1.8396, val loss = 1.7957\n",
            "step 42730: train loss = 1.7632, val loss = 1.9294\n",
            "step 42740: train loss = 1.7600, val loss = 1.8727\n",
            "step 42750: train loss = 1.7952, val loss = 1.9189\n",
            "step 42760: train loss = 2.0007, val loss = 1.8651\n",
            "step 42770: train loss = 2.0100, val loss = 1.9462\n",
            "step 42780: train loss = 1.8745, val loss = 1.9046\n",
            "step 42790: train loss = 1.9471, val loss = 1.9752\n",
            "step 42800: train loss = 1.9745, val loss = 1.8069\n",
            "step 42810: train loss = 1.7695, val loss = 1.9205\n",
            "step 42820: train loss = 1.8546, val loss = 1.8825\n",
            "step 42830: train loss = 1.8442, val loss = 1.7748\n",
            "step 42840: train loss = 1.7641, val loss = 1.8320\n",
            "step 42850: train loss = 2.0007, val loss = 2.0274\n",
            "step 42860: train loss = 1.7024, val loss = 1.7817\n",
            "step 42870: train loss = 1.8739, val loss = 1.7890\n",
            "step 42880: train loss = 1.7417, val loss = 1.8383\n",
            "step 42890: train loss = 1.9278, val loss = 1.9081\n",
            "step 42900: train loss = 1.8413, val loss = 1.8148\n",
            "step 42910: train loss = 1.8612, val loss = 2.0081\n",
            "step 42920: train loss = 1.7720, val loss = 1.8640\n",
            "step 42930: train loss = 1.9412, val loss = 1.9277\n",
            "step 42940: train loss = 2.0351, val loss = 1.9496\n",
            "step 42950: train loss = 1.9072, val loss = 1.8721\n",
            "step 42960: train loss = 1.9705, val loss = 1.7935\n",
            "step 42970: train loss = 1.8794, val loss = 1.8381\n",
            "step 42980: train loss = 1.9250, val loss = 1.8984\n",
            "step 42990: train loss = 1.9363, val loss = 1.8861\n",
            "step 43000: train loss = 1.7603, val loss = 1.8524\n",
            "step 43010: train loss = 1.8091, val loss = 1.8722\n",
            "step 43020: train loss = 1.9564, val loss = 1.9488\n",
            "step 43030: train loss = 1.9772, val loss = 1.8928\n",
            "step 43040: train loss = 1.8316, val loss = 1.7019\n",
            "step 43050: train loss = 1.8246, val loss = 1.8442\n",
            "step 43060: train loss = 2.0075, val loss = 1.9345\n",
            "step 43070: train loss = 1.7408, val loss = 1.8497\n",
            "step 43080: train loss = 1.8897, val loss = 1.9756\n",
            "step 43090: train loss = 1.8681, val loss = 1.9192\n",
            "step 43100: train loss = 1.8631, val loss = 1.8349\n",
            "step 43110: train loss = 1.6153, val loss = 1.8312\n",
            "step 43120: train loss = 1.8058, val loss = 1.9201\n",
            "step 43130: train loss = 1.8211, val loss = 1.7726\n",
            "step 43140: train loss = 1.8052, val loss = 1.9237\n",
            "step 43150: train loss = 1.7608, val loss = 1.9627\n",
            "step 43160: train loss = 1.9337, val loss = 1.9822\n",
            "step 43170: train loss = 1.9409, val loss = 1.8224\n",
            "step 43180: train loss = 1.8740, val loss = 2.0083\n",
            "step 43190: train loss = 1.9480, val loss = 1.8445\n",
            "step 43200: train loss = 1.7322, val loss = 1.8967\n",
            "step 43210: train loss = 1.8045, val loss = 1.8321\n",
            "step 43220: train loss = 1.9120, val loss = 1.9271\n",
            "step 43230: train loss = 1.9927, val loss = 1.9863\n",
            "step 43240: train loss = 1.8598, val loss = 1.8424\n",
            "step 43250: train loss = 1.9223, val loss = 1.8420\n",
            "step 43260: train loss = 1.8155, val loss = 1.8757\n",
            "step 43270: train loss = 1.8768, val loss = 1.9810\n",
            "step 43280: train loss = 2.0304, val loss = 2.0490\n",
            "step 43290: train loss = 1.8324, val loss = 1.7271\n",
            "step 43300: train loss = 1.9118, val loss = 1.7423\n",
            "step 43310: train loss = 1.8226, val loss = 1.8995\n",
            "step 43320: train loss = 1.9397, val loss = 1.8157\n",
            "step 43330: train loss = 1.8523, val loss = 1.9075\n",
            "step 43340: train loss = 1.8756, val loss = 1.7477\n",
            "step 43350: train loss = 1.8081, val loss = 1.9339\n",
            "step 43360: train loss = 1.6956, val loss = 1.7827\n",
            "step 43370: train loss = 1.8879, val loss = 1.9853\n",
            "step 43380: train loss = 1.8709, val loss = 1.8511\n",
            "step 43390: train loss = 1.9935, val loss = 1.7563\n",
            "step 43400: train loss = 1.7628, val loss = 1.8349\n",
            "step 43410: train loss = 1.9605, val loss = 1.9256\n",
            "step 43420: train loss = 1.7348, val loss = 1.8791\n",
            "step 43430: train loss = 1.9022, val loss = 1.9580\n",
            "step 43440: train loss = 1.8266, val loss = 1.8017\n",
            "step 43450: train loss = 1.9518, val loss = 1.7518\n",
            "step 43460: train loss = 2.0139, val loss = 1.9372\n",
            "step 43470: train loss = 1.9173, val loss = 1.7932\n",
            "step 43480: train loss = 1.7491, val loss = 1.8108\n",
            "step 43490: train loss = 1.8270, val loss = 1.8530\n",
            "step 43500: train loss = 1.9478, val loss = 1.9494\n",
            "step 43510: train loss = 1.8483, val loss = 1.9479\n",
            "step 43520: train loss = 1.9380, val loss = 1.8970\n",
            "step 43530: train loss = 2.0026, val loss = 1.9060\n",
            "step 43540: train loss = 1.6504, val loss = 1.9908\n",
            "step 43550: train loss = 2.0569, val loss = 2.0223\n",
            "step 43560: train loss = 1.8059, val loss = 1.8703\n",
            "step 43570: train loss = 1.9083, val loss = 1.8977\n",
            "step 43580: train loss = 1.8490, val loss = 1.9103\n",
            "step 43590: train loss = 1.9232, val loss = 1.9124\n",
            "step 43600: train loss = 1.8209, val loss = 1.8388\n",
            "step 43610: train loss = 2.0633, val loss = 1.9236\n",
            "step 43620: train loss = 1.8293, val loss = 1.9563\n",
            "step 43630: train loss = 1.8303, val loss = 1.8773\n",
            "step 43640: train loss = 1.8960, val loss = 1.8781\n",
            "step 43650: train loss = 2.1242, val loss = 2.0644\n",
            "step 43660: train loss = 1.7998, val loss = 1.8773\n",
            "step 43670: train loss = 1.9246, val loss = 1.9753\n",
            "step 43680: train loss = 1.8649, val loss = 1.8153\n",
            "step 43690: train loss = 1.8974, val loss = 1.8279\n",
            "step 43700: train loss = 1.7645, val loss = 1.9341\n",
            "step 43710: train loss = 1.8126, val loss = 2.0094\n",
            "step 43720: train loss = 1.9720, val loss = 1.7573\n",
            "step 43730: train loss = 1.8353, val loss = 1.7947\n",
            "step 43740: train loss = 1.8811, val loss = 1.8925\n",
            "step 43750: train loss = 1.8131, val loss = 2.0113\n",
            "step 43760: train loss = 1.8429, val loss = 1.8771\n",
            "step 43770: train loss = 1.9356, val loss = 1.8589\n",
            "step 43780: train loss = 1.8040, val loss = 1.9182\n",
            "step 43790: train loss = 1.9457, val loss = 1.7570\n",
            "step 43800: train loss = 1.7171, val loss = 1.9354\n",
            "step 43810: train loss = 2.0073, val loss = 1.9005\n",
            "step 43820: train loss = 1.7925, val loss = 1.9779\n",
            "step 43830: train loss = 1.8548, val loss = 1.8090\n",
            "step 43840: train loss = 1.9451, val loss = 1.8415\n",
            "step 43850: train loss = 1.9321, val loss = 1.7812\n",
            "step 43860: train loss = 1.7566, val loss = 1.8252\n",
            "step 43870: train loss = 1.9241, val loss = 1.9158\n",
            "step 43880: train loss = 1.8279, val loss = 1.9434\n",
            "step 43890: train loss = 1.9962, val loss = 1.9690\n",
            "step 43900: train loss = 1.9336, val loss = 1.8967\n",
            "step 43910: train loss = 1.8783, val loss = 1.8548\n",
            "step 43920: train loss = 1.8181, val loss = 1.9540\n",
            "step 43930: train loss = 1.8706, val loss = 1.8458\n",
            "step 43940: train loss = 1.9121, val loss = 1.8028\n",
            "step 43950: train loss = 1.6803, val loss = 2.0325\n",
            "step 43960: train loss = 1.8676, val loss = 1.8437\n",
            "step 43970: train loss = 1.8831, val loss = 1.8231\n",
            "step 43980: train loss = 1.7198, val loss = 1.8227\n",
            "step 43990: train loss = 1.7196, val loss = 2.0610\n",
            "step 44000: train loss = 1.7921, val loss = 1.8430\n",
            "step 44010: train loss = 1.8149, val loss = 1.8573\n",
            "step 44020: train loss = 1.8204, val loss = 1.9544\n",
            "step 44030: train loss = 1.8603, val loss = 1.8542\n",
            "step 44040: train loss = 1.9737, val loss = 1.8009\n",
            "step 44050: train loss = 1.8682, val loss = 1.7428\n",
            "step 44060: train loss = 1.7933, val loss = 1.9637\n",
            "step 44070: train loss = 1.7899, val loss = 1.9626\n",
            "step 44080: train loss = 1.7863, val loss = 1.7470\n",
            "step 44090: train loss = 1.7795, val loss = 1.8966\n",
            "step 44100: train loss = 1.9125, val loss = 1.8579\n",
            "step 44110: train loss = 1.9069, val loss = 1.8495\n",
            "step 44120: train loss = 1.9701, val loss = 1.9128\n",
            "step 44130: train loss = 1.7294, val loss = 1.7923\n",
            "step 44140: train loss = 1.7377, val loss = 1.8638\n",
            "step 44150: train loss = 1.7213, val loss = 1.9465\n",
            "step 44160: train loss = 1.7991, val loss = 2.0015\n",
            "step 44170: train loss = 1.7156, val loss = 1.8473\n",
            "step 44180: train loss = 1.8293, val loss = 1.9748\n",
            "step 44190: train loss = 1.8788, val loss = 1.9442\n",
            "step 44200: train loss = 1.9286, val loss = 1.8678\n",
            "step 44210: train loss = 1.8626, val loss = 1.9250\n",
            "step 44220: train loss = 1.9954, val loss = 1.9317\n",
            "step 44230: train loss = 1.9245, val loss = 1.9454\n",
            "step 44240: train loss = 1.7937, val loss = 1.8543\n",
            "step 44250: train loss = 1.8654, val loss = 1.9219\n",
            "step 44260: train loss = 1.8127, val loss = 1.8622\n",
            "step 44270: train loss = 2.0938, val loss = 1.7606\n",
            "step 44280: train loss = 1.8001, val loss = 1.8797\n",
            "step 44290: train loss = 1.9206, val loss = 1.7189\n",
            "step 44300: train loss = 1.8886, val loss = 1.8146\n",
            "step 44310: train loss = 1.8789, val loss = 1.8231\n",
            "step 44320: train loss = 1.9931, val loss = 1.9704\n",
            "step 44330: train loss = 1.9494, val loss = 1.8837\n",
            "step 44340: train loss = 1.7451, val loss = 1.8834\n",
            "step 44350: train loss = 1.6496, val loss = 2.0318\n",
            "step 44360: train loss = 1.8018, val loss = 1.8609\n",
            "step 44370: train loss = 1.8850, val loss = 1.7517\n",
            "step 44380: train loss = 1.8344, val loss = 1.8962\n",
            "step 44390: train loss = 1.9774, val loss = 1.8219\n",
            "step 44400: train loss = 1.8923, val loss = 1.8551\n",
            "step 44410: train loss = 1.8029, val loss = 1.8834\n",
            "step 44420: train loss = 1.8619, val loss = 1.7642\n",
            "step 44430: train loss = 1.9192, val loss = 1.8855\n",
            "step 44440: train loss = 1.8235, val loss = 1.7698\n",
            "step 44450: train loss = 1.8225, val loss = 1.8031\n",
            "step 44460: train loss = 1.8070, val loss = 1.7252\n",
            "step 44470: train loss = 1.8789, val loss = 1.8436\n",
            "step 44480: train loss = 1.9087, val loss = 1.9018\n",
            "step 44490: train loss = 1.8883, val loss = 1.8769\n",
            "step 44500: train loss = 1.8395, val loss = 1.8214\n",
            "step 44510: train loss = 1.7382, val loss = 1.9385\n",
            "step 44520: train loss = 1.8052, val loss = 1.8350\n",
            "step 44530: train loss = 1.8624, val loss = 1.8087\n",
            "step 44540: train loss = 1.9548, val loss = 1.8901\n",
            "step 44550: train loss = 1.7579, val loss = 1.8245\n",
            "step 44560: train loss = 1.9135, val loss = 1.9155\n",
            "step 44570: train loss = 2.0094, val loss = 1.8541\n",
            "step 44580: train loss = 1.8071, val loss = 1.9428\n",
            "step 44590: train loss = 1.9304, val loss = 1.9343\n",
            "step 44600: train loss = 1.6206, val loss = 1.9181\n",
            "step 44610: train loss = 1.7988, val loss = 1.8716\n",
            "step 44620: train loss = 1.9282, val loss = 1.8881\n",
            "step 44630: train loss = 1.8859, val loss = 1.9783\n",
            "step 44640: train loss = 1.8316, val loss = 1.8760\n",
            "step 44650: train loss = 1.9214, val loss = 1.9481\n",
            "step 44660: train loss = 1.7397, val loss = 1.8512\n",
            "step 44670: train loss = 1.9424, val loss = 1.8198\n",
            "step 44680: train loss = 1.7451, val loss = 1.8298\n",
            "step 44690: train loss = 1.8055, val loss = 1.8479\n",
            "step 44700: train loss = 1.7398, val loss = 1.8823\n",
            "step 44710: train loss = 1.8946, val loss = 1.7449\n",
            "step 44720: train loss = 1.8054, val loss = 1.8604\n",
            "step 44730: train loss = 1.9558, val loss = 1.9609\n",
            "step 44740: train loss = 1.8576, val loss = 1.8478\n",
            "step 44750: train loss = 1.8690, val loss = 1.6918\n",
            "step 44760: train loss = 1.9518, val loss = 1.7750\n",
            "step 44770: train loss = 1.7003, val loss = 1.7896\n",
            "step 44780: train loss = 1.9801, val loss = 1.8928\n",
            "step 44790: train loss = 1.8279, val loss = 1.8065\n",
            "step 44800: train loss = 1.6786, val loss = 1.9667\n",
            "step 44810: train loss = 1.8683, val loss = 1.8294\n",
            "step 44820: train loss = 1.8192, val loss = 1.9712\n",
            "step 44830: train loss = 1.9394, val loss = 1.9768\n",
            "step 44840: train loss = 1.8455, val loss = 1.9402\n",
            "step 44850: train loss = 1.7733, val loss = 1.9047\n",
            "step 44860: train loss = 1.8602, val loss = 1.8781\n",
            "step 44870: train loss = 1.7601, val loss = 1.8771\n",
            "step 44880: train loss = 1.8536, val loss = 1.8404\n",
            "step 44890: train loss = 1.7273, val loss = 1.8061\n",
            "step 44900: train loss = 1.6631, val loss = 1.8610\n",
            "step 44910: train loss = 1.7761, val loss = 1.7838\n",
            "step 44920: train loss = 1.7235, val loss = 1.8557\n",
            "step 44930: train loss = 1.8671, val loss = 1.7878\n",
            "step 44940: train loss = 1.9810, val loss = 1.9655\n",
            "step 44950: train loss = 1.7776, val loss = 1.8816\n",
            "step 44960: train loss = 1.7677, val loss = 1.8646\n",
            "step 44970: train loss = 1.8227, val loss = 1.8533\n",
            "step 44980: train loss = 1.7362, val loss = 1.7695\n",
            "step 44990: train loss = 1.8126, val loss = 1.9100\n",
            "step 45000: train loss = 1.6624, val loss = 1.8956\n",
            "step 45010: train loss = 1.8485, val loss = 1.8662\n",
            "step 45020: train loss = 1.8422, val loss = 1.7862\n",
            "step 45030: train loss = 1.8351, val loss = 1.8855\n",
            "step 45040: train loss = 1.7748, val loss = 1.7795\n",
            "step 45050: train loss = 1.7283, val loss = 1.9874\n",
            "step 45060: train loss = 1.9629, val loss = 1.7804\n",
            "step 45070: train loss = 1.7357, val loss = 1.7831\n",
            "step 45080: train loss = 1.8570, val loss = 1.8543\n",
            "step 45090: train loss = 1.9987, val loss = 1.8487\n",
            "step 45100: train loss = 1.9423, val loss = 1.8308\n",
            "step 45110: train loss = 1.8097, val loss = 1.9181\n",
            "step 45120: train loss = 1.8466, val loss = 1.9319\n",
            "step 45130: train loss = 1.8339, val loss = 1.7598\n",
            "step 45140: train loss = 1.9887, val loss = 1.7885\n",
            "step 45150: train loss = 1.7790, val loss = 1.8980\n",
            "step 45160: train loss = 1.8026, val loss = 1.7820\n",
            "step 45170: train loss = 1.8496, val loss = 1.9301\n",
            "step 45180: train loss = 1.8530, val loss = 1.7755\n",
            "step 45190: train loss = 1.9555, val loss = 1.8704\n",
            "step 45200: train loss = 2.0621, val loss = 1.7764\n",
            "step 45210: train loss = 1.9954, val loss = 1.7386\n",
            "step 45220: train loss = 1.7082, val loss = 2.0048\n",
            "step 45230: train loss = 1.7331, val loss = 1.9214\n",
            "step 45240: train loss = 1.8119, val loss = 1.8178\n",
            "step 45250: train loss = 1.6796, val loss = 1.8357\n",
            "step 45260: train loss = 1.9196, val loss = 1.9398\n",
            "step 45270: train loss = 1.7282, val loss = 1.8440\n",
            "step 45280: train loss = 1.7515, val loss = 1.8471\n",
            "step 45290: train loss = 1.8809, val loss = 1.9449\n",
            "step 45300: train loss = 1.7545, val loss = 1.8450\n",
            "step 45310: train loss = 1.8563, val loss = 1.8138\n",
            "step 45320: train loss = 1.8999, val loss = 1.8940\n",
            "step 45330: train loss = 1.8474, val loss = 1.7473\n",
            "step 45340: train loss = 1.9864, val loss = 1.8378\n",
            "step 45350: train loss = 1.7977, val loss = 1.8784\n",
            "step 45360: train loss = 1.7461, val loss = 1.8440\n",
            "step 45370: train loss = 1.7617, val loss = 1.8648\n",
            "step 45380: train loss = 1.8118, val loss = 1.8847\n",
            "step 45390: train loss = 1.9953, val loss = 1.8125\n",
            "step 45400: train loss = 1.7946, val loss = 1.7161\n",
            "step 45410: train loss = 1.8497, val loss = 2.0486\n",
            "step 45420: train loss = 1.8219, val loss = 1.9700\n",
            "step 45430: train loss = 1.9647, val loss = 1.8675\n",
            "step 45440: train loss = 1.8548, val loss = 1.7795\n",
            "step 45450: train loss = 1.9798, val loss = 1.8580\n",
            "step 45460: train loss = 1.8210, val loss = 1.9162\n",
            "step 45470: train loss = 1.7313, val loss = 1.8438\n",
            "step 45480: train loss = 1.7367, val loss = 1.9121\n",
            "step 45490: train loss = 1.7560, val loss = 1.9189\n",
            "step 45500: train loss = 1.9271, val loss = 1.8318\n",
            "step 45510: train loss = 1.9230, val loss = 1.8326\n",
            "step 45520: train loss = 1.9252, val loss = 1.8826\n",
            "step 45530: train loss = 1.8516, val loss = 1.8552\n",
            "step 45540: train loss = 1.9400, val loss = 1.7638\n",
            "step 45550: train loss = 1.7707, val loss = 1.9838\n",
            "step 45560: train loss = 1.9005, val loss = 1.9588\n",
            "step 45570: train loss = 1.6427, val loss = 1.8801\n",
            "step 45580: train loss = 1.8908, val loss = 1.8880\n",
            "step 45590: train loss = 1.8840, val loss = 1.8558\n",
            "step 45600: train loss = 1.8099, val loss = 1.7230\n",
            "step 45610: train loss = 1.8923, val loss = 2.0636\n",
            "step 45620: train loss = 1.7266, val loss = 1.9763\n",
            "step 45630: train loss = 1.7674, val loss = 1.8210\n",
            "step 45640: train loss = 1.8294, val loss = 1.7987\n",
            "step 45650: train loss = 1.9489, val loss = 1.9095\n",
            "step 45660: train loss = 1.9357, val loss = 1.9726\n",
            "step 45670: train loss = 1.8449, val loss = 1.8711\n",
            "step 45680: train loss = 1.7899, val loss = 2.0635\n",
            "step 45690: train loss = 1.8708, val loss = 1.8998\n",
            "step 45700: train loss = 1.9959, val loss = 1.7188\n",
            "step 45710: train loss = 1.9618, val loss = 1.8997\n",
            "step 45720: train loss = 1.7990, val loss = 1.9309\n",
            "step 45730: train loss = 1.8060, val loss = 1.7545\n",
            "step 45740: train loss = 1.9418, val loss = 1.9404\n",
            "step 45750: train loss = 1.9373, val loss = 1.8218\n",
            "step 45760: train loss = 1.7910, val loss = 1.7163\n",
            "step 45770: train loss = 1.9032, val loss = 1.7316\n",
            "step 45780: train loss = 1.7402, val loss = 1.9603\n",
            "step 45790: train loss = 1.8803, val loss = 1.7914\n",
            "step 45800: train loss = 1.7832, val loss = 1.8008\n",
            "step 45810: train loss = 1.8355, val loss = 1.8577\n",
            "step 45820: train loss = 1.7604, val loss = 1.8835\n",
            "step 45830: train loss = 1.8206, val loss = 1.7075\n",
            "step 45840: train loss = 1.8420, val loss = 1.8261\n",
            "step 45850: train loss = 1.9318, val loss = 1.7993\n",
            "step 45860: train loss = 1.9035, val loss = 1.9127\n",
            "step 45870: train loss = 1.9235, val loss = 1.7701\n",
            "step 45880: train loss = 1.8129, val loss = 1.9489\n",
            "step 45890: train loss = 1.8920, val loss = 1.9325\n",
            "step 45900: train loss = 1.7560, val loss = 1.9155\n",
            "step 45910: train loss = 1.8813, val loss = 1.8988\n",
            "step 45920: train loss = 1.9132, val loss = 1.8815\n",
            "step 45930: train loss = 1.9564, val loss = 1.8424\n",
            "step 45940: train loss = 1.9498, val loss = 1.9811\n",
            "step 45950: train loss = 1.9928, val loss = 1.7179\n",
            "step 45960: train loss = 1.8338, val loss = 1.9082\n",
            "step 45970: train loss = 1.8403, val loss = 1.8394\n",
            "step 45980: train loss = 1.8721, val loss = 1.8442\n",
            "step 45990: train loss = 1.7221, val loss = 1.7527\n",
            "step 46000: train loss = 1.8981, val loss = 1.8362\n",
            "step 46010: train loss = 1.7416, val loss = 1.8070\n",
            "step 46020: train loss = 1.8603, val loss = 1.7507\n",
            "step 46030: train loss = 1.6946, val loss = 1.9812\n",
            "step 46040: train loss = 1.7252, val loss = 1.9746\n",
            "step 46050: train loss = 1.8675, val loss = 1.8199\n",
            "step 46060: train loss = 1.8378, val loss = 1.9095\n",
            "step 46070: train loss = 1.8817, val loss = 1.9187\n",
            "step 46080: train loss = 1.9440, val loss = 1.9025\n",
            "step 46090: train loss = 1.8121, val loss = 1.8236\n",
            "step 46100: train loss = 1.9884, val loss = 1.7946\n",
            "step 46110: train loss = 1.9139, val loss = 1.8774\n",
            "step 46120: train loss = 1.8591, val loss = 1.9072\n",
            "step 46130: train loss = 1.8450, val loss = 1.7702\n",
            "step 46140: train loss = 1.7642, val loss = 1.8646\n",
            "step 46150: train loss = 1.8916, val loss = 1.9779\n",
            "step 46160: train loss = 1.8964, val loss = 1.9830\n",
            "step 46170: train loss = 1.8748, val loss = 1.7655\n",
            "step 46180: train loss = 1.9437, val loss = 1.8341\n",
            "step 46190: train loss = 1.7694, val loss = 1.8588\n",
            "step 46200: train loss = 1.8634, val loss = 1.8183\n",
            "step 46210: train loss = 1.8455, val loss = 1.7675\n",
            "step 46220: train loss = 1.9871, val loss = 1.8571\n",
            "step 46230: train loss = 1.7518, val loss = 1.8282\n",
            "step 46240: train loss = 2.0324, val loss = 1.8523\n",
            "step 46250: train loss = 1.9519, val loss = 1.7616\n",
            "step 46260: train loss = 1.8350, val loss = 1.6590\n",
            "step 46270: train loss = 1.8098, val loss = 1.7090\n",
            "step 46280: train loss = 1.7574, val loss = 1.7675\n",
            "step 46290: train loss = 1.7528, val loss = 1.9600\n",
            "step 46300: train loss = 1.8360, val loss = 1.8906\n",
            "step 46310: train loss = 1.8199, val loss = 1.8801\n",
            "step 46320: train loss = 1.9901, val loss = 1.8830\n",
            "step 46330: train loss = 1.9025, val loss = 1.9131\n",
            "step 46340: train loss = 1.8538, val loss = 1.8357\n",
            "step 46350: train loss = 1.7301, val loss = 1.8057\n",
            "step 46360: train loss = 1.8475, val loss = 1.8308\n",
            "step 46370: train loss = 1.7458, val loss = 1.8540\n",
            "step 46380: train loss = 1.8039, val loss = 1.8724\n",
            "step 46390: train loss = 1.8399, val loss = 1.8019\n",
            "step 46400: train loss = 1.8793, val loss = 1.8627\n",
            "step 46410: train loss = 1.8877, val loss = 1.9174\n",
            "step 46420: train loss = 1.9236, val loss = 1.8620\n",
            "step 46430: train loss = 1.9014, val loss = 1.7405\n",
            "step 46440: train loss = 1.8837, val loss = 1.6874\n",
            "step 46450: train loss = 1.9854, val loss = 1.8559\n",
            "step 46460: train loss = 1.9399, val loss = 1.8446\n",
            "step 46470: train loss = 1.7908, val loss = 1.7702\n",
            "step 46480: train loss = 1.7121, val loss = 1.8731\n",
            "step 46490: train loss = 1.8670, val loss = 1.8984\n",
            "step 46500: train loss = 1.7966, val loss = 1.9111\n",
            "step 46510: train loss = 1.8222, val loss = 1.8537\n",
            "step 46520: train loss = 1.8492, val loss = 1.8746\n",
            "step 46530: train loss = 1.6600, val loss = 1.7565\n",
            "step 46540: train loss = 1.9463, val loss = 1.8598\n",
            "step 46550: train loss = 1.8748, val loss = 1.7724\n",
            "step 46560: train loss = 1.7771, val loss = 1.9352\n",
            "step 46570: train loss = 1.8976, val loss = 1.8705\n",
            "step 46580: train loss = 2.0100, val loss = 2.1641\n",
            "step 46590: train loss = 1.8705, val loss = 1.7412\n",
            "step 46600: train loss = 1.7373, val loss = 1.8237\n",
            "step 46610: train loss = 1.7739, val loss = 1.9140\n",
            "step 46620: train loss = 1.7109, val loss = 1.7666\n",
            "step 46630: train loss = 1.8546, val loss = 1.9141\n",
            "step 46640: train loss = 1.8601, val loss = 1.9573\n",
            "step 46650: train loss = 1.8708, val loss = 1.7957\n",
            "step 46660: train loss = 1.7874, val loss = 1.7897\n",
            "step 46670: train loss = 1.8479, val loss = 1.8072\n",
            "step 46680: train loss = 1.7978, val loss = 1.9661\n",
            "step 46690: train loss = 1.8416, val loss = 1.8742\n",
            "step 46700: train loss = 1.8974, val loss = 1.9803\n",
            "step 46710: train loss = 1.9033, val loss = 1.8526\n",
            "step 46720: train loss = 1.9105, val loss = 1.9632\n",
            "step 46730: train loss = 1.9142, val loss = 1.7220\n",
            "step 46740: train loss = 1.8069, val loss = 1.9465\n",
            "step 46750: train loss = 2.0260, val loss = 1.8648\n",
            "step 46760: train loss = 1.7281, val loss = 1.8370\n",
            "step 46770: train loss = 1.7479, val loss = 1.9047\n",
            "step 46780: train loss = 1.8073, val loss = 1.8245\n",
            "step 46790: train loss = 1.8175, val loss = 1.9852\n",
            "step 46800: train loss = 1.7415, val loss = 2.0758\n",
            "step 46810: train loss = 1.7831, val loss = 1.9334\n",
            "step 46820: train loss = 1.7453, val loss = 1.8979\n",
            "step 46830: train loss = 1.8183, val loss = 1.7637\n",
            "step 46840: train loss = 1.7418, val loss = 1.8375\n",
            "step 46850: train loss = 1.8689, val loss = 1.9407\n",
            "step 46860: train loss = 1.8639, val loss = 1.8101\n",
            "step 46870: train loss = 1.8695, val loss = 1.8988\n",
            "step 46880: train loss = 1.8523, val loss = 1.8526\n",
            "step 46890: train loss = 1.7589, val loss = 1.8841\n",
            "step 46900: train loss = 1.7764, val loss = 1.7429\n",
            "step 46910: train loss = 1.9427, val loss = 1.8732\n",
            "step 46920: train loss = 1.8195, val loss = 1.8505\n",
            "step 46930: train loss = 1.9140, val loss = 1.9163\n",
            "step 46940: train loss = 1.9522, val loss = 1.7928\n",
            "step 46950: train loss = 1.8022, val loss = 1.8388\n",
            "step 46960: train loss = 1.8467, val loss = 1.7380\n",
            "step 46970: train loss = 1.8510, val loss = 1.9132\n",
            "step 46980: train loss = 1.8712, val loss = 1.9069\n",
            "step 46990: train loss = 1.7351, val loss = 1.6888\n",
            "step 47000: train loss = 1.8799, val loss = 2.0120\n",
            "step 47010: train loss = 1.9134, val loss = 1.8000\n",
            "step 47020: train loss = 1.8887, val loss = 1.8410\n",
            "step 47030: train loss = 1.8658, val loss = 1.7467\n",
            "step 47040: train loss = 1.7812, val loss = 1.9166\n",
            "step 47050: train loss = 1.8215, val loss = 1.7680\n",
            "step 47060: train loss = 1.8247, val loss = 1.7572\n",
            "step 47070: train loss = 1.8314, val loss = 1.7398\n",
            "step 47080: train loss = 1.8096, val loss = 1.7832\n",
            "step 47090: train loss = 1.9812, val loss = 1.8936\n",
            "step 47100: train loss = 1.7805, val loss = 1.7849\n",
            "step 47110: train loss = 1.7376, val loss = 1.7848\n",
            "step 47120: train loss = 1.8334, val loss = 1.8543\n",
            "step 47130: train loss = 2.0484, val loss = 1.7428\n",
            "step 47140: train loss = 1.8156, val loss = 1.8620\n",
            "step 47150: train loss = 1.9097, val loss = 1.9736\n",
            "step 47160: train loss = 1.6974, val loss = 1.8529\n",
            "step 47170: train loss = 1.9117, val loss = 1.8170\n",
            "step 47180: train loss = 1.9179, val loss = 1.8727\n",
            "step 47190: train loss = 1.8005, val loss = 1.7955\n",
            "step 47200: train loss = 1.9674, val loss = 1.7594\n",
            "step 47210: train loss = 1.8346, val loss = 1.7839\n",
            "step 47220: train loss = 1.8529, val loss = 1.7957\n",
            "step 47230: train loss = 1.6351, val loss = 1.7978\n",
            "step 47240: train loss = 1.7077, val loss = 1.8934\n",
            "step 47250: train loss = 1.8715, val loss = 1.9326\n",
            "step 47260: train loss = 1.9713, val loss = 1.8712\n",
            "step 47270: train loss = 1.8504, val loss = 1.9698\n",
            "step 47280: train loss = 1.9556, val loss = 1.8481\n",
            "step 47290: train loss = 1.9796, val loss = 1.8688\n",
            "step 47300: train loss = 1.9328, val loss = 1.8522\n",
            "step 47310: train loss = 1.8487, val loss = 1.8990\n",
            "step 47320: train loss = 1.8944, val loss = 2.0200\n",
            "step 47330: train loss = 1.9074, val loss = 1.8931\n",
            "step 47340: train loss = 1.9328, val loss = 1.8730\n",
            "step 47350: train loss = 1.9685, val loss = 1.7938\n",
            "step 47360: train loss = 1.7384, val loss = 1.7397\n",
            "step 47370: train loss = 1.8112, val loss = 1.8483\n",
            "step 47380: train loss = 1.8653, val loss = 1.8942\n",
            "step 47390: train loss = 1.8174, val loss = 1.8296\n",
            "step 47400: train loss = 1.9141, val loss = 1.8931\n",
            "step 47410: train loss = 1.8928, val loss = 1.8654\n",
            "step 47420: train loss = 1.8317, val loss = 1.9689\n",
            "step 47430: train loss = 1.8392, val loss = 1.9263\n",
            "step 47440: train loss = 1.9498, val loss = 1.9639\n",
            "step 47450: train loss = 1.7790, val loss = 1.9880\n",
            "step 47460: train loss = 1.7384, val loss = 1.7856\n",
            "step 47470: train loss = 1.9272, val loss = 1.8248\n",
            "step 47480: train loss = 1.7800, val loss = 1.9037\n",
            "step 47490: train loss = 1.8003, val loss = 1.8934\n",
            "step 47500: train loss = 1.9541, val loss = 1.8125\n",
            "step 47510: train loss = 1.9258, val loss = 1.8284\n",
            "step 47520: train loss = 1.8403, val loss = 1.8075\n",
            "step 47530: train loss = 1.7561, val loss = 2.0218\n",
            "step 47540: train loss = 1.8213, val loss = 1.7610\n",
            "step 47550: train loss = 1.7743, val loss = 1.9106\n",
            "step 47560: train loss = 1.8728, val loss = 1.6996\n",
            "step 47570: train loss = 1.7112, val loss = 1.8688\n",
            "step 47580: train loss = 1.8203, val loss = 1.9843\n",
            "step 47590: train loss = 1.7826, val loss = 1.8646\n",
            "step 47600: train loss = 1.7985, val loss = 1.9372\n",
            "step 47610: train loss = 1.7665, val loss = 1.7932\n",
            "step 47620: train loss = 1.8775, val loss = 1.8291\n",
            "step 47630: train loss = 1.7517, val loss = 1.8163\n",
            "step 47640: train loss = 1.9259, val loss = 1.7604\n",
            "step 47650: train loss = 1.8050, val loss = 1.8926\n",
            "step 47660: train loss = 1.8821, val loss = 1.9223\n",
            "step 47670: train loss = 1.8482, val loss = 2.0092\n",
            "step 47680: train loss = 1.8289, val loss = 1.8102\n",
            "step 47690: train loss = 1.7683, val loss = 1.8837\n",
            "step 47700: train loss = 1.8710, val loss = 1.9923\n",
            "step 47710: train loss = 1.9151, val loss = 1.8471\n",
            "step 47720: train loss = 1.8706, val loss = 1.9441\n",
            "step 47730: train loss = 1.6922, val loss = 1.8927\n",
            "step 47740: train loss = 1.8021, val loss = 1.9539\n",
            "step 47750: train loss = 1.7675, val loss = 1.9144\n",
            "step 47760: train loss = 1.7614, val loss = 1.7334\n",
            "step 47770: train loss = 1.9480, val loss = 1.7453\n",
            "step 47780: train loss = 1.7964, val loss = 1.8332\n",
            "step 47790: train loss = 1.7436, val loss = 2.0744\n",
            "step 47800: train loss = 1.8016, val loss = 1.8209\n",
            "step 47810: train loss = 1.7400, val loss = 1.7183\n",
            "step 47820: train loss = 1.8338, val loss = 1.8274\n",
            "step 47830: train loss = 2.0021, val loss = 1.8320\n",
            "step 47840: train loss = 1.7267, val loss = 1.8491\n",
            "step 47850: train loss = 1.7893, val loss = 1.7610\n",
            "step 47860: train loss = 1.8428, val loss = 1.8179\n",
            "step 47870: train loss = 1.7224, val loss = 1.8202\n",
            "step 47880: train loss = 1.9066, val loss = 1.9195\n",
            "step 47890: train loss = 1.7928, val loss = 1.8663\n",
            "step 47900: train loss = 1.8598, val loss = 1.7826\n",
            "step 47910: train loss = 1.8993, val loss = 1.8141\n",
            "step 47920: train loss = 1.7722, val loss = 1.8703\n",
            "step 47930: train loss = 1.9567, val loss = 1.8682\n",
            "step 47940: train loss = 1.7657, val loss = 1.8830\n",
            "step 47950: train loss = 1.7409, val loss = 1.9436\n",
            "step 47960: train loss = 1.8470, val loss = 1.7233\n",
            "step 47970: train loss = 1.8276, val loss = 1.9619\n",
            "step 47980: train loss = 1.8197, val loss = 1.7749\n",
            "step 47990: train loss = 1.8946, val loss = 1.8996\n",
            "step 48000: train loss = 1.7531, val loss = 1.8546\n",
            "step 48010: train loss = 1.8199, val loss = 1.9010\n",
            "step 48020: train loss = 1.8355, val loss = 1.8898\n",
            "step 48030: train loss = 1.8952, val loss = 1.8245\n",
            "step 48040: train loss = 1.7899, val loss = 1.8035\n",
            "step 48050: train loss = 1.9507, val loss = 1.9669\n",
            "step 48060: train loss = 1.7892, val loss = 1.9638\n",
            "step 48070: train loss = 1.7964, val loss = 1.7122\n",
            "step 48080: train loss = 1.8232, val loss = 1.9187\n",
            "step 48090: train loss = 1.8435, val loss = 1.9275\n",
            "step 48100: train loss = 1.7627, val loss = 1.8109\n",
            "step 48110: train loss = 1.8065, val loss = 1.8435\n",
            "step 48120: train loss = 1.7960, val loss = 1.9959\n",
            "step 48130: train loss = 1.8169, val loss = 1.8968\n",
            "step 48140: train loss = 1.7781, val loss = 1.8326\n",
            "step 48150: train loss = 1.9026, val loss = 1.8265\n",
            "step 48160: train loss = 1.9777, val loss = 1.8573\n",
            "step 48170: train loss = 1.7985, val loss = 1.8217\n",
            "step 48180: train loss = 1.9492, val loss = 1.8696\n",
            "step 48190: train loss = 1.8263, val loss = 1.9682\n",
            "step 48200: train loss = 1.8920, val loss = 1.9198\n",
            "step 48210: train loss = 1.8608, val loss = 1.8187\n",
            "step 48220: train loss = 1.9729, val loss = 1.8769\n",
            "step 48230: train loss = 1.7632, val loss = 1.8384\n",
            "step 48240: train loss = 1.8254, val loss = 1.8168\n",
            "step 48250: train loss = 1.7061, val loss = 1.8508\n",
            "step 48260: train loss = 1.9224, val loss = 2.0438\n",
            "step 48270: train loss = 1.7685, val loss = 1.8495\n",
            "step 48280: train loss = 1.7609, val loss = 1.8705\n",
            "step 48290: train loss = 1.9185, val loss = 1.9509\n",
            "step 48300: train loss = 1.6898, val loss = 1.7522\n",
            "step 48310: train loss = 1.9289, val loss = 1.8582\n",
            "step 48320: train loss = 1.9882, val loss = 1.9443\n",
            "step 48330: train loss = 1.8028, val loss = 1.7137\n",
            "step 48340: train loss = 1.9641, val loss = 1.7752\n",
            "step 48350: train loss = 1.6677, val loss = 2.0175\n",
            "step 48360: train loss = 1.9102, val loss = 1.7217\n",
            "step 48370: train loss = 1.9801, val loss = 1.8709\n",
            "step 48380: train loss = 1.8554, val loss = 2.1307\n",
            "step 48390: train loss = 1.9392, val loss = 1.9427\n",
            "step 48400: train loss = 1.9830, val loss = 1.7756\n",
            "step 48410: train loss = 1.7038, val loss = 1.9869\n",
            "step 48420: train loss = 1.7939, val loss = 1.8759\n",
            "step 48430: train loss = 1.8085, val loss = 1.9468\n",
            "step 48440: train loss = 1.8172, val loss = 1.8214\n",
            "step 48450: train loss = 1.8478, val loss = 1.8340\n",
            "step 48460: train loss = 1.8176, val loss = 1.9719\n",
            "step 48470: train loss = 1.8434, val loss = 1.7984\n",
            "step 48480: train loss = 1.8136, val loss = 1.8956\n",
            "step 48490: train loss = 1.9342, val loss = 1.7476\n",
            "step 48500: train loss = 2.0350, val loss = 1.7781\n",
            "step 48510: train loss = 1.8208, val loss = 1.9036\n",
            "step 48520: train loss = 1.8091, val loss = 2.0526\n",
            "step 48530: train loss = 1.9927, val loss = 1.8259\n",
            "step 48540: train loss = 1.8104, val loss = 1.9120\n",
            "step 48550: train loss = 1.9219, val loss = 1.9072\n",
            "step 48560: train loss = 1.9767, val loss = 1.9867\n",
            "step 48570: train loss = 1.7792, val loss = 1.8778\n",
            "step 48580: train loss = 1.9004, val loss = 1.7828\n",
            "step 48590: train loss = 1.8876, val loss = 1.7989\n",
            "step 48600: train loss = 1.8513, val loss = 1.9820\n",
            "step 48610: train loss = 1.7811, val loss = 1.8124\n",
            "step 48620: train loss = 1.6999, val loss = 1.8240\n",
            "step 48630: train loss = 2.0724, val loss = 1.8543\n",
            "step 48640: train loss = 1.9617, val loss = 1.7412\n",
            "step 48650: train loss = 1.8980, val loss = 1.8779\n",
            "step 48660: train loss = 1.8873, val loss = 1.8524\n",
            "step 48670: train loss = 1.9515, val loss = 2.1050\n",
            "step 48680: train loss = 1.9277, val loss = 1.8911\n",
            "step 48690: train loss = 1.8473, val loss = 2.0325\n",
            "step 48700: train loss = 1.9996, val loss = 1.9631\n",
            "step 48710: train loss = 1.8240, val loss = 1.9462\n",
            "step 48720: train loss = 1.9212, val loss = 1.8823\n",
            "step 48730: train loss = 1.8984, val loss = 1.9812\n",
            "step 48740: train loss = 1.8900, val loss = 1.9824\n",
            "step 48750: train loss = 1.9563, val loss = 1.9433\n",
            "step 48760: train loss = 1.9066, val loss = 1.9080\n",
            "step 48770: train loss = 1.8628, val loss = 1.9192\n",
            "step 48780: train loss = 1.8346, val loss = 1.9373\n",
            "step 48790: train loss = 1.8373, val loss = 1.7645\n",
            "step 48800: train loss = 1.8253, val loss = 1.9303\n",
            "step 48810: train loss = 1.9265, val loss = 1.8876\n",
            "step 48820: train loss = 1.7827, val loss = 1.8188\n",
            "step 48830: train loss = 1.8426, val loss = 1.8524\n",
            "step 48840: train loss = 1.7730, val loss = 1.7957\n",
            "step 48850: train loss = 1.7409, val loss = 1.9016\n",
            "step 48860: train loss = 1.8013, val loss = 1.7551\n",
            "step 48870: train loss = 1.8591, val loss = 1.8657\n",
            "step 48880: train loss = 1.8292, val loss = 1.7997\n",
            "step 48890: train loss = 1.7255, val loss = 1.9363\n",
            "step 48900: train loss = 1.7669, val loss = 1.7357\n",
            "step 48910: train loss = 1.8384, val loss = 1.9222\n",
            "step 48920: train loss = 1.9430, val loss = 1.8990\n",
            "step 48930: train loss = 1.9262, val loss = 1.8581\n",
            "step 48940: train loss = 1.7105, val loss = 2.0102\n",
            "step 48950: train loss = 1.9518, val loss = 1.8446\n",
            "step 48960: train loss = 1.8543, val loss = 1.7880\n",
            "step 48970: train loss = 1.8804, val loss = 1.8241\n",
            "step 48980: train loss = 1.7714, val loss = 1.8185\n",
            "step 48990: train loss = 1.8291, val loss = 1.7706\n",
            "step 49000: train loss = 1.7114, val loss = 1.8265\n",
            "step 49010: train loss = 1.9009, val loss = 1.7983\n",
            "step 49020: train loss = 1.8641, val loss = 1.8508\n",
            "step 49030: train loss = 1.8104, val loss = 1.9035\n",
            "step 49040: train loss = 1.9129, val loss = 1.9818\n",
            "step 49050: train loss = 1.6557, val loss = 1.8452\n",
            "step 49060: train loss = 1.9773, val loss = 1.9584\n",
            "step 49070: train loss = 1.8152, val loss = 1.8321\n",
            "step 49080: train loss = 1.8019, val loss = 1.9212\n",
            "step 49090: train loss = 1.8227, val loss = 1.8492\n",
            "step 49100: train loss = 1.9912, val loss = 1.9009\n",
            "step 49110: train loss = 1.9161, val loss = 1.8996\n",
            "step 49120: train loss = 1.8206, val loss = 1.8781\n",
            "step 49130: train loss = 1.9276, val loss = 1.8500\n",
            "step 49140: train loss = 1.9235, val loss = 1.8726\n",
            "step 49150: train loss = 2.1664, val loss = 1.8582\n",
            "step 49160: train loss = 1.7590, val loss = 1.9302\n",
            "step 49170: train loss = 1.8290, val loss = 1.8580\n",
            "step 49180: train loss = 1.8627, val loss = 1.8368\n",
            "step 49190: train loss = 1.7741, val loss = 1.8740\n",
            "step 49200: train loss = 1.7917, val loss = 1.8384\n",
            "step 49210: train loss = 1.9530, val loss = 2.0055\n",
            "step 49220: train loss = 1.7677, val loss = 2.1655\n",
            "step 49230: train loss = 1.8088, val loss = 1.8124\n",
            "step 49240: train loss = 1.9842, val loss = 1.9394\n",
            "step 49250: train loss = 1.8661, val loss = 1.8889\n",
            "step 49260: train loss = 1.7586, val loss = 1.7726\n",
            "step 49270: train loss = 1.8343, val loss = 1.8604\n",
            "step 49280: train loss = 1.8277, val loss = 1.8213\n",
            "step 49290: train loss = 1.8614, val loss = 1.9450\n",
            "step 49300: train loss = 1.8774, val loss = 1.7932\n",
            "step 49310: train loss = 1.8858, val loss = 1.7975\n",
            "step 49320: train loss = 1.8857, val loss = 1.8456\n",
            "step 49330: train loss = 1.7603, val loss = 1.7890\n",
            "step 49340: train loss = 1.9536, val loss = 1.9532\n",
            "step 49350: train loss = 1.8881, val loss = 1.8506\n",
            "step 49360: train loss = 1.9104, val loss = 1.7783\n",
            "step 49370: train loss = 1.8279, val loss = 1.9949\n",
            "step 49380: train loss = 1.8589, val loss = 1.8060\n",
            "step 49390: train loss = 1.7133, val loss = 1.9497\n",
            "step 49400: train loss = 1.8578, val loss = 1.8949\n",
            "step 49410: train loss = 1.9353, val loss = 1.8795\n",
            "step 49420: train loss = 1.8582, val loss = 1.8808\n",
            "step 49430: train loss = 1.9271, val loss = 1.7237\n",
            "step 49440: train loss = 1.7054, val loss = 1.8449\n",
            "step 49450: train loss = 1.8851, val loss = 1.7433\n",
            "step 49460: train loss = 1.8131, val loss = 1.8092\n",
            "step 49470: train loss = 1.7980, val loss = 1.7570\n",
            "step 49480: train loss = 1.8892, val loss = 1.6584\n",
            "step 49490: train loss = 1.8828, val loss = 1.8291\n",
            "step 49500: train loss = 1.8460, val loss = 2.0380\n",
            "step 49510: train loss = 1.7980, val loss = 1.9373\n",
            "step 49520: train loss = 2.0013, val loss = 1.8791\n",
            "step 49530: train loss = 2.0461, val loss = 2.0121\n",
            "step 49540: train loss = 1.8042, val loss = 2.1035\n",
            "step 49550: train loss = 1.8081, val loss = 1.8562\n",
            "step 49560: train loss = 1.7078, val loss = 1.7926\n",
            "step 49570: train loss = 1.9442, val loss = 1.8317\n",
            "step 49580: train loss = 1.7899, val loss = 1.8277\n",
            "step 49590: train loss = 1.7996, val loss = 1.7859\n",
            "step 49600: train loss = 1.7592, val loss = 1.8632\n",
            "step 49610: train loss = 1.9884, val loss = 1.8464\n",
            "step 49620: train loss = 1.8163, val loss = 1.8111\n",
            "step 49630: train loss = 1.9402, val loss = 1.8200\n",
            "step 49640: train loss = 1.7747, val loss = 1.8439\n",
            "step 49650: train loss = 1.7949, val loss = 1.8559\n",
            "step 49660: train loss = 1.7744, val loss = 1.8709\n",
            "step 49670: train loss = 1.8303, val loss = 1.8923\n",
            "step 49680: train loss = 1.9172, val loss = 1.8179\n",
            "step 49690: train loss = 1.7786, val loss = 1.8325\n",
            "step 49700: train loss = 1.8987, val loss = 1.9311\n",
            "step 49710: train loss = 1.7358, val loss = 1.7862\n",
            "step 49720: train loss = 1.8592, val loss = 1.8930\n",
            "step 49730: train loss = 1.7049, val loss = 2.0001\n",
            "step 49740: train loss = 1.7634, val loss = 1.6535\n",
            "step 49750: train loss = 1.7283, val loss = 1.7731\n",
            "step 49760: train loss = 1.6713, val loss = 1.8384\n",
            "step 49770: train loss = 1.9378, val loss = 2.1097\n",
            "step 49780: train loss = 1.8844, val loss = 1.6936\n",
            "step 49790: train loss = 1.8321, val loss = 1.9285\n",
            "step 49800: train loss = 1.7966, val loss = 1.8084\n",
            "step 49810: train loss = 1.8789, val loss = 1.8857\n",
            "step 49820: train loss = 1.8583, val loss = 1.9605\n",
            "step 49830: train loss = 1.7300, val loss = 1.9511\n",
            "step 49840: train loss = 1.8811, val loss = 1.8238\n",
            "step 49850: train loss = 1.8554, val loss = 1.9452\n",
            "step 49860: train loss = 1.7734, val loss = 1.8689\n",
            "step 49870: train loss = 1.9139, val loss = 1.8330\n",
            "step 49880: train loss = 1.9476, val loss = 1.7917\n",
            "step 49890: train loss = 1.8504, val loss = 1.8400\n",
            "step 49900: train loss = 1.8813, val loss = 1.8806\n",
            "step 49910: train loss = 1.8638, val loss = 1.9046\n",
            "step 49920: train loss = 1.8027, val loss = 1.8710\n",
            "step 49930: train loss = 1.8260, val loss = 1.8875\n",
            "step 49940: train loss = 1.8306, val loss = 1.8609\n",
            "step 49950: train loss = 1.8321, val loss = 1.8648\n",
            "step 49960: train loss = 1.8467, val loss = 1.8460\n",
            "step 49970: train loss = 1.7320, val loss = 1.9146\n",
            "step 49980: train loss = 1.8554, val loss = 1.8603\n",
            "step 49990: train loss = 1.8761, val loss = 1.8213\n",
            "step 50000: train loss = 1.7671, val loss = 1.8411\n",
            "step 50010: train loss = 1.8706, val loss = 1.8379\n",
            "step 50020: train loss = 1.7394, val loss = 1.9632\n",
            "step 50030: train loss = 1.9069, val loss = 1.9554\n",
            "step 50040: train loss = 1.7401, val loss = 1.7889\n",
            "step 50050: train loss = 1.8785, val loss = 1.9379\n",
            "step 50060: train loss = 1.8473, val loss = 1.8063\n",
            "step 50070: train loss = 1.9087, val loss = 1.9411\n",
            "step 50080: train loss = 1.7402, val loss = 2.0131\n",
            "step 50090: train loss = 1.6563, val loss = 1.8117\n",
            "step 50100: train loss = 1.9164, val loss = 1.7927\n",
            "step 50110: train loss = 1.8508, val loss = 1.8200\n",
            "step 50120: train loss = 1.8671, val loss = 1.7770\n",
            "step 50130: train loss = 1.7815, val loss = 1.9258\n",
            "step 50140: train loss = 1.7927, val loss = 1.7997\n",
            "step 50150: train loss = 1.8199, val loss = 1.8286\n",
            "step 50160: train loss = 1.8191, val loss = 1.7441\n",
            "step 50170: train loss = 1.9023, val loss = 1.9385\n",
            "step 50180: train loss = 1.8348, val loss = 1.9823\n",
            "step 50190: train loss = 1.6470, val loss = 1.8526\n",
            "step 50200: train loss = 1.7699, val loss = 1.9417\n",
            "step 50210: train loss = 1.7276, val loss = 1.8255\n",
            "step 50220: train loss = 1.8150, val loss = 1.8968\n",
            "step 50230: train loss = 1.8386, val loss = 1.9544\n",
            "step 50240: train loss = 1.7479, val loss = 1.7152\n",
            "step 50250: train loss = 1.7991, val loss = 1.8300\n",
            "step 50260: train loss = 1.8676, val loss = 1.7512\n",
            "step 50270: train loss = 1.8430, val loss = 1.8459\n",
            "step 50280: train loss = 1.8245, val loss = 1.9735\n",
            "step 50290: train loss = 1.8750, val loss = 1.9453\n",
            "step 50300: train loss = 1.7137, val loss = 1.9753\n",
            "step 50310: train loss = 1.8868, val loss = 1.9029\n",
            "step 50320: train loss = 1.8887, val loss = 1.8524\n",
            "step 50330: train loss = 1.8441, val loss = 1.7579\n",
            "step 50340: train loss = 1.8484, val loss = 1.7832\n",
            "step 50350: train loss = 1.7298, val loss = 1.7663\n",
            "step 50360: train loss = 2.0917, val loss = 1.8535\n",
            "step 50370: train loss = 1.7833, val loss = 1.8426\n",
            "step 50380: train loss = 1.9471, val loss = 1.9643\n",
            "step 50390: train loss = 1.8621, val loss = 1.7539\n",
            "step 50400: train loss = 1.8136, val loss = 1.8228\n",
            "step 50410: train loss = 1.8761, val loss = 1.9516\n",
            "step 50420: train loss = 1.6915, val loss = 1.8174\n",
            "step 50430: train loss = 1.8455, val loss = 1.8207\n",
            "step 50440: train loss = 1.7787, val loss = 1.7715\n",
            "step 50450: train loss = 1.7461, val loss = 1.9150\n",
            "step 50460: train loss = 1.8556, val loss = 1.9020\n",
            "step 50470: train loss = 1.9014, val loss = 1.8660\n",
            "step 50480: train loss = 1.9028, val loss = 1.9223\n",
            "step 50490: train loss = 1.9597, val loss = 1.7594\n",
            "step 50500: train loss = 1.9313, val loss = 1.8494\n",
            "step 50510: train loss = 1.7681, val loss = 1.7556\n",
            "step 50520: train loss = 1.7594, val loss = 1.8523\n",
            "step 50530: train loss = 1.9446, val loss = 1.7573\n",
            "step 50540: train loss = 1.7839, val loss = 1.6967\n",
            "step 50550: train loss = 1.7779, val loss = 2.0440\n",
            "step 50560: train loss = 1.7898, val loss = 1.9728\n",
            "step 50570: train loss = 1.7119, val loss = 1.7130\n",
            "step 50580: train loss = 1.7387, val loss = 1.8831\n",
            "step 50590: train loss = 1.8727, val loss = 1.8768\n",
            "step 50600: train loss = 1.8364, val loss = 1.8360\n",
            "step 50610: train loss = 1.9467, val loss = 1.9212\n",
            "step 50620: train loss = 2.1605, val loss = 2.1241\n",
            "step 50630: train loss = 1.7858, val loss = 1.9384\n",
            "step 50640: train loss = 1.8404, val loss = 1.8434\n",
            "step 50650: train loss = 1.7970, val loss = 1.8734\n",
            "step 50660: train loss = 1.8071, val loss = 1.7689\n",
            "step 50670: train loss = 1.8474, val loss = 1.7558\n",
            "step 50680: train loss = 1.9190, val loss = 1.7057\n",
            "step 50690: train loss = 1.7909, val loss = 1.8013\n",
            "step 50700: train loss = 1.8904, val loss = 1.8890\n",
            "step 50710: train loss = 1.9127, val loss = 1.8871\n",
            "step 50720: train loss = 1.8596, val loss = 1.8457\n",
            "step 50730: train loss = 1.9132, val loss = 1.8015\n",
            "step 50740: train loss = 1.7901, val loss = 1.7337\n",
            "step 50750: train loss = 1.8834, val loss = 1.6697\n",
            "step 50760: train loss = 1.8423, val loss = 1.8966\n",
            "step 50770: train loss = 1.8076, val loss = 1.6983\n",
            "step 50780: train loss = 1.8587, val loss = 1.8190\n",
            "step 50790: train loss = 1.8031, val loss = 1.8774\n",
            "step 50800: train loss = 1.6216, val loss = 1.9661\n",
            "step 50810: train loss = 1.8166, val loss = 1.8122\n",
            "step 50820: train loss = 1.7316, val loss = 1.7827\n",
            "step 50830: train loss = 1.6607, val loss = 1.9782\n",
            "step 50840: train loss = 1.7540, val loss = 1.8553\n",
            "step 50850: train loss = 1.9068, val loss = 1.8878\n",
            "step 50860: train loss = 1.9529, val loss = 2.0041\n",
            "step 50870: train loss = 1.9640, val loss = 1.9098\n",
            "step 50880: train loss = 1.7632, val loss = 1.8795\n",
            "step 50890: train loss = 1.8976, val loss = 1.8353\n",
            "step 50900: train loss = 1.9398, val loss = 1.7844\n",
            "step 50910: train loss = 1.7816, val loss = 1.8114\n",
            "step 50920: train loss = 1.6503, val loss = 1.8544\n",
            "step 50930: train loss = 1.7881, val loss = 1.8754\n",
            "step 50940: train loss = 1.6932, val loss = 1.8322\n",
            "step 50950: train loss = 1.8570, val loss = 1.9707\n",
            "step 50960: train loss = 1.7700, val loss = 1.8972\n",
            "step 50970: train loss = 1.7975, val loss = 1.9102\n",
            "step 50980: train loss = 1.9160, val loss = 1.9813\n",
            "step 50990: train loss = 1.8481, val loss = 1.9333\n",
            "step 51000: train loss = 1.8129, val loss = 1.8439\n",
            "step 51010: train loss = 1.8855, val loss = 1.8364\n",
            "step 51020: train loss = 1.7432, val loss = 1.8150\n",
            "step 51030: train loss = 1.8266, val loss = 1.7318\n",
            "step 51040: train loss = 1.6304, val loss = 1.9160\n",
            "step 51050: train loss = 1.7146, val loss = 1.8379\n",
            "step 51060: train loss = 1.8455, val loss = 1.8467\n",
            "step 51070: train loss = 1.7618, val loss = 1.8568\n",
            "step 51080: train loss = 1.9686, val loss = 1.7445\n",
            "step 51090: train loss = 1.8501, val loss = 1.7321\n",
            "step 51100: train loss = 1.7480, val loss = 1.8164\n",
            "step 51110: train loss = 1.8101, val loss = 1.9189\n",
            "step 51120: train loss = 1.8258, val loss = 1.8948\n",
            "step 51130: train loss = 1.7858, val loss = 1.8652\n",
            "step 51140: train loss = 1.7485, val loss = 1.8259\n",
            "step 51150: train loss = 1.8562, val loss = 1.7683\n",
            "step 51160: train loss = 1.6837, val loss = 1.7719\n",
            "step 51170: train loss = 1.8276, val loss = 1.7675\n",
            "step 51180: train loss = 1.8791, val loss = 1.7097\n",
            "step 51190: train loss = 1.8859, val loss = 1.8890\n",
            "step 51200: train loss = 1.7445, val loss = 1.9964\n",
            "step 51210: train loss = 1.8547, val loss = 1.9375\n",
            "step 51220: train loss = 1.8133, val loss = 1.8459\n",
            "step 51230: train loss = 1.8637, val loss = 1.8303\n",
            "step 51240: train loss = 1.8320, val loss = 1.8661\n",
            "step 51250: train loss = 1.8625, val loss = 1.9279\n",
            "step 51260: train loss = 1.7826, val loss = 1.8334\n",
            "step 51270: train loss = 1.8544, val loss = 1.8181\n",
            "step 51280: train loss = 1.7723, val loss = 2.0049\n",
            "step 51290: train loss = 1.8785, val loss = 1.8290\n",
            "step 51300: train loss = 1.7847, val loss = 1.7011\n",
            "step 51310: train loss = 2.0593, val loss = 1.8698\n",
            "step 51320: train loss = 1.8757, val loss = 1.9892\n",
            "step 51330: train loss = 1.8081, val loss = 1.8875\n",
            "step 51340: train loss = 1.8443, val loss = 1.7750\n",
            "step 51350: train loss = 1.8754, val loss = 1.9738\n",
            "step 51360: train loss = 1.8442, val loss = 2.0048\n",
            "step 51370: train loss = 1.9639, val loss = 1.9119\n",
            "step 51380: train loss = 1.6420, val loss = 1.8243\n",
            "step 51390: train loss = 1.7788, val loss = 1.8527\n",
            "step 51400: train loss = 1.8231, val loss = 1.8470\n",
            "step 51410: train loss = 1.8738, val loss = 1.9661\n",
            "step 51420: train loss = 1.7133, val loss = 1.7662\n",
            "step 51430: train loss = 1.8277, val loss = 1.8558\n",
            "step 51440: train loss = 1.8709, val loss = 1.9213\n",
            "step 51450: train loss = 1.7948, val loss = 1.9655\n",
            "step 51460: train loss = 1.8964, val loss = 1.8957\n",
            "step 51470: train loss = 1.9681, val loss = 1.9150\n",
            "step 51480: train loss = 1.9138, val loss = 1.8819\n",
            "step 51490: train loss = 1.9435, val loss = 2.0144\n",
            "step 51500: train loss = 1.7167, val loss = 1.9131\n",
            "step 51510: train loss = 1.9982, val loss = 1.8746\n",
            "step 51520: train loss = 1.7196, val loss = 1.6788\n",
            "step 51530: train loss = 1.8527, val loss = 1.8217\n",
            "step 51540: train loss = 1.8389, val loss = 1.8504\n",
            "step 51550: train loss = 1.8346, val loss = 1.8114\n",
            "step 51560: train loss = 1.7402, val loss = 1.6577\n",
            "step 51570: train loss = 1.7231, val loss = 1.7319\n",
            "step 51580: train loss = 1.8466, val loss = 1.7744\n",
            "step 51590: train loss = 1.7859, val loss = 1.8843\n",
            "step 51600: train loss = 1.7417, val loss = 1.6198\n",
            "step 51610: train loss = 1.9459, val loss = 1.7884\n",
            "step 51620: train loss = 1.7250, val loss = 1.8453\n",
            "step 51630: train loss = 1.8470, val loss = 1.8620\n",
            "step 51640: train loss = 1.7903, val loss = 1.6290\n",
            "step 51650: train loss = 1.8760, val loss = 1.7804\n",
            "step 51660: train loss = 1.8176, val loss = 1.8000\n",
            "step 51670: train loss = 1.7190, val loss = 1.8295\n",
            "step 51680: train loss = 1.8910, val loss = 1.7844\n",
            "step 51690: train loss = 1.8137, val loss = 1.8132\n",
            "step 51700: train loss = 1.9558, val loss = 1.8638\n",
            "step 51710: train loss = 1.6562, val loss = 1.9031\n",
            "step 51720: train loss = 1.7349, val loss = 1.8499\n",
            "step 51730: train loss = 1.8067, val loss = 1.9527\n",
            "step 51740: train loss = 1.8272, val loss = 1.7105\n",
            "step 51750: train loss = 1.8809, val loss = 1.7758\n",
            "step 51760: train loss = 1.8901, val loss = 1.7470\n",
            "step 51770: train loss = 1.8213, val loss = 1.8361\n",
            "step 51780: train loss = 1.9560, val loss = 1.8519\n",
            "step 51790: train loss = 1.7186, val loss = 1.7919\n",
            "step 51800: train loss = 1.8748, val loss = 1.7719\n",
            "step 51810: train loss = 1.9548, val loss = 1.9157\n",
            "step 51820: train loss = 1.8826, val loss = 1.8358\n",
            "step 51830: train loss = 1.8062, val loss = 1.7517\n",
            "step 51840: train loss = 1.8714, val loss = 1.8332\n",
            "step 51850: train loss = 1.7992, val loss = 1.7654\n",
            "step 51860: train loss = 1.6982, val loss = 1.9610\n",
            "step 51870: train loss = 1.8724, val loss = 1.8992\n",
            "step 51880: train loss = 1.7987, val loss = 2.0474\n",
            "step 51890: train loss = 1.9248, val loss = 1.7334\n",
            "step 51900: train loss = 1.8351, val loss = 1.7209\n",
            "step 51910: train loss = 1.8337, val loss = 1.8341\n",
            "step 51920: train loss = 1.8905, val loss = 1.9089\n",
            "step 51930: train loss = 1.7749, val loss = 1.8626\n",
            "step 51940: train loss = 1.7580, val loss = 1.9279\n",
            "step 51950: train loss = 1.7993, val loss = 1.7246\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3d6ded8422f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mlossi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mlossi_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mlossi_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3d6ded8422f2>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5e4dfa4d5033>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply dropout to the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# apply transformer blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply LayerNorm to the final output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# the shape of x now is n_samples X (n_patches + 1) X embed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5e4dfa4d5033>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add to the residual highway after performing Layernorm and attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add to the residual highway after performing Layernorm and MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5e4dfa4d5033>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (n_samples, n_heads, n_patches+1, head_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# perform the dot product and scale the dot product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdot_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;31m# (n_samples, n_heads, n_patches+1, n_patches+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;31m# apply a softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot_prod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n_samples, n_heads, n_patches+1, n_patches+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss function\n",
        "lossi_train_mean = torch.Tensor(lossi_train[0:50000]).view(-1,100).mean(1)\n",
        "lossi_test_mean = torch.Tensor(lossi_test[0:50000]).view(-1,100).mean(1)\n",
        "plt.plot(lossi_train_mean, label = 'train')\n",
        "plt.plot(lossi_test_mean, label = 'test')\n",
        "plt.legend()\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cross entropy loss')\n",
        "\n",
        "# With data augmentation, the behavior is much better.\n",
        "# the divergence is minimized up to 50k iterations.\n",
        "\n",
        "# We will do 50k more soon."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "OKOC9V1PXp6o",
        "outputId": "d315a788-e638-4803-a21a-0cd8d55ca6f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'cross entropy loss')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show an example of a picture and its prediction on the side, also plot the attention weights\n",
        "import seaborn as sns\n",
        "\n",
        "## Extract the attention weights from your model\n",
        "#layer_number = ... # Choose a specific layer number\n",
        "#head_number = ... # Choose a specific head number\n",
        "#weights = model.transformer.layer[layer_number].attention.self.weights[head_number].detach().numpy()\n",
        "\n",
        "# Make some basic image transformations\n",
        "def augment_image(img):\n",
        "    # Color jitter\n",
        "    img = img + 0.1 * torch.randn_like(img)\n",
        "    \n",
        "    # Random rotation\n",
        "    angle = 30 * torch.randn(1).item()\n",
        "    angle = angle / 180 * 3.14\n",
        "    angle = torch.tensor(angle)\n",
        "    rotation_matrix = torch.tensor([\n",
        "        [torch.cos(angle), -torch.sin(angle), 0],\n",
        "        [torch.sin(angle), torch.cos(angle), 0]\n",
        "    ]).unsqueeze(0)\n",
        "    grid = F.affine_grid(rotation_matrix, torch.Size([1, 3, 32, 32]))\n",
        "    grid = grid.to(device)\n",
        "    img = F.grid_sample(img.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros').squeeze(0)\n",
        "    \n",
        "    # Channel shuffling\n",
        "    channel_shuffle_order = torch.randperm(3)\n",
        "    img = img[channel_shuffle_order, :, :]\n",
        "\n",
        "    # Random Flipping\n",
        "    flip_dir = torch.randint(2, (1,)).item()\n",
        "    if flip_dir == 0:\n",
        "        img = torch.flip(img, [2]) # flip horizontally\n",
        "    else:\n",
        "        img = torch.flip(img, [1]) # flip vertically\n",
        "\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "# class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "def pred2label(pred):\n",
        "    return class_names[pred]\n",
        "\n",
        "\n",
        "def show_example():\n",
        "    imgs,yb = get_batch('test')\n",
        "    #imgs = [augment_image(img) for img in imgs]\n",
        "    #xb = torch.stack(imgs)\n",
        "    xb = imgs\n",
        "    logits = model(xb)\n",
        "    pred = torch.argmax(logits, dim = 1)\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(xb[0].cpu().numpy().transpose(1,2,0).astype('uint8'))\n",
        "    # Plot the attention weights as a heatmap\n",
        "    #sns.heatmap(weights, cmap=\"YlGnBu\")\n",
        "    plt.title(f'Prediction: {class_names[pred[0].item()]}, Actual: {class_names[yb[0].item()]}')\n",
        "    plt.show()\n",
        "    \n",
        "show_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "CI2GYhjCZSir",
        "outputId": "3ace8d0f-1276-4ced-d1f1-1c5987018fff"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}