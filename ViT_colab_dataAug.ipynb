{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCNfhQe47wL2W2RwRn/gHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56859f1ea2794947b2248bce936935cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b3b2380b99a438890180be973a6086c",
              "IPY_MODEL_9d6bc5de427445f0ac4dc30caf901f80",
              "IPY_MODEL_83b6402810d84202a40f7006887c371d"
            ],
            "layout": "IPY_MODEL_24f3b06afa524a3c9bff83cc6ddd02b6"
          }
        },
        "8b3b2380b99a438890180be973a6086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc4fd62d550f44d6beced7e7e95a079f",
            "placeholder": "​",
            "style": "IPY_MODEL_320580924b734455ae3387ed9a48f92b",
            "value": "100%"
          }
        },
        "9d6bc5de427445f0ac4dc30caf901f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a11852f46a694eb3b825b32b61550e6f",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d98215e52194e0fa75c2dd766a51437",
            "value": 170498071
          }
        },
        "83b6402810d84202a40f7006887c371d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d03372295f449d8840ee92363e9776",
            "placeholder": "​",
            "style": "IPY_MODEL_f171c57807024fb29b2abf826b847daa",
            "value": " 170498071/170498071 [00:05&lt;00:00, 32037888.33it/s]"
          }
        },
        "24f3b06afa524a3c9bff83cc6ddd02b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc4fd62d550f44d6beced7e7e95a079f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320580924b734455ae3387ed9a48f92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a11852f46a694eb3b825b32b61550e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d98215e52194e0fa75c2dd766a51437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71d03372295f449d8840ee92363e9776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f171c57807024fb29b2abf826b847daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarajnanda2021/ViT_original/blob/main/ViT_colab_dataAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OEbbicMjWw8N"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init as init\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the CIFAR-10 dataset\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "56859f1ea2794947b2248bce936935cb",
            "8b3b2380b99a438890180be973a6086c",
            "9d6bc5de427445f0ac4dc30caf901f80",
            "83b6402810d84202a40f7006887c371d",
            "24f3b06afa524a3c9bff83cc6ddd02b6",
            "bc4fd62d550f44d6beced7e7e95a079f",
            "320580924b734455ae3387ed9a48f92b",
            "a11852f46a694eb3b825b32b61550e6f",
            "6d98215e52194e0fa75c2dd766a51437",
            "71d03372295f449d8840ee92363e9776",
            "f171c57807024fb29b2abf826b847daa"
          ]
        },
        "id": "LZG3TriHXAKP",
        "outputId": "7f72aa3e-b543-4f73-9b88-ccf44349e129"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56859f1ea2794947b2248bce936935cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle \n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "%matplotlib inline\n",
        "def load_batch(fpath, label_key='labels'):\n",
        "    with open(fpath, 'rb') as f:\n",
        "        if sys.version_info < (3,):\n",
        "            d = pickle.load(f)\n",
        "        else:\n",
        "            d = pickle.load(f, encoding='bytes')\n",
        "            # decode utf8\n",
        "            d_decoded = {}\n",
        "            for k, v in d.items():\n",
        "                d_decoded[k.decode('utf8')] = v\n",
        "            d = d_decoded\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "def load_cifar10():\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for i in range(1, 6):\n",
        "        fpath = './data/cifar-10-batches-py/data_batch_' + str(i)\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train.append(data)\n",
        "        y_train.append(labels)\n",
        "\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    x_test, y_test = load_batch(\"./data/cifar-10-batches-py/test_batch\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DyDWb8wYXgBC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "\n",
        "# We will now write a script for getting image patches\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "\n",
        "    '''\n",
        "    Params:\n",
        "    -------\n",
        "    img_size (int)      : size of the image (assumed to be square) \n",
        "    patch_size (int)    : size of the patch (assumed to be square)\n",
        "    in_chans (int)      : number of channels in the image (assumed to be RGB typically)\n",
        "    embed_dim (int)     : embedding dimension (will be constant throughout the network)\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "    num_patches (int)   : number of patches in the image\n",
        "    proj (nn.Conv2d)    : convolutional layer to get the patches, will have same stride as patch_size\n",
        "    '''\n",
        "    def __init__(self, img_size, patch_size, in_chans=3,embed_dim=256):\n",
        "        super().__init__() # call the super class constructor which is used to inherit the properties of the parent class\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2 # assuming square image\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride = patch_size\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ''' Parameters: \n",
        "        x (torch.Tensor): input image of shape (n_samples or batches, number of channels, height, width)\n",
        "        Returns: \n",
        "        output = n_samplex X n_patches X embed_dim shape tensor\n",
        "        '''\n",
        "        x = self.proj(x) # n_samples X embed_dim X sqrt(n_patches) X sqrt(n_patches)\n",
        "        x = x.flatten(2) # n_sample X embed_dim X n_patches\n",
        "        x = x.transpose(1, 2) # n_samples X n_patches X embed_dim (dimensions are swapped)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Let us now write the attention module\n",
        "class Attention(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding dimension, \n",
        "    n_heads (int)       : number of attention heads\n",
        "    qkv_bias (bool)     : if True, we will include a bias in the query, key and value projections\n",
        "    attn_d (float)      : Probability of dropout added to q, k and v during the training\n",
        "    proj_d (float)      : Probability of dropout added to the projection layer\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    scale (float)               : Used for norrmalizing the dot product\n",
        "    qkv (nn.Linear)             : Linear projection, which are used for performing the attention\n",
        "    proj (nn.Linear)            : Takes in the concatenated output of all attention heads and maps it further\n",
        "    attn_d, proj_d (nn.Dropout) : Dropout layers\n",
        "\n",
        "    '''\n",
        "    def __init__(self,dim, n_heads=4, qkv_bias = False, attn_d = 0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5 # scaling added as per Vaswani paper for not feeding extremely large values to softmas\n",
        "        self.qkv = nn.Linear(dim,dim * 3, bias = qkv_bias) # can be written separately too\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_d = nn.Dropout(proj_d)\n",
        "        self.attn_d = nn.Dropout(attn_d)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        ''' \n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor) : has shape (n_samples/batch, n_patches+1, dim)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor (n_samples, n_patches+1, dim)\n",
        "\n",
        "        '''\n",
        "        n_samples, n_tokens, dim = x.shape # extract shapes, tokens and dimensions from the output of the embeddings\n",
        "        if dim != self.dim:\n",
        "            raise ValueError # raise an error if dim isn't equal to the dimension set in the attention layer\n",
        "        \n",
        "        qkv = self.qkv(x) # Perform the query, key, value projections. (n_samples/batches, n_patches+1, 3*dim), the middle dimension is maintained\n",
        "\n",
        "        # Let us now reshape the qkv tensor to separate the query, key and value\n",
        "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(2,0,3,1,4) # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # Now extract the query, key and value\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2] # (n_samples, n_heads, n_patches+1, head_dim)\n",
        "        # perform the dot product and scale the dot product\n",
        "        dot_prod = (q @ k.transpose(-2,-1)) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        # apply a softmax\n",
        "        attention = dot_prod.softmax(dim = -1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
        "        attention = self.attn_d(attention) # apply dropout for regularization during training\n",
        "        # weighted average\n",
        "        wei = (attention @ v).transpose(1,2) # (n_samples, n_patches+1, n_heads, head_dim)\n",
        "        # flatten\n",
        "        wei = wei.flatten(2) # (n_samples, n_patches+1, dim) as dim = n_heads * head_dim\n",
        "        # we now apply the projection\n",
        "        x = self.proj(wei) # (n_samples, n_patches+1, dim)\n",
        "        x = self.proj_d(x) # apply dropout for regularization during training\n",
        "        return x\n",
        "\n",
        "    # Let us now write the MLP module\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    ''' \n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features (int)           : embedding dimension, \n",
        "    hidden_features(int)        : dimension of the hidden layer\n",
        "    out_features (int)          : dimension of the hidden layer\n",
        "    dropout (float)     : probability of dropout\n",
        "    \n",
        "    Attributes\n",
        "    __________\n",
        "    fc1 (nn.Linear)     : Linear projection, which are used for performing the attention\n",
        "    fc2 (nn.Linear)     : Takes in the concatenated output of all attention heads and maps it further\n",
        "    dropout (nn.Dropout): Dropout layer\n",
        "\n",
        "    '''\n",
        "    def __init__(self, in_features,hidden_features,out_features, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features) # takes in the input and maps it to the hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features) # takes in the hidden layer and maps it to the output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.GELU() # we will the GELU activation function in the paper\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) # apply the first linear projection, (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.act(x) # apply the activation function (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, hidden_features)\n",
        "        x = self.fc2(x) # apply the second linear projection (n_samples, n_patches+1, out_features)\n",
        "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, out_features)\n",
        "        return x\n",
        "\n",
        "# We have everything we need to write the ViT class\n",
        "\n",
        "class Block(nn.Module):\n",
        "    ''' Transformer with Vision Token\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim (int)           : embedding\n",
        "    n_heads (int)       : number of attention heads\n",
        "    mlp_ratio (float)   : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)     : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,          : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2        :  LayerNorm layers\n",
        "    attn                : Attention layer\n",
        "    mlp                 : MLP layer\n",
        "    '''\n",
        "    def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias=True, attn_d=0., proj_d = 0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6) # division by zero is prevented and we match the props of the pretrained model\n",
        "        self.attn = Attention(dim, n_heads, qkv_bias, attn_d, proj_d)\n",
        "        self.norm2 = nn.LayerNorm(dim, eps = 1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features = dim, hidden_features = hidden_features, out_features = dim, dropout = proj_d)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x)) # add to the residual highway after performing Layernorm and attention\n",
        "        x = x + self.mlp(self.norm2(x)) # add to the residual highway after performing Layernorm and MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "# now we can write the Vision Transformer class\n",
        "class ViT(nn.Module):\n",
        "    ''' Vision Transformer\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_size (int)            : size of the input image\n",
        "    patch_size (int)            : size of the patches to be extracted from the input image\n",
        "    in_channels (int)           : number of input channels\n",
        "    num_classes (int)           : number of classes\n",
        "    embed_dim (int              : embedding dimension\n",
        "    depth (int)                 : number of transformer blocks\n",
        "    n_heads (int)               : number of attention heads per block\n",
        "    mlp_ratio (float)           : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
        "    qkv_bias (bool)             : whether to add a bias to the qkv projection layer\n",
        "    attn_d, proj_d,             : dropout probabilities\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed (nn.Conv2d)     : Convolutional embedding layer\n",
        "    pos_embed (nn.Parameter)    : learnable positional embedding\n",
        "    cls_token (nn.Parameter)    : learnable class token\n",
        "    blocks (nn.ModuleList)      : list of transformer blocks\n",
        "    norm (nn.LayerNorm)         : final LayerNorm layer\n",
        "    head (nn.Linear)            : final linear projection layer\n",
        "    '''\n",
        "    # initialize\n",
        "    def __init__(self, \n",
        "                img_size = 384, \n",
        "                patch_size = 16, \n",
        "                in_chans=3, \n",
        "                n_classes = 1000, \n",
        "                embed_dim = 768, \n",
        "                depth = 12,\n",
        "                n_heads = 12,\n",
        "                mlp_ratio = 4.0,\n",
        "                qkv_bias = True,\n",
        "                attn_d = 0.,\n",
        "                proj_d = 0.):\n",
        "        super().__init__()\n",
        "        # we will use the same image size as the pretrained model\n",
        "        self.patch_embed = PatchEmbed(\n",
        "                                        img_size = img_size,\n",
        "                                        patch_size = patch_size,\n",
        "                                        in_chans = in_chans,\n",
        "                                        embed_dim = embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)) # learnable class token\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim)) # learnable positional embedding\n",
        "        self.pos_d     = nn.Dropout(p = proj_d) # dropout layer\n",
        "        self.blocks    = nn.ModuleList(\n",
        "                            [\n",
        "                                Block( \n",
        "                                    dim = embed_dim, \n",
        "                                    n_heads = n_heads, \n",
        "                                    mlp_ratio = mlp_ratio, \n",
        "                                    qkv_bias = qkv_bias, \n",
        "                                    attn_d = attn_d, \n",
        "                                    proj_d = proj_d) for _ in range(depth)] # iteratively create the transformer blocks with same parameters\n",
        "                                    )\n",
        "        self.norm       = nn.LayerNorm(embed_dim, eps = 1e-6) # final LayerNorm layer\n",
        "        self.head       = nn.Linear(embed_dim, n_classes) # final linear projection layer    \n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        ''' Forward pass\n",
        "        Parameters\n",
        "        ----------\n",
        "        x (torch.Tensor)            : n_samples X in_chans X img_size X img_size\n",
        "        Returns\n",
        "        -------\n",
        "        logits (torch.Tensor)       : n_samples X n_classes\n",
        "        '''\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x) # extract patches from the input image and turn them into patch embeddings\n",
        "        cls_tokens = self.cls_token.expand(n_samples, -1, -1) # expand the class token to match the batch size\n",
        "        # pre-append the class token to the patch embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim = 1) # n_samples X (n_patches + 1) X embed_dim\n",
        "        x = x + self.pos_embed # add the positional embedding to the patch embeddings\n",
        "        x = self.pos_d(x) # apply dropout to the embeddings\n",
        "        for block in self.blocks: # apply transformer blocks\n",
        "            x = block(x)\n",
        "        x = self.norm(x) # apply LayerNorm to the final output\n",
        "        # the shape of x now is n_samples X (n_patches + 1) X embed_dim\n",
        "        # extract the class token from the output\n",
        "        cls_token = x[:, 0] # n_samples X embed_dim\n",
        "        x = self.head(cls_token) # n_samples X n_classes\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZiCj6Ce6X41D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a small test script with a smaller model to train on the cifar10 dataset\n",
        "\n",
        "# construct model\n",
        "model = ViT(img_size = 32, \n",
        "                    patch_size = 16, \n",
        "                    in_chans=3, \n",
        "                    n_classes = 10, \n",
        "                    embed_dim = 768, \n",
        "                    depth = 12,\n",
        "                    n_heads = 8,\n",
        "                    mlp_ratio = 4.0,\n",
        "                    qkv_bias = True,\n",
        "                    attn_d = 0.2,\n",
        "                    proj_d = 0.2)\n",
        "\n",
        "print(f'Total params: {sum(p.numel() for p in model.parameters())}')\n",
        "\n",
        "\n",
        "# Make some basic image transformations\n",
        "def augment_image(img):\n",
        "    # Color jitter\n",
        "    img = img + 0.1 * torch.randn_like(img)\n",
        "    \n",
        "    # Random rotation\n",
        "    angle = 30 * torch.randn(1).item()\n",
        "    angle = angle / 180 * 3.14\n",
        "    angle = torch.tensor(angle)\n",
        "    rotation_matrix = torch.tensor([\n",
        "        [torch.cos(angle), -torch.sin(angle), 0],\n",
        "        [torch.sin(angle), torch.cos(angle), 0]\n",
        "    ]).unsqueeze(0)\n",
        "    grid = F.affine_grid(rotation_matrix, torch.Size([1, 3, 32, 32]))\n",
        "    grid = grid.to(device)\n",
        "    img = F.grid_sample(img.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros').squeeze(0)\n",
        "    \n",
        "    # Channel shuffling\n",
        "    channel_shuffle_order = torch.randperm(3)\n",
        "    img = img[channel_shuffle_order, :, :]\n",
        "\n",
        "    # Random Flipping\n",
        "    flip_dir = torch.randint(2, (1,)).item()\n",
        "    if flip_dir == 0:\n",
        "        img = torch.flip(img, [2]) # flip horizontally\n",
        "    else:\n",
        "        img = torch.flip(img, [1]) # flip vertically\n",
        "    \n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the weights\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.normal_(m.weight, std = 0.02)\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        init.ones_(m.weight)\n",
        "        init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight, mode = 'fan_in', nonlinearity = 'leaky_relu')\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "\n",
        "# load data and convert to torch tensors\n",
        "x_train, y_train, x_test, y_test = load_cifar10()\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.FloatTensor(y_test).long()\n",
        "\n",
        "print(f'Training set:: input: {x_train.shape}, output: {y_train.shape}')\n",
        "print(f'Test set:: input: {x_test.shape}, output: {y_test.shape}')\n",
        "\n",
        "# define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4) # use Adam optimizer with a small learning rate\n",
        "\n",
        "# Send all to device\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# define a function to get mini batches of data\n",
        "def get_batch(mode):\n",
        "    if mode == 'train':\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        xb = x_train[idx]\n",
        "        yb = y_train[idx]\n",
        "    elif mode == 'test':\n",
        "        idx = np.random.randint(0, x_test.shape[0], batch_size)\n",
        "        xb = x_test[idx]\n",
        "        yb = y_test[idx]\n",
        "    return xb,yb\n",
        "\n",
        "# Write a function to evaluate the loss on the training and test set\n",
        "def estimate_loss():\n",
        "    losses = {}\n",
        "    for mode in ['train', 'val']:\n",
        "        if mode == 'train':\n",
        "            xb,yb = get_batch('train')\n",
        "        elif mode == 'val':\n",
        "            xb,yb = get_batch('test')\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        losses[mode] = loss.item()\n",
        "    return losses\n",
        "\n",
        "# train the model\n",
        "max_iters = 1000\n",
        "batch_size = 32\n",
        "eval_interval = 10\n",
        "lossi_train = []\n",
        "lossi_test = []\n",
        "for i in range(max_iters):\n",
        "    # print training and testing loss\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n",
        "    # sample from the batch\n",
        "    imgs,yb = get_batch('train')\n",
        "    # perform a randomized data augmentation of the inputs\n",
        "    imgs = [augment_image(img) for img in imgs]\n",
        "    xb = torch.stack(imgs)\n",
        "    # forward pass\n",
        "    logits = model(xb)\n",
        "    loss = criterion(logits, yb)\n",
        "    lossi = estimate_loss()\n",
        "    lossi_train.append(lossi['train'])\n",
        "    lossi_test.append(lossi['val'])\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "loss_fin = estimate_loss()\n",
        "print(f\"step {i}: train loss = {loss_fin['train']:.4f}, val loss = {loss_fin['val']:.4f}\")\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X49TPSWgX8aA",
        "outputId": "51e99f64-9d1f-4c27-80e4-d4bc673110ef"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 38231562\n",
            "Training set:: input: torch.Size([50000, 3, 32, 32]), output: torch.Size([50000])\n",
            "Test set:: input: torch.Size([10000, 3, 32, 32]), output: torch.Size([10000])\n",
            "step 0: train loss = 2.3480, val loss = 2.4216\n",
            "step 10: train loss = 2.4423, val loss = 2.5004\n",
            "step 20: train loss = 2.4349, val loss = 2.3156\n",
            "step 30: train loss = 2.2554, val loss = 2.3555\n",
            "step 40: train loss = 2.2947, val loss = 2.2994\n",
            "step 50: train loss = 2.3335, val loss = 2.3201\n",
            "step 60: train loss = 2.3784, val loss = 2.4202\n",
            "step 70: train loss = 2.3368, val loss = 2.3672\n",
            "step 80: train loss = 2.3682, val loss = 2.3131\n",
            "step 90: train loss = 2.2543, val loss = 2.3595\n",
            "step 100: train loss = 2.3024, val loss = 2.3911\n",
            "step 110: train loss = 2.3514, val loss = 2.3516\n",
            "step 120: train loss = 2.3219, val loss = 2.3600\n",
            "step 130: train loss = 2.3402, val loss = 2.3352\n",
            "step 140: train loss = 2.5210, val loss = 2.4290\n",
            "step 150: train loss = 2.3337, val loss = 2.2913\n",
            "step 160: train loss = 2.3467, val loss = 2.3740\n",
            "step 170: train loss = 2.3701, val loss = 2.3701\n",
            "step 180: train loss = 2.3862, val loss = 2.3518\n",
            "step 190: train loss = 2.2924, val loss = 2.3823\n",
            "step 200: train loss = 2.3193, val loss = 2.2815\n",
            "step 210: train loss = 2.3579, val loss = 2.3100\n",
            "step 220: train loss = 2.3469, val loss = 2.3088\n",
            "step 230: train loss = 2.2589, val loss = 2.3532\n",
            "step 240: train loss = 2.2759, val loss = 2.4323\n",
            "step 250: train loss = 2.2518, val loss = 2.2149\n",
            "step 260: train loss = 2.3566, val loss = 2.2668\n",
            "step 270: train loss = 2.2614, val loss = 2.2858\n",
            "step 280: train loss = 2.3327, val loss = 2.3079\n",
            "step 290: train loss = 2.2958, val loss = 2.2670\n",
            "step 300: train loss = 2.2036, val loss = 2.3006\n",
            "step 310: train loss = 2.3141, val loss = 2.3157\n",
            "step 320: train loss = 2.2062, val loss = 2.2846\n",
            "step 330: train loss = 2.3344, val loss = 2.2682\n",
            "step 340: train loss = 2.1901, val loss = 2.3540\n",
            "step 350: train loss = 2.3407, val loss = 2.2998\n",
            "step 360: train loss = 2.2340, val loss = 2.3283\n",
            "step 370: train loss = 2.2369, val loss = 2.3387\n",
            "step 380: train loss = 2.1968, val loss = 2.3164\n",
            "step 390: train loss = 2.3585, val loss = 2.1778\n",
            "step 400: train loss = 2.3301, val loss = 2.1891\n",
            "step 410: train loss = 2.2833, val loss = 2.3365\n",
            "step 420: train loss = 2.3733, val loss = 2.2887\n",
            "step 430: train loss = 2.2403, val loss = 2.1937\n",
            "step 440: train loss = 2.1452, val loss = 2.3166\n",
            "step 450: train loss = 2.2058, val loss = 2.2117\n",
            "step 460: train loss = 2.2891, val loss = 2.1521\n",
            "step 470: train loss = 2.2282, val loss = 2.1178\n",
            "step 480: train loss = 2.3146, val loss = 2.4470\n",
            "step 490: train loss = 2.1380, val loss = 2.1436\n",
            "step 500: train loss = 2.1666, val loss = 2.1456\n",
            "step 510: train loss = 2.0362, val loss = 2.2907\n",
            "step 520: train loss = 2.3480, val loss = 2.4700\n",
            "step 530: train loss = 2.2515, val loss = 2.2923\n",
            "step 540: train loss = 2.1990, val loss = 2.3459\n",
            "step 550: train loss = 2.3705, val loss = 2.3454\n",
            "step 560: train loss = 2.5355, val loss = 2.2441\n",
            "step 570: train loss = 2.3843, val loss = 2.1977\n",
            "step 580: train loss = 2.4055, val loss = 2.4150\n",
            "step 590: train loss = 2.2481, val loss = 2.1122\n",
            "step 600: train loss = 2.2337, val loss = 2.3274\n",
            "step 610: train loss = 2.0066, val loss = 2.3068\n",
            "step 620: train loss = 2.2790, val loss = 2.2494\n",
            "step 630: train loss = 2.1679, val loss = 2.1640\n",
            "step 640: train loss = 2.1787, val loss = 2.2772\n",
            "step 650: train loss = 2.2675, val loss = 2.1616\n",
            "step 660: train loss = 2.1519, val loss = 2.1938\n",
            "step 670: train loss = 2.2263, val loss = 2.1251\n",
            "step 680: train loss = 2.4708, val loss = 2.4487\n",
            "step 690: train loss = 2.2259, val loss = 2.1895\n",
            "step 700: train loss = 2.2147, val loss = 2.3518\n",
            "step 710: train loss = 2.3225, val loss = 2.2706\n",
            "step 720: train loss = 2.1706, val loss = 2.1696\n",
            "step 730: train loss = 2.2019, val loss = 2.2038\n",
            "step 740: train loss = 2.4449, val loss = 2.2176\n",
            "step 750: train loss = 2.2170, val loss = 2.1726\n",
            "step 760: train loss = 2.0763, val loss = 2.1582\n",
            "step 770: train loss = 2.1640, val loss = 2.2726\n",
            "step 780: train loss = 2.0802, val loss = 2.1801\n",
            "step 790: train loss = 2.0994, val loss = 2.0666\n",
            "step 800: train loss = 2.2200, val loss = 2.3420\n",
            "step 810: train loss = 2.1468, val loss = 2.1036\n",
            "step 820: train loss = 2.2238, val loss = 2.1105\n",
            "step 830: train loss = 2.2668, val loss = 2.3057\n",
            "step 840: train loss = 2.2985, val loss = 2.1536\n",
            "step 850: train loss = 2.4078, val loss = 2.2371\n",
            "step 860: train loss = 2.1411, val loss = 2.2394\n",
            "step 870: train loss = 2.1761, val loss = 2.2036\n",
            "step 880: train loss = 2.2588, val loss = 2.2154\n",
            "step 890: train loss = 2.1882, val loss = 2.1580\n",
            "step 900: train loss = 2.2604, val loss = 2.1249\n",
            "step 910: train loss = 2.3324, val loss = 2.1427\n",
            "step 920: train loss = 2.1415, val loss = 2.0830\n",
            "step 930: train loss = 2.2085, val loss = 1.9785\n",
            "step 940: train loss = 2.3033, val loss = 2.2975\n",
            "step 950: train loss = 2.1897, val loss = 2.1913\n",
            "step 960: train loss = 2.2030, val loss = 2.3632\n",
            "step 970: train loss = 2.2325, val loss = 2.1316\n",
            "step 980: train loss = 2.0949, val loss = 2.2976\n",
            "step 990: train loss = 2.2088, val loss = 2.2508\n",
            "step 999: train loss = 2.3876, val loss = 2.1418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss function\n",
        "lossi_train_mean = torch.Tensor(lossi_train).view(-1,100).mean(1)\n",
        "lossi_test_mean = torch.Tensor(lossi_test).view(-1,100).mean(1)\n",
        "plt.plot(lossi_train_mean, label = 'train')\n",
        "plt.plot(lossi_test_mean, label = 'test')\n",
        "plt.legend()\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('cross entropy loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OKOC9V1PXp6o",
        "outputId": "4819265a-4394-4039-8d89-f1a5c89f120d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'cross entropy loss')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show an example of a picture and its prediction on the side, also plot the attention weights\n",
        "import seaborn as sns\n",
        "\n",
        "## Extract the attention weights from your model\n",
        "#layer_number = ... # Choose a specific layer number\n",
        "#head_number = ... # Choose a specific head number\n",
        "#weights = model.transformer.layer[layer_number].attention.self.weights[head_number].detach().numpy()\n",
        "\n",
        "# Make some basic image transformations\n",
        "def augment_image(img):\n",
        "    # Color jitter\n",
        "    img = img + 0.1 * torch.randn_like(img)\n",
        "    \n",
        "    # Random rotation\n",
        "    angle = 30 * torch.randn(1).item()\n",
        "    angle = angle / 180 * 3.14\n",
        "    angle = torch.tensor(angle)\n",
        "    rotation_matrix = torch.tensor([\n",
        "        [torch.cos(angle), -torch.sin(angle), 0],\n",
        "        [torch.sin(angle), torch.cos(angle), 0]\n",
        "    ]).unsqueeze(0)\n",
        "    grid = F.affine_grid(rotation_matrix, torch.Size([1, 3, 32, 32]))\n",
        "    grid = grid.to(device)\n",
        "    img = F.grid_sample(img.unsqueeze(0), grid, mode='bilinear', padding_mode='zeros').squeeze(0)\n",
        "    \n",
        "    # Channel shuffling\n",
        "    channel_shuffle_order = torch.randperm(3)\n",
        "    img = img[channel_shuffle_order, :, :]\n",
        "\n",
        "    # Random Flipping\n",
        "    flip_dir = torch.randint(2, (1,)).item()\n",
        "    if flip_dir == 0:\n",
        "        img = torch.flip(img, [2]) # flip horizontally\n",
        "    else:\n",
        "        img = torch.flip(img, [1]) # flip vertically\n",
        "\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "# class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "def pred2label(pred):\n",
        "    return class_names[pred]\n",
        "\n",
        "\n",
        "def show_example():\n",
        "    imgs,yb = get_batch('train')\n",
        "    imgs = [augment_image(img) for img in imgs]\n",
        "    xb = torch.stack(imgs)\n",
        "    logits = model(xb)\n",
        "    pred = torch.argmax(logits, dim = 1)\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(xb[0].cpu().numpy().transpose(1,2,0).astype('uint8'))\n",
        "    # Plot the attention weights as a heatmap\n",
        "    #sns.heatmap(weights, cmap=\"YlGnBu\")\n",
        "    plt.title(f'Prediction: {class_names[pred[0].item()]}, Actual: {class_names[yb[0].item()]}')\n",
        "    plt.show()\n",
        "    \n",
        "show_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "CI2GYhjCZSir",
        "outputId": "86e97af4-abd5-4df4-c9d5-cb5e668d6cb2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-1f02f706486b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mshow_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-1f02f706486b>\u001b[0m in \u001b[0;36mshow_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5e4dfa4d5033>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# pre-append the class token to the patch embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n_samples X (n_patches + 1) X embed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m \u001b[0;31m# add the positional embedding to the patch embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply dropout to the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# apply transformer blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}