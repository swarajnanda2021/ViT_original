{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to implement the ViT model from scratch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def load_batch(fpath, label_key='labels'):\n",
    "    with open(fpath, 'rb') as f:\n",
    "        if sys.version_info < (3,):\n",
    "            d = pickle.load(f)\n",
    "        else:\n",
    "            d = pickle.load(f, encoding='bytes')\n",
    "            # decode utf8\n",
    "            d_decoded = {}\n",
    "            for k, v in d.items():\n",
    "                d_decoded[k.decode('utf8')] = v\n",
    "            d = d_decoded\n",
    "    data = d[\"data\"]\n",
    "    labels = d[label_key]\n",
    "\n",
    "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
    "    return data, labels\n",
    "\n",
    "def load_cifar10():\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for i in range(1, 6):\n",
    "        fpath = 'cifar-10-batches-py/data_batch_' + str(i)\n",
    "        data, labels = load_batch(fpath)\n",
    "        x_train.append(data)\n",
    "        y_train.append(labels)\n",
    "\n",
    "    x_train = np.concatenate(x_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    x_test, y_test = load_batch(\"cifar-10-batches-py/test_batch\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 2, 2])\n",
      "torch.Size([1, 3, 4])\n",
      "torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let us now extract the patches from the image using a simple convolution layer\n",
    "# We will use a kernel size of 16 and stride of 16\n",
    "\n",
    "patch_size = 16\n",
    "img = torch.tensor(x_train[0]).unsqueeze(0).float()\n",
    "#img = nn.functional.pad(img, (8, 8, 8, 8))\n",
    "print(img.shape)\n",
    "\n",
    "patch_extractor = nn.Conv2d(\n",
    "    in_channels=3, \n",
    "    out_channels=3, \n",
    "    kernel_size=patch_size, \n",
    "    stride=patch_size, \n",
    "    bias=False)\n",
    "patches = patch_extractor(img)\n",
    "print(patches.shape)\n",
    "\n",
    "# now flatten the patches\n",
    "patches = patches.flatten(2) # img is 3 X 32 X 32, so patches is 3 X 2 X 2 and flattened, it is a vector that is 12 long\n",
    "print(patches.shape)  \n",
    "\n",
    "patches = patches.transpose(1,2)\n",
    "print(patches.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will now write a script for getting image patches\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Params:\n",
    "    -------\n",
    "    img_size (int)      : size of the image (assumed to be square) \n",
    "    patch_size (int)    : size of the patch (assumed to be square)\n",
    "    in_chans (int)      : number of channels in the image (assumed to be RGB typically)\n",
    "    embed_dim (int)     : embedding dimension (will be constant throughout the network)\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    num_patches (int)   : number of patches in the image\n",
    "    proj (nn.Conv2d)    : convolutional layer to get the patches, will have same stride as patch_size\n",
    "    '''\n",
    "    def __init__(self, img_size, patch_size, in_chans=3,embed_dim=256):\n",
    "        super().__init__() # call the super class constructor which is used to inherit the properties of the parent class\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2 # assuming square image\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride = patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' Parameters: \n",
    "        x (torch.Tensor): input image of shape (n_samples or batches, number of channels, height, width)\n",
    "        Returns: \n",
    "        output = n_samplex X n_patches X embed_dim shape tensor\n",
    "        '''\n",
    "        x = self.proj(x) # n_samples X embed_dim X sqrt(n_patches) X sqrt(n_patches)\n",
    "        x = x.flatten(2) # n_sample X embed_dim X n_patches\n",
    "        x = x.transpose(1, 2) # n_samples X n_patches X embed_dim (dimensions are swapped)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Let us now write the attention module\n",
    "class Attention(nn.Module):\n",
    "    ''' \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim (int)           : embedding dimension, \n",
    "    n_heads (int)       : number of attention heads\n",
    "    qkv_bias (bool)     : if True, we will include a bias in the query, key and value projections\n",
    "    attn_d (float)      : Probability of dropout added to q, k and v during the training\n",
    "    proj_d (float)      : Probability of dropout added to the projection layer\n",
    "    \n",
    "    Attributes\n",
    "    __________\n",
    "    scale (float)               : Used for norrmalizing the dot product\n",
    "    qkv (nn.Linear)             : Linear projection, which are used for performing the attention\n",
    "    proj (nn.Linear)            : Takes in the concatenated output of all attention heads and maps it further\n",
    "    attn_d, proj_d (nn.Dropout) : Dropout layers\n",
    "\n",
    "    '''\n",
    "    def __init__(self,dim, n_heads=4, qkv_bias = False, attn_d = 0., proj_d = 0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5 # scaling added as per Vaswani paper for not feeding extremely large values to softmas\n",
    "        self.qkv = nn.Linear(dim,dim * 3, bias = qkv_bias) # can be written separately too\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_d = nn.Dropout(proj_d)\n",
    "        self.attn_d = nn.Dropout(attn_d)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        ''' \n",
    "        Parameters\n",
    "        ----------\n",
    "        x (torch.Tensor) : has shape (n_samples/batch, n_patches+1, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (n_samples, n_patches+1, dim)\n",
    "\n",
    "        '''\n",
    "        n_samples, n_tokens, dim = x.shape # extract shapes, tokens and dimensions from the output of the embeddings\n",
    "        if dim != self.dim:\n",
    "            raise ValueError # raise an error if dim isn't equal to the dimension set in the attention layer\n",
    "        \n",
    "        qkv = self.qkv(x) # Perform the query, key, value projections. (n_samples/batches, n_patches+1, 3*dim), the middle dimension is maintained\n",
    "\n",
    "        # Let us now reshape the qkv tensor to separate the query, key and value\n",
    "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4) # (3, n_samples, n_heads, n_patches+1, head_dim)\n",
    "        # Now extract the query, key and value\n",
    "        q,k,v = qkv[0], qkv[1], qkv[2] # (n_samples, n_heads, n_patches+1, head_dim)\n",
    "        # perform the dot product and scale the dot product\n",
    "        dot_prod = (q @ k.transpose(-2,-1)) * self.scale # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        # apply a softmax\n",
    "        attention = dot_prod.softmax(dim = -1) # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        attention = self.attn_d(attention) # apply dropout for regularization during training\n",
    "        # weighted average\n",
    "        wei = (attention @ v).transpose(1,2) # (n_samples, n_patches+1, n_heads, head_dim)\n",
    "        # flatten\n",
    "        wei = wei.flatten(2) # (n_samples, n_patches+1, dim) as dim = n_heads * head_dim\n",
    "        # we now apply the projection\n",
    "        x = self.proj(wei) # (n_samples, n_patches+1, dim)\n",
    "        x = self.proj_d(x) # apply dropout for regularization during training\n",
    "        return x\n",
    "\n",
    "    # Let us now write the MLP module\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ''' \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features (int)           : embedding dimension, \n",
    "    hidden_features(int)        : dimension of the hidden layer\n",
    "    out_features (int)          : dimension of the hidden layer\n",
    "    dropout (float)     : probability of dropout\n",
    "    \n",
    "    Attributes\n",
    "    __________\n",
    "    fc1 (nn.Linear)     : Linear projection, which are used for performing the attention\n",
    "    fc2 (nn.Linear)     : Takes in the concatenated output of all attention heads and maps it further\n",
    "    dropout (nn.Dropout): Dropout layer\n",
    "\n",
    "    '''\n",
    "    def __init__(self, in_features,hidden_features,out_features, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features) # takes in the input and maps it to the hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features) # takes in the hidden layer and maps it to the output\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.GELU() # we will the GELU activation function in the paper\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # apply the first linear projection, (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.act(x) # apply the activation function (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.fc2(x) # apply the second linear projection (n_samples, n_patches+1, out_features)\n",
    "        x = self.dropout(x) # apply dropout (n_samples, n_patches+1, out_features)\n",
    "        return x\n",
    "\n",
    "# We have everything we need to write the ViT class\n",
    "\n",
    "class Block(nn.Module):\n",
    "    ''' Transformer with Vision Token\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim (int)           : embedding\n",
    "    n_heads (int)       : number of attention heads\n",
    "    mlp_ratio (float)   : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
    "    qkv_bias (bool)     : whether to add a bias to the qkv projection layer\n",
    "    attn_d, proj_d,          : dropout probabilities\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2        :  LayerNorm layers\n",
    "    attn                : Attention layer\n",
    "    mlp                 : MLP layer\n",
    "    '''\n",
    "    def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias=True, attn_d=0., proj_d = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6) # division by zero is prevented and we match the props of the pretrained model\n",
    "        self.attn = Attention(dim, n_heads, qkv_bias, attn_d, proj_d)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps = 1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features = dim, hidden_features = hidden_features, out_features = dim, dropout = proj_d)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x)) # add to the residual highway after performing Layernorm and attention\n",
    "        x = x + self.mlp(self.norm2(x)) # add to the residual highway after performing Layernorm and MLP\n",
    "        return x\n",
    "\n",
    "\n",
    "# now we can write the Vision Transformer class\n",
    "class ViT(nn.Module):\n",
    "    ''' Vision Transformer\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_size (int)            : size of the input image\n",
    "    patch_size (int)            : size of the patches to be extracted from the input image\n",
    "    in_channels (int)           : number of input channels\n",
    "    num_classes (int)           : number of classes\n",
    "    embed_dim (int              : embedding dimension\n",
    "    depth (int)                 : number of transformer blocks\n",
    "    n_heads (int)               : number of attention heads per block\n",
    "    mlp_ratio (float)           : ratio of mlp hidden dim to embedding dim, determines the hidden dimension size of the MLP module\n",
    "    qkv_bias (bool)             : whether to add a bias to the qkv projection layer\n",
    "    attn_d, proj_d,             : dropout probabilities\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed (nn.Conv2d)     : Convolutional embedding layer\n",
    "    pos_embed (nn.Parameter)    : learnable positional embedding\n",
    "    cls_token (nn.Parameter)    : learnable class token\n",
    "    blocks (nn.ModuleList)      : list of transformer blocks\n",
    "    norm (nn.LayerNorm)         : final LayerNorm layer\n",
    "    head (nn.Linear)            : final linear projection layer\n",
    "    '''\n",
    "    # initialize\n",
    "    def __init__(self, \n",
    "                img_size = 384, \n",
    "                patch_size = 16, \n",
    "                in_chans=3, \n",
    "                n_classes = 1000, \n",
    "                embed_dim = 768, \n",
    "                depth = 12,\n",
    "                n_heads = 12,\n",
    "                mlp_ratio = 4.0,\n",
    "                qkv_bias = True,\n",
    "                attn_d = 0.,\n",
    "                proj_d = 0.):\n",
    "        super().__init__()\n",
    "        # we will use the same image size as the pretrained model\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                                        img_size = img_size,\n",
    "                                        patch_size = patch_size,\n",
    "                                        in_chans = in_chans,\n",
    "                                        embed_dim = embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)) # learnable class token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim)) # learnable positional embedding\n",
    "        self.pos_d     = nn.Dropout(p = proj_d) # dropout layer\n",
    "        self.blocks    = nn.ModuleList(\n",
    "                            [\n",
    "                                Block( \n",
    "                                    dim = embed_dim, \n",
    "                                    n_heads = n_heads, \n",
    "                                    mlp_ratio = mlp_ratio, \n",
    "                                    qkv_bias = qkv_bias, \n",
    "                                    attn_d = attn_d, \n",
    "                                    proj_d = proj_d) for _ in range(depth)] # iteratively create the transformer blocks with same parameters\n",
    "                                    )\n",
    "        self.norm       = nn.LayerNorm(embed_dim, eps = 1e-6) # final LayerNorm layer\n",
    "        self.head       = nn.Linear(embed_dim, n_classes) # final linear projection layer    \n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        ''' Forward pass\n",
    "        Parameters\n",
    "        ----------\n",
    "        x (torch.Tensor)            : n_samples X in_chans X img_size X img_size\n",
    "        Returns\n",
    "        -------\n",
    "        logits (torch.Tensor)       : n_samples X n_classes\n",
    "        '''\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x) # extract patches from the input image and turn them into patch embeddings\n",
    "        cls_tokens = self.cls_token.expand(n_samples, -1, -1) # expand the class token to match the batch size\n",
    "        # pre-append the class token to the patch embeddings\n",
    "        x = torch.cat((cls_tokens, x), dim = 1) # n_samples X (n_patches + 1) X embed_dim\n",
    "        x = x + self.pos_embed # add the positional embedding to the patch embeddings\n",
    "        x = self.pos_d(x) # apply dropout to the embeddings\n",
    "        for block in self.blocks: # apply transformer blocks\n",
    "            x = block(x)\n",
    "        x = self.norm(x) # apply LayerNorm to the final output\n",
    "        # the shape of x now is n_samples X (n_patches + 1) X embed_dim\n",
    "        # extract the class token from the output\n",
    "        cls_token = x[:, 0] # n_samples X embed_dim\n",
    "        x = self.head(cls_token) # n_samples X n_classes\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "Total params: 86859496\n"
     ]
    }
   ],
   "source": [
    "# Verify that the model works\n",
    "model = ViT()\n",
    "x = torch.randn(1, 3, 384, 384)\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "print(f'Total params: {sum(p.numel() for p in model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 897418\n",
      "Training set:: input: torch.Size([50000, 3, 32, 32]), output: torch.Size([50000])\n",
      "Test set:: input: torch.Size([10000, 3, 32, 32]), output: torch.Size([10000])\n",
      "step 0: train loss = 2.3166, val loss = 2.3258\n",
      "step 10: train loss = 2.3357, val loss = 2.2501\n",
      "step 20: train loss = 2.3052, val loss = 2.3305\n",
      "step 30: train loss = 2.1434, val loss = 2.2160\n",
      "step 40: train loss = 2.3285, val loss = 2.3135\n",
      "step 50: train loss = 2.2864, val loss = 2.5609\n",
      "step 60: train loss = 2.2960, val loss = 2.2860\n",
      "step 70: train loss = 2.1351, val loss = 2.0985\n",
      "step 80: train loss = 2.1960, val loss = 2.2030\n",
      "step 90: train loss = 1.9264, val loss = 2.0453\n",
      "step 100: train loss = 2.1382, val loss = 2.2116\n",
      "step 110: train loss = 2.0729, val loss = 1.7554\n",
      "step 120: train loss = 2.4334, val loss = 2.1328\n",
      "step 130: train loss = 1.9437, val loss = 2.1130\n",
      "step 140: train loss = 2.1074, val loss = 2.0554\n",
      "step 150: train loss = 2.0565, val loss = 1.9906\n",
      "step 160: train loss = 2.0545, val loss = 2.1612\n",
      "step 170: train loss = 1.8976, val loss = 2.1337\n",
      "step 180: train loss = 2.0219, val loss = 1.8737\n",
      "step 190: train loss = 1.9570, val loss = 2.0770\n",
      "step 200: train loss = 2.0938, val loss = 2.0241\n",
      "step 210: train loss = 2.1940, val loss = 1.8808\n",
      "step 220: train loss = 2.0986, val loss = 2.1605\n",
      "step 230: train loss = 2.0708, val loss = 2.0555\n",
      "step 240: train loss = 1.8456, val loss = 1.7970\n",
      "step 250: train loss = 2.0581, val loss = 1.9733\n",
      "step 260: train loss = 2.3301, val loss = 2.1427\n",
      "step 270: train loss = 1.9724, val loss = 2.1361\n",
      "step 280: train loss = 2.2363, val loss = 1.9469\n",
      "step 290: train loss = 1.9731, val loss = 2.2926\n",
      "step 300: train loss = 2.3742, val loss = 1.9021\n",
      "step 310: train loss = 1.9915, val loss = 1.9832\n",
      "step 320: train loss = 1.9634, val loss = 2.0409\n",
      "step 330: train loss = 1.9733, val loss = 1.8589\n",
      "step 340: train loss = 1.7223, val loss = 2.0674\n",
      "step 350: train loss = 1.9025, val loss = 1.7396\n",
      "step 360: train loss = 1.7655, val loss = 1.7690\n",
      "step 370: train loss = 1.9416, val loss = 1.9160\n",
      "step 380: train loss = 2.1370, val loss = 1.9298\n",
      "step 390: train loss = 2.0156, val loss = 1.9602\n",
      "step 400: train loss = 1.9328, val loss = 1.9447\n",
      "step 410: train loss = 1.9149, val loss = 1.8566\n",
      "step 420: train loss = 1.6825, val loss = 2.0159\n",
      "step 430: train loss = 1.8544, val loss = 1.9883\n",
      "step 440: train loss = 1.7984, val loss = 1.8252\n",
      "step 450: train loss = 1.7263, val loss = 1.9485\n",
      "step 460: train loss = 2.1175, val loss = 2.0691\n",
      "step 470: train loss = 1.9512, val loss = 2.1738\n",
      "step 480: train loss = 1.7446, val loss = 1.7278\n",
      "step 490: train loss = 1.8566, val loss = 2.2291\n",
      "step 500: train loss = 1.8550, val loss = 2.0958\n",
      "step 510: train loss = 1.9140, val loss = 1.7336\n",
      "step 520: train loss = 1.9596, val loss = 1.9990\n",
      "step 530: train loss = 1.8145, val loss = 1.8688\n",
      "step 540: train loss = 1.8282, val loss = 1.5918\n",
      "step 550: train loss = 2.0914, val loss = 1.9513\n",
      "step 560: train loss = 1.6752, val loss = 1.9456\n",
      "step 570: train loss = 1.9742, val loss = 2.0086\n",
      "step 580: train loss = 1.6980, val loss = 1.7377\n",
      "step 590: train loss = 1.8713, val loss = 1.9246\n",
      "step 600: train loss = 1.8404, val loss = 1.9338\n",
      "step 610: train loss = 1.7726, val loss = 1.9889\n",
      "step 620: train loss = 2.1452, val loss = 1.7430\n",
      "step 630: train loss = 2.1456, val loss = 1.9147\n",
      "step 640: train loss = 1.7686, val loss = 1.9984\n",
      "step 650: train loss = 1.7669, val loss = 1.9176\n",
      "step 660: train loss = 1.6296, val loss = 1.7061\n",
      "step 670: train loss = 1.7147, val loss = 1.9548\n",
      "step 680: train loss = 1.7139, val loss = 1.8415\n",
      "step 690: train loss = 1.8846, val loss = 1.7912\n",
      "step 700: train loss = 2.0027, val loss = 1.9164\n",
      "step 710: train loss = 1.7436, val loss = 1.9059\n",
      "step 720: train loss = 1.5955, val loss = 1.7511\n",
      "step 730: train loss = 1.9637, val loss = 1.8530\n",
      "step 740: train loss = 1.7453, val loss = 2.0424\n",
      "step 750: train loss = 1.8190, val loss = 2.0402\n",
      "step 760: train loss = 1.7065, val loss = 1.6613\n",
      "step 770: train loss = 1.9725, val loss = 1.9064\n",
      "step 780: train loss = 1.6914, val loss = 1.8873\n",
      "step 790: train loss = 1.9231, val loss = 1.8990\n",
      "step 800: train loss = 1.8344, val loss = 1.6808\n",
      "step 810: train loss = 1.9482, val loss = 1.8527\n",
      "step 820: train loss = 1.6349, val loss = 1.9513\n",
      "step 830: train loss = 2.0598, val loss = 2.0041\n",
      "step 840: train loss = 1.5586, val loss = 1.7021\n",
      "step 850: train loss = 1.8313, val loss = 1.7600\n",
      "step 860: train loss = 1.6999, val loss = 2.0432\n",
      "step 870: train loss = 1.6320, val loss = 1.9254\n",
      "step 880: train loss = 1.7299, val loss = 1.9007\n",
      "step 890: train loss = 1.7949, val loss = 1.7893\n",
      "step 900: train loss = 2.2287, val loss = 2.1570\n",
      "step 910: train loss = 1.9702, val loss = 1.7473\n",
      "step 920: train loss = 1.8436, val loss = 2.0665\n",
      "step 930: train loss = 1.8166, val loss = 1.9650\n",
      "step 940: train loss = 1.6961, val loss = 1.8076\n",
      "step 950: train loss = 1.7218, val loss = 1.9178\n",
      "step 960: train loss = 1.6504, val loss = 1.6921\n",
      "step 970: train loss = 1.8567, val loss = 1.9715\n",
      "step 980: train loss = 1.7518, val loss = 1.7670\n",
      "step 990: train loss = 1.7939, val loss = 1.6093\n",
      "step 1000: train loss = 1.9674, val loss = 1.7602\n",
      "step 1010: train loss = 1.8896, val loss = 1.7603\n",
      "step 1020: train loss = 1.6010, val loss = 1.6422\n",
      "step 1030: train loss = 1.9538, val loss = 1.9353\n",
      "step 1040: train loss = 1.9777, val loss = 1.9755\n",
      "step 1050: train loss = 1.9697, val loss = 1.8068\n",
      "step 1060: train loss = 1.8091, val loss = 1.8064\n",
      "step 1070: train loss = 1.6857, val loss = 1.8082\n",
      "step 1080: train loss = 1.6531, val loss = 1.6119\n",
      "step 1090: train loss = 1.8872, val loss = 1.9739\n",
      "step 1100: train loss = 1.8729, val loss = 2.1506\n",
      "step 1110: train loss = 1.9157, val loss = 2.0832\n",
      "step 1120: train loss = 1.7589, val loss = 1.6992\n",
      "step 1130: train loss = 1.5483, val loss = 1.8287\n",
      "step 1140: train loss = 2.0332, val loss = 1.5897\n",
      "step 1150: train loss = 1.7148, val loss = 2.0312\n",
      "step 1160: train loss = 1.7065, val loss = 1.8149\n",
      "step 1170: train loss = 2.0604, val loss = 2.3136\n",
      "step 1180: train loss = 1.9889, val loss = 1.6178\n",
      "step 1190: train loss = 1.9096, val loss = 1.9769\n",
      "step 1200: train loss = 1.5322, val loss = 1.6435\n",
      "step 1210: train loss = 1.8233, val loss = 1.6347\n",
      "step 1220: train loss = 2.0118, val loss = 1.7893\n",
      "step 1230: train loss = 1.8846, val loss = 1.8322\n",
      "step 1240: train loss = 1.8483, val loss = 2.1655\n",
      "step 1250: train loss = 2.1164, val loss = 1.9178\n",
      "step 1260: train loss = 1.7956, val loss = 1.8994\n",
      "step 1270: train loss = 1.9892, val loss = 2.0376\n",
      "step 1280: train loss = 1.5734, val loss = 1.9140\n",
      "step 1290: train loss = 1.8140, val loss = 1.7210\n",
      "step 1300: train loss = 1.8971, val loss = 1.8629\n",
      "step 1310: train loss = 1.7733, val loss = 1.8318\n",
      "step 1320: train loss = 1.6577, val loss = 1.6614\n",
      "step 1330: train loss = 1.7182, val loss = 1.7760\n",
      "step 1340: train loss = 1.8410, val loss = 1.8825\n",
      "step 1350: train loss = 1.8086, val loss = 1.8538\n",
      "step 1360: train loss = 1.7789, val loss = 1.8865\n",
      "step 1370: train loss = 1.9909, val loss = 2.0556\n",
      "step 1380: train loss = 1.7399, val loss = 1.6800\n",
      "step 1390: train loss = 1.8918, val loss = 1.7758\n",
      "step 1400: train loss = 1.7967, val loss = 1.8235\n",
      "step 1410: train loss = 1.5682, val loss = 1.6558\n",
      "step 1420: train loss = 1.9408, val loss = 1.3461\n",
      "step 1430: train loss = 1.7877, val loss = 1.9424\n",
      "step 1440: train loss = 1.8261, val loss = 1.7893\n",
      "step 1450: train loss = 1.9953, val loss = 1.6247\n",
      "step 1460: train loss = 2.0106, val loss = 1.9471\n",
      "step 1470: train loss = 1.8966, val loss = 1.7709\n",
      "step 1480: train loss = 2.0499, val loss = 1.5665\n",
      "step 1490: train loss = 1.5885, val loss = 1.6124\n",
      "step 1500: train loss = 1.7437, val loss = 1.6022\n",
      "step 1510: train loss = 1.7617, val loss = 1.6241\n",
      "step 1520: train loss = 1.9674, val loss = 1.8050\n",
      "step 1530: train loss = 1.7650, val loss = 2.0314\n",
      "step 1540: train loss = 2.1088, val loss = 1.7681\n",
      "step 1550: train loss = 1.6851, val loss = 1.6731\n",
      "step 1560: train loss = 1.5323, val loss = 1.9898\n",
      "step 1570: train loss = 1.6498, val loss = 1.9819\n",
      "step 1580: train loss = 1.9011, val loss = 1.6405\n",
      "step 1590: train loss = 1.6474, val loss = 1.7073\n",
      "step 1600: train loss = 1.6232, val loss = 2.1543\n",
      "step 1610: train loss = 1.5463, val loss = 1.7837\n",
      "step 1620: train loss = 1.5652, val loss = 1.6616\n",
      "step 1630: train loss = 1.8390, val loss = 1.5766\n",
      "step 1640: train loss = 1.7278, val loss = 1.5767\n",
      "step 1650: train loss = 1.6971, val loss = 1.8104\n",
      "step 1660: train loss = 1.8672, val loss = 1.6428\n",
      "step 1670: train loss = 2.0990, val loss = 1.7041\n",
      "step 1680: train loss = 1.7734, val loss = 1.6877\n",
      "step 1690: train loss = 1.7089, val loss = 1.6126\n",
      "step 1700: train loss = 1.9838, val loss = 1.8003\n",
      "step 1710: train loss = 2.0704, val loss = 1.7111\n",
      "step 1720: train loss = 1.7406, val loss = 1.7456\n",
      "step 1730: train loss = 1.5651, val loss = 1.7263\n",
      "step 1740: train loss = 1.7074, val loss = 1.5055\n",
      "step 1750: train loss = 2.0777, val loss = 1.7041\n",
      "step 1760: train loss = 1.9115, val loss = 1.7960\n",
      "step 1770: train loss = 1.8460, val loss = 1.8926\n",
      "step 1780: train loss = 1.8016, val loss = 1.7349\n",
      "step 1790: train loss = 2.0567, val loss = 1.7010\n",
      "step 1800: train loss = 2.1098, val loss = 1.5273\n",
      "step 1810: train loss = 1.4706, val loss = 1.5635\n",
      "step 1820: train loss = 1.8912, val loss = 1.7340\n",
      "step 1830: train loss = 1.7147, val loss = 1.7711\n",
      "step 1840: train loss = 1.6927, val loss = 1.6845\n",
      "step 1850: train loss = 1.6736, val loss = 2.1744\n",
      "step 1860: train loss = 1.8380, val loss = 1.7492\n",
      "step 1870: train loss = 1.3684, val loss = 1.6687\n",
      "step 1880: train loss = 1.6733, val loss = 1.7737\n",
      "step 1890: train loss = 1.9358, val loss = 1.7368\n",
      "step 1900: train loss = 1.9015, val loss = 1.6539\n",
      "step 1910: train loss = 1.8564, val loss = 1.6116\n",
      "step 1920: train loss = 1.6981, val loss = 1.7298\n",
      "step 1930: train loss = 1.8142, val loss = 1.7467\n",
      "step 1940: train loss = 1.7506, val loss = 1.8185\n",
      "step 1950: train loss = 1.5178, val loss = 1.8506\n",
      "step 1960: train loss = 1.9859, val loss = 1.6101\n",
      "step 1970: train loss = 1.9476, val loss = 1.9096\n",
      "step 1980: train loss = 1.6891, val loss = 2.0375\n",
      "step 1990: train loss = 1.9780, val loss = 1.8902\n",
      "step 2000: train loss = 1.5814, val loss = 1.8301\n",
      "step 2010: train loss = 1.5408, val loss = 1.9334\n",
      "step 2020: train loss = 1.8425, val loss = 1.5875\n",
      "step 2030: train loss = 1.5640, val loss = 1.7223\n",
      "step 2040: train loss = 1.7895, val loss = 1.7693\n",
      "step 2050: train loss = 1.8368, val loss = 1.5283\n",
      "step 2060: train loss = 1.5716, val loss = 1.7588\n",
      "step 2070: train loss = 1.9255, val loss = 1.5179\n",
      "step 2080: train loss = 1.7338, val loss = 1.7397\n",
      "step 2090: train loss = 1.7249, val loss = 1.7686\n",
      "step 2100: train loss = 1.7372, val loss = 1.6372\n",
      "step 2110: train loss = 2.0923, val loss = 1.7331\n",
      "step 2120: train loss = 1.5545, val loss = 1.7652\n",
      "step 2130: train loss = 1.6414, val loss = 1.8431\n",
      "step 2140: train loss = 1.5595, val loss = 2.0054\n",
      "step 2150: train loss = 1.6524, val loss = 1.8965\n",
      "step 2160: train loss = 1.7737, val loss = 2.0182\n",
      "step 2170: train loss = 1.5749, val loss = 1.7092\n",
      "step 2180: train loss = 1.8164, val loss = 2.1774\n",
      "step 2190: train loss = 1.7548, val loss = 1.7257\n",
      "step 2200: train loss = 1.7713, val loss = 1.7188\n",
      "step 2210: train loss = 1.7374, val loss = 1.8962\n",
      "step 2220: train loss = 1.6995, val loss = 1.6562\n",
      "step 2230: train loss = 1.5143, val loss = 1.5518\n",
      "step 2240: train loss = 1.5251, val loss = 1.7577\n",
      "step 2250: train loss = 1.3541, val loss = 1.6506\n",
      "step 2260: train loss = 1.6115, val loss = 1.8199\n",
      "step 2270: train loss = 1.7700, val loss = 1.7413\n",
      "step 2280: train loss = 1.9218, val loss = 1.6677\n",
      "step 2290: train loss = 1.6784, val loss = 1.6803\n",
      "step 2300: train loss = 1.7886, val loss = 1.8777\n",
      "step 2310: train loss = 1.7082, val loss = 2.1229\n",
      "step 2320: train loss = 1.8230, val loss = 1.5739\n",
      "step 2330: train loss = 1.6271, val loss = 1.8712\n",
      "step 2340: train loss = 1.7403, val loss = 1.6538\n",
      "step 2350: train loss = 1.7772, val loss = 1.9176\n",
      "step 2360: train loss = 1.5717, val loss = 1.6717\n",
      "step 2370: train loss = 1.6809, val loss = 1.5638\n",
      "step 2380: train loss = 1.7813, val loss = 1.7187\n",
      "step 2390: train loss = 2.0068, val loss = 1.5865\n",
      "step 2400: train loss = 1.7661, val loss = 2.0377\n",
      "step 2410: train loss = 1.6289, val loss = 1.6162\n",
      "step 2420: train loss = 1.6668, val loss = 1.6678\n",
      "step 2430: train loss = 1.8900, val loss = 1.7517\n",
      "step 2440: train loss = 1.8561, val loss = 1.6005\n",
      "step 2450: train loss = 1.7331, val loss = 1.5943\n",
      "step 2460: train loss = 1.5264, val loss = 1.6594\n",
      "step 2470: train loss = 1.7463, val loss = 1.5086\n",
      "step 2480: train loss = 1.8638, val loss = 1.6935\n",
      "step 2490: train loss = 1.6856, val loss = 1.7069\n",
      "step 2500: train loss = 1.4396, val loss = 1.5759\n",
      "step 2510: train loss = 1.7443, val loss = 1.7584\n",
      "step 2520: train loss = 1.7755, val loss = 1.9348\n",
      "step 2530: train loss = 1.6323, val loss = 1.8859\n",
      "step 2540: train loss = 1.4725, val loss = 1.6738\n",
      "step 2550: train loss = 1.7571, val loss = 1.8599\n",
      "step 2560: train loss = 1.6705, val loss = 1.5345\n",
      "step 2570: train loss = 1.9257, val loss = 1.6526\n",
      "step 2580: train loss = 1.7596, val loss = 1.9093\n",
      "step 2590: train loss = 1.6091, val loss = 1.5283\n",
      "step 2600: train loss = 1.8492, val loss = 1.7820\n",
      "step 2610: train loss = 2.0257, val loss = 1.9218\n",
      "step 2620: train loss = 1.7401, val loss = 1.5371\n",
      "step 2630: train loss = 1.5078, val loss = 1.8036\n",
      "step 2640: train loss = 1.6465, val loss = 1.9218\n",
      "step 2650: train loss = 1.7961, val loss = 1.8489\n",
      "step 2660: train loss = 1.8990, val loss = 2.0088\n",
      "step 2670: train loss = 1.6580, val loss = 1.8162\n",
      "step 2680: train loss = 1.7305, val loss = 1.5563\n",
      "step 2690: train loss = 1.8545, val loss = 1.9428\n",
      "step 2700: train loss = 1.9911, val loss = 1.7909\n",
      "step 2710: train loss = 1.8461, val loss = 1.7079\n",
      "step 2720: train loss = 1.8724, val loss = 1.6303\n",
      "step 2730: train loss = 1.6938, val loss = 1.8711\n",
      "step 2740: train loss = 1.6819, val loss = 1.5465\n",
      "step 2750: train loss = 1.6481, val loss = 1.8456\n",
      "step 2760: train loss = 1.7896, val loss = 1.8086\n",
      "step 2770: train loss = 1.6043, val loss = 2.0246\n",
      "step 2780: train loss = 1.6430, val loss = 1.6699\n",
      "step 2790: train loss = 1.8132, val loss = 1.7503\n",
      "step 2800: train loss = 1.5749, val loss = 1.7842\n",
      "step 2810: train loss = 1.5214, val loss = 1.5503\n",
      "step 2820: train loss = 1.7839, val loss = 1.5014\n",
      "step 2830: train loss = 1.5863, val loss = 1.7601\n",
      "step 2840: train loss = 2.1786, val loss = 1.7742\n",
      "step 2850: train loss = 1.5147, val loss = 2.3259\n",
      "step 2860: train loss = 1.5898, val loss = 1.6789\n",
      "step 2870: train loss = 1.7372, val loss = 1.7277\n",
      "step 2880: train loss = 1.6922, val loss = 1.7780\n",
      "step 2890: train loss = 1.8843, val loss = 1.5086\n",
      "step 2900: train loss = 1.5084, val loss = 1.9079\n",
      "step 2910: train loss = 1.6315, val loss = 1.8796\n",
      "step 2920: train loss = 1.5879, val loss = 1.6329\n",
      "step 2930: train loss = 1.9495, val loss = 1.8550\n",
      "step 2940: train loss = 1.5927, val loss = 1.9417\n",
      "step 2950: train loss = 1.6978, val loss = 1.7195\n",
      "step 2960: train loss = 1.7592, val loss = 1.6922\n",
      "step 2970: train loss = 1.7803, val loss = 1.6514\n",
      "step 2980: train loss = 1.8664, val loss = 1.7835\n",
      "step 2990: train loss = 1.3948, val loss = 1.7573\n",
      "step 3000: train loss = 1.5897, val loss = 1.6109\n",
      "step 3010: train loss = 1.6730, val loss = 1.8244\n",
      "step 3020: train loss = 1.8745, val loss = 1.4599\n",
      "step 3030: train loss = 1.8644, val loss = 2.0773\n",
      "step 3040: train loss = 1.5046, val loss = 2.2072\n",
      "step 3050: train loss = 1.7524, val loss = 1.5657\n",
      "step 3060: train loss = 1.8337, val loss = 2.0742\n",
      "step 3070: train loss = 1.8635, val loss = 1.6473\n",
      "step 3080: train loss = 1.6419, val loss = 1.5793\n",
      "step 3090: train loss = 1.6694, val loss = 1.6884\n",
      "step 3100: train loss = 1.6993, val loss = 1.6681\n",
      "step 3110: train loss = 1.5141, val loss = 1.7957\n",
      "step 3120: train loss = 2.2034, val loss = 1.4113\n",
      "step 3130: train loss = 1.3464, val loss = 1.8204\n",
      "step 3140: train loss = 1.5087, val loss = 1.6748\n",
      "step 3150: train loss = 1.5301, val loss = 1.5678\n",
      "step 3160: train loss = 1.5307, val loss = 1.8034\n",
      "step 3170: train loss = 1.6415, val loss = 1.5303\n",
      "step 3180: train loss = 1.6792, val loss = 1.5976\n",
      "step 3190: train loss = 1.5380, val loss = 1.7560\n",
      "step 3200: train loss = 1.8638, val loss = 1.7977\n",
      "step 3210: train loss = 1.7637, val loss = 1.6148\n",
      "step 3220: train loss = 1.5198, val loss = 1.7657\n",
      "step 3230: train loss = 2.0051, val loss = 1.6721\n",
      "step 3240: train loss = 1.6718, val loss = 1.9436\n",
      "step 3250: train loss = 1.7483, val loss = 1.4950\n",
      "step 3260: train loss = 1.8922, val loss = 1.6156\n",
      "step 3270: train loss = 1.4941, val loss = 1.6076\n",
      "step 3280: train loss = 2.2984, val loss = 1.7956\n",
      "step 3290: train loss = 1.5965, val loss = 1.7456\n",
      "step 3300: train loss = 1.7426, val loss = 1.7058\n",
      "step 3310: train loss = 1.7791, val loss = 1.8166\n",
      "step 3320: train loss = 1.7350, val loss = 1.5671\n",
      "step 3330: train loss = 1.7663, val loss = 2.1810\n",
      "step 3340: train loss = 1.8731, val loss = 1.8312\n",
      "step 3350: train loss = 1.8471, val loss = 1.7919\n",
      "step 3360: train loss = 1.4777, val loss = 1.7200\n",
      "step 3370: train loss = 1.7493, val loss = 1.6702\n",
      "step 3380: train loss = 1.5511, val loss = 1.4979\n",
      "step 3390: train loss = 1.4924, val loss = 1.5055\n",
      "step 3400: train loss = 1.6518, val loss = 1.6122\n",
      "step 3410: train loss = 1.6382, val loss = 1.5328\n",
      "step 3420: train loss = 1.5292, val loss = 1.9996\n",
      "step 3430: train loss = 1.6317, val loss = 1.6153\n",
      "step 3440: train loss = 1.7079, val loss = 1.9493\n",
      "step 3450: train loss = 1.5286, val loss = 1.7445\n",
      "step 3460: train loss = 1.5246, val loss = 1.7385\n",
      "step 3470: train loss = 1.5404, val loss = 1.6358\n",
      "step 3480: train loss = 1.7576, val loss = 1.6612\n",
      "step 3490: train loss = 1.6584, val loss = 1.5705\n",
      "step 3500: train loss = 1.6200, val loss = 1.5426\n",
      "step 3510: train loss = 1.9391, val loss = 1.7703\n",
      "step 3520: train loss = 1.2728, val loss = 2.0482\n",
      "step 3530: train loss = 1.6382, val loss = 1.7988\n",
      "step 3540: train loss = 1.5967, val loss = 1.7475\n",
      "step 3550: train loss = 1.7239, val loss = 1.8209\n",
      "step 3560: train loss = 1.6186, val loss = 1.7130\n",
      "step 3570: train loss = 1.5033, val loss = 1.7532\n",
      "step 3580: train loss = 1.4037, val loss = 1.6282\n",
      "step 3590: train loss = 1.6474, val loss = 2.0652\n",
      "step 3600: train loss = 1.4822, val loss = 1.8746\n",
      "step 3610: train loss = 2.2289, val loss = 1.7213\n",
      "step 3620: train loss = 1.5119, val loss = 1.6487\n",
      "step 3630: train loss = 1.6385, val loss = 1.5922\n",
      "step 3640: train loss = 1.7772, val loss = 1.6679\n",
      "step 3650: train loss = 1.6434, val loss = 1.4501\n",
      "step 3660: train loss = 1.8563, val loss = 1.7954\n",
      "step 3670: train loss = 1.4377, val loss = 1.8949\n",
      "step 3680: train loss = 1.6544, val loss = 1.5380\n",
      "step 3690: train loss = 1.7818, val loss = 1.5341\n",
      "step 3700: train loss = 1.8862, val loss = 1.6692\n",
      "step 3710: train loss = 1.5361, val loss = 1.6919\n",
      "step 3720: train loss = 1.5132, val loss = 1.5956\n",
      "step 3730: train loss = 1.4690, val loss = 1.8925\n",
      "step 3740: train loss = 1.6509, val loss = 1.6686\n",
      "step 3750: train loss = 1.5219, val loss = 1.7015\n",
      "step 3760: train loss = 1.7790, val loss = 1.5781\n",
      "step 3770: train loss = 1.7438, val loss = 1.5971\n",
      "step 3780: train loss = 1.6372, val loss = 1.9129\n",
      "step 3790: train loss = 1.7288, val loss = 1.5906\n",
      "step 3800: train loss = 1.7315, val loss = 1.7190\n",
      "step 3810: train loss = 1.6409, val loss = 1.8418\n",
      "step 3820: train loss = 1.6916, val loss = 1.5021\n",
      "step 3830: train loss = 1.4937, val loss = 1.8643\n",
      "step 3840: train loss = 1.5881, val loss = 1.8035\n",
      "step 3850: train loss = 1.8205, val loss = 1.8262\n",
      "step 3860: train loss = 1.4359, val loss = 2.0702\n",
      "step 3870: train loss = 1.6600, val loss = 1.7091\n",
      "step 3880: train loss = 1.3274, val loss = 1.7166\n",
      "step 3890: train loss = 1.6989, val loss = 1.5997\n",
      "step 3900: train loss = 1.5144, val loss = 1.9437\n",
      "step 3910: train loss = 1.3709, val loss = 1.7587\n",
      "step 3920: train loss = 1.4666, val loss = 1.3598\n",
      "step 3930: train loss = 1.7224, val loss = 1.4890\n",
      "step 3940: train loss = 1.6409, val loss = 1.5333\n",
      "step 3950: train loss = 1.6872, val loss = 1.2922\n",
      "step 3960: train loss = 1.5791, val loss = 1.9235\n",
      "step 3970: train loss = 1.6109, val loss = 1.6160\n",
      "step 3980: train loss = 1.2752, val loss = 1.5420\n",
      "step 3990: train loss = 1.6424, val loss = 1.7442\n",
      "step 4000: train loss = 1.6646, val loss = 1.7799\n",
      "step 4010: train loss = 1.4611, val loss = 1.6272\n",
      "step 4020: train loss = 1.7443, val loss = 1.8101\n",
      "step 4030: train loss = 1.6670, val loss = 1.6469\n",
      "step 4040: train loss = 1.6010, val loss = 2.0498\n",
      "step 4050: train loss = 1.8651, val loss = 1.7688\n",
      "step 4060: train loss = 1.8611, val loss = 1.7767\n",
      "step 4070: train loss = 1.8106, val loss = 1.6992\n",
      "step 4080: train loss = 1.5318, val loss = 1.6724\n",
      "step 4090: train loss = 1.7880, val loss = 1.6380\n",
      "step 4100: train loss = 1.6231, val loss = 1.6901\n",
      "step 4110: train loss = 1.6661, val loss = 1.5746\n",
      "step 4120: train loss = 1.5731, val loss = 1.9278\n",
      "step 4130: train loss = 1.5246, val loss = 2.0568\n",
      "step 4140: train loss = 1.6308, val loss = 1.6116\n",
      "step 4150: train loss = 1.7960, val loss = 1.6905\n",
      "step 4160: train loss = 1.6645, val loss = 1.7010\n",
      "step 4170: train loss = 1.3687, val loss = 1.7015\n",
      "step 4180: train loss = 1.5160, val loss = 1.8025\n",
      "step 4190: train loss = 1.7758, val loss = 1.7732\n",
      "step 4200: train loss = 1.7513, val loss = 1.8685\n",
      "step 4210: train loss = 1.8387, val loss = 1.7844\n",
      "step 4220: train loss = 1.8586, val loss = 1.6116\n",
      "step 4230: train loss = 1.4489, val loss = 1.4612\n",
      "step 4240: train loss = 1.5125, val loss = 1.7642\n",
      "step 4250: train loss = 1.7175, val loss = 1.6907\n",
      "step 4260: train loss = 1.5410, val loss = 1.9027\n",
      "step 4270: train loss = 1.5412, val loss = 1.6362\n",
      "step 4280: train loss = 1.4481, val loss = 1.7455\n",
      "step 4290: train loss = 1.8355, val loss = 1.7262\n",
      "step 4300: train loss = 1.5601, val loss = 1.4056\n",
      "step 4310: train loss = 1.6636, val loss = 1.6874\n",
      "step 4320: train loss = 1.7049, val loss = 1.5929\n",
      "step 4330: train loss = 2.0102, val loss = 1.7799\n",
      "step 4340: train loss = 1.7420, val loss = 1.8965\n",
      "step 4350: train loss = 1.7989, val loss = 1.8600\n",
      "step 4360: train loss = 1.6137, val loss = 1.7247\n",
      "step 4370: train loss = 1.4741, val loss = 1.6444\n",
      "step 4380: train loss = 1.8460, val loss = 1.6477\n",
      "step 4390: train loss = 1.5251, val loss = 1.7442\n",
      "step 4400: train loss = 1.6016, val loss = 1.5160\n",
      "step 4410: train loss = 1.5877, val loss = 1.9354\n",
      "step 4420: train loss = 1.5844, val loss = 1.8354\n",
      "step 4430: train loss = 1.6673, val loss = 1.4594\n",
      "step 4440: train loss = 1.6526, val loss = 1.5623\n",
      "step 4450: train loss = 1.3000, val loss = 1.8322\n",
      "step 4460: train loss = 1.8327, val loss = 1.8453\n",
      "step 4470: train loss = 1.6471, val loss = 1.5262\n",
      "step 4480: train loss = 1.7265, val loss = 1.8554\n",
      "step 4490: train loss = 1.6110, val loss = 1.5572\n",
      "step 4500: train loss = 1.6502, val loss = 1.4658\n",
      "step 4510: train loss = 1.7217, val loss = 1.9255\n",
      "step 4520: train loss = 1.8657, val loss = 1.7062\n",
      "step 4530: train loss = 1.8648, val loss = 1.6585\n",
      "step 4540: train loss = 1.6597, val loss = 1.7737\n",
      "step 4550: train loss = 1.5465, val loss = 1.2444\n",
      "step 4560: train loss = 1.7623, val loss = 1.5303\n",
      "step 4570: train loss = 1.5255, val loss = 1.4473\n",
      "step 4580: train loss = 1.4821, val loss = 1.5875\n",
      "step 4590: train loss = 1.5258, val loss = 1.6308\n",
      "step 4600: train loss = 1.7701, val loss = 1.8020\n",
      "step 4610: train loss = 1.4981, val loss = 1.6596\n",
      "step 4620: train loss = 1.4414, val loss = 1.7634\n",
      "step 4630: train loss = 1.6610, val loss = 1.3934\n",
      "step 4640: train loss = 1.5783, val loss = 1.3220\n",
      "step 4650: train loss = 1.4850, val loss = 1.7690\n",
      "step 4660: train loss = 1.4449, val loss = 1.5497\n",
      "step 4670: train loss = 1.8864, val loss = 1.6523\n",
      "step 4680: train loss = 1.5380, val loss = 1.7365\n",
      "step 4690: train loss = 1.5975, val loss = 1.4894\n",
      "step 4700: train loss = 1.7588, val loss = 1.7351\n",
      "step 4710: train loss = 1.5016, val loss = 1.6993\n",
      "step 4720: train loss = 1.8234, val loss = 1.7027\n",
      "step 4730: train loss = 1.5223, val loss = 2.1354\n",
      "step 4740: train loss = 1.9536, val loss = 1.6209\n",
      "step 4750: train loss = 1.3795, val loss = 1.4730\n",
      "step 4760: train loss = 1.7589, val loss = 1.8482\n",
      "step 4770: train loss = 1.5712, val loss = 1.5291\n",
      "step 4780: train loss = 1.6632, val loss = 1.4253\n",
      "step 4790: train loss = 1.6434, val loss = 1.6980\n",
      "step 4800: train loss = 1.4151, val loss = 1.7554\n",
      "step 4810: train loss = 1.5118, val loss = 1.8568\n",
      "step 4820: train loss = 1.7827, val loss = 1.6807\n",
      "step 4830: train loss = 1.6747, val loss = 1.6628\n",
      "step 4840: train loss = 1.6802, val loss = 1.4897\n",
      "step 4850: train loss = 1.5693, val loss = 1.4224\n",
      "step 4860: train loss = 1.7216, val loss = 1.4599\n",
      "step 4870: train loss = 1.7001, val loss = 1.6081\n",
      "step 4880: train loss = 1.6351, val loss = 1.7584\n",
      "step 4890: train loss = 1.4648, val loss = 1.7139\n",
      "step 4900: train loss = 1.9651, val loss = 1.9435\n",
      "step 4910: train loss = 1.5924, val loss = 1.6818\n",
      "step 4920: train loss = 1.4732, val loss = 1.7999\n",
      "step 4930: train loss = 1.7160, val loss = 1.4159\n",
      "step 4940: train loss = 1.4717, val loss = 1.6714\n",
      "step 4950: train loss = 1.8354, val loss = 1.5394\n",
      "step 4960: train loss = 1.4518, val loss = 1.8145\n",
      "step 4970: train loss = 1.7014, val loss = 1.6874\n",
      "step 4980: train loss = 1.4217, val loss = 1.6919\n",
      "step 4990: train loss = 1.7562, val loss = 1.5153\n",
      "step 5000: train loss = 1.5754, val loss = 1.6153\n",
      "step 5010: train loss = 1.5243, val loss = 1.6792\n",
      "step 5020: train loss = 1.7003, val loss = 1.9237\n",
      "step 5030: train loss = 1.6512, val loss = 2.1318\n",
      "step 5040: train loss = 1.4286, val loss = 1.4639\n",
      "step 5050: train loss = 1.5693, val loss = 1.3992\n",
      "step 5060: train loss = 1.7510, val loss = 1.6502\n",
      "step 5070: train loss = 1.6612, val loss = 2.0746\n",
      "step 5080: train loss = 1.6496, val loss = 2.1512\n",
      "step 5090: train loss = 1.6443, val loss = 1.6842\n",
      "step 5100: train loss = 1.7553, val loss = 1.6624\n",
      "step 5110: train loss = 1.7091, val loss = 1.8420\n",
      "step 5120: train loss = 1.7284, val loss = 1.7387\n",
      "step 5130: train loss = 1.4004, val loss = 1.7551\n",
      "step 5140: train loss = 1.6224, val loss = 1.3398\n",
      "step 5150: train loss = 1.5682, val loss = 1.4370\n",
      "step 5160: train loss = 1.7242, val loss = 1.9005\n",
      "step 5170: train loss = 1.7112, val loss = 1.6116\n",
      "step 5180: train loss = 1.7982, val loss = 1.4974\n",
      "step 5190: train loss = 1.8773, val loss = 1.6776\n",
      "step 5200: train loss = 1.6538, val loss = 1.4896\n",
      "step 5210: train loss = 1.5284, val loss = 1.6306\n",
      "step 5220: train loss = 1.4684, val loss = 1.7318\n",
      "step 5230: train loss = 1.4730, val loss = 1.9124\n",
      "step 5240: train loss = 1.3512, val loss = 1.6906\n",
      "step 5250: train loss = 1.3607, val loss = 1.4851\n",
      "step 5260: train loss = 1.8195, val loss = 1.7621\n",
      "step 5270: train loss = 1.4730, val loss = 1.9303\n",
      "step 5280: train loss = 1.4574, val loss = 1.7950\n",
      "step 5290: train loss = 1.5094, val loss = 1.7006\n",
      "step 5300: train loss = 1.6733, val loss = 1.8024\n",
      "step 5310: train loss = 1.5840, val loss = 1.5342\n",
      "step 5320: train loss = 1.5102, val loss = 1.7918\n",
      "step 5330: train loss = 1.5576, val loss = 1.5875\n",
      "step 5340: train loss = 1.5065, val loss = 1.8956\n",
      "step 5350: train loss = 1.6432, val loss = 1.7684\n",
      "step 5360: train loss = 1.7124, val loss = 1.6326\n",
      "step 5370: train loss = 1.3875, val loss = 1.8645\n",
      "step 5380: train loss = 1.5098, val loss = 1.7202\n",
      "step 5390: train loss = 1.6472, val loss = 1.4987\n",
      "step 5400: train loss = 1.9798, val loss = 1.8645\n",
      "step 5410: train loss = 1.5332, val loss = 1.5701\n",
      "step 5420: train loss = 1.7735, val loss = 1.8269\n",
      "step 5430: train loss = 1.7872, val loss = 1.6664\n",
      "step 5440: train loss = 1.4204, val loss = 1.9567\n",
      "step 5450: train loss = 1.2587, val loss = 1.6754\n",
      "step 5460: train loss = 1.8887, val loss = 1.6711\n",
      "step 5470: train loss = 1.6017, val loss = 1.8806\n",
      "step 5480: train loss = 2.1571, val loss = 1.4424\n",
      "step 5490: train loss = 1.8051, val loss = 1.7420\n",
      "step 5500: train loss = 1.4765, val loss = 1.7476\n",
      "step 5510: train loss = 1.4953, val loss = 1.9459\n",
      "step 5520: train loss = 1.8834, val loss = 1.9689\n",
      "step 5530: train loss = 1.6440, val loss = 1.3051\n",
      "step 5540: train loss = 1.7146, val loss = 1.4290\n",
      "step 5550: train loss = 1.7272, val loss = 1.4704\n",
      "step 5560: train loss = 1.6673, val loss = 1.9301\n",
      "step 5570: train loss = 1.5746, val loss = 1.5355\n",
      "step 5580: train loss = 1.3802, val loss = 1.7104\n",
      "step 5590: train loss = 1.7316, val loss = 1.5562\n",
      "step 5600: train loss = 1.6507, val loss = 1.8232\n",
      "step 5610: train loss = 1.9744, val loss = 1.6160\n",
      "step 5620: train loss = 1.5478, val loss = 1.5057\n",
      "step 5630: train loss = 1.7969, val loss = 1.9169\n",
      "step 5640: train loss = 1.5704, val loss = 2.1001\n",
      "step 5650: train loss = 1.4818, val loss = 1.5216\n",
      "step 5660: train loss = 1.7851, val loss = 1.6498\n",
      "step 5670: train loss = 1.5114, val loss = 1.8740\n",
      "step 5680: train loss = 1.7168, val loss = 1.7416\n",
      "step 5690: train loss = 1.4368, val loss = 1.6992\n",
      "step 5700: train loss = 1.6647, val loss = 1.5307\n",
      "step 5710: train loss = 1.7312, val loss = 2.2408\n",
      "step 5720: train loss = 1.6601, val loss = 1.5375\n",
      "step 5730: train loss = 1.7085, val loss = 1.5697\n",
      "step 5740: train loss = 1.4621, val loss = 1.3238\n",
      "step 5750: train loss = 1.4469, val loss = 1.8966\n",
      "step 5760: train loss = 1.7120, val loss = 1.6633\n",
      "step 5770: train loss = 1.7519, val loss = 1.5096\n",
      "step 5780: train loss = 1.6643, val loss = 1.6484\n",
      "step 5790: train loss = 1.8399, val loss = 1.5623\n",
      "step 5800: train loss = 1.4879, val loss = 1.4809\n",
      "step 5810: train loss = 1.5738, val loss = 1.5556\n",
      "step 5820: train loss = 1.7357, val loss = 1.7035\n",
      "step 5830: train loss = 1.4629, val loss = 1.9179\n",
      "step 5840: train loss = 1.4042, val loss = 1.4165\n",
      "step 5850: train loss = 1.3744, val loss = 1.4257\n",
      "step 5860: train loss = 1.5293, val loss = 1.2505\n",
      "step 5870: train loss = 1.6433, val loss = 1.4820\n",
      "step 5880: train loss = 1.7921, val loss = 1.3863\n",
      "step 5890: train loss = 1.3635, val loss = 1.3561\n",
      "step 5900: train loss = 1.4953, val loss = 1.5151\n",
      "step 5910: train loss = 1.7900, val loss = 2.0626\n",
      "step 5920: train loss = 1.4526, val loss = 1.2892\n",
      "step 5930: train loss = 1.4716, val loss = 1.6566\n",
      "step 5940: train loss = 1.5324, val loss = 1.8656\n",
      "step 5950: train loss = 1.8270, val loss = 1.5739\n",
      "step 5960: train loss = 1.5050, val loss = 1.8163\n",
      "step 5970: train loss = 1.5025, val loss = 1.6998\n",
      "step 5980: train loss = 1.7810, val loss = 1.6669\n",
      "step 5990: train loss = 1.5902, val loss = 1.6396\n",
      "step 6000: train loss = 1.8052, val loss = 1.5254\n",
      "step 6010: train loss = 1.6968, val loss = 1.8244\n",
      "step 6020: train loss = 1.6899, val loss = 1.8562\n",
      "step 6030: train loss = 1.6267, val loss = 1.5280\n",
      "step 6040: train loss = 1.5561, val loss = 1.8197\n",
      "step 6050: train loss = 1.6574, val loss = 1.4256\n",
      "step 6060: train loss = 1.4668, val loss = 1.3326\n",
      "step 6070: train loss = 1.8309, val loss = 1.6347\n",
      "step 6080: train loss = 1.5077, val loss = 1.7889\n",
      "step 6090: train loss = 1.5458, val loss = 1.4168\n",
      "step 6100: train loss = 1.6023, val loss = 1.4866\n",
      "step 6110: train loss = 1.5511, val loss = 1.6523\n",
      "step 6120: train loss = 1.6916, val loss = 1.8564\n",
      "step 6130: train loss = 1.5520, val loss = 1.5641\n",
      "step 6140: train loss = 1.4516, val loss = 1.7642\n",
      "step 6150: train loss = 1.5829, val loss = 1.3333\n",
      "step 6160: train loss = 1.4822, val loss = 1.8616\n",
      "step 6170: train loss = 1.5547, val loss = 1.7404\n",
      "step 6180: train loss = 1.3655, val loss = 1.8656\n",
      "step 6190: train loss = 1.5269, val loss = 1.5046\n",
      "step 6200: train loss = 1.6673, val loss = 1.9092\n",
      "step 6210: train loss = 1.8812, val loss = 1.7500\n",
      "step 6220: train loss = 1.5556, val loss = 1.4996\n",
      "step 6230: train loss = 1.5039, val loss = 1.5467\n",
      "step 6240: train loss = 1.7120, val loss = 1.8981\n",
      "step 6250: train loss = 1.6870, val loss = 1.8154\n",
      "step 6260: train loss = 1.7158, val loss = 1.6061\n",
      "step 6270: train loss = 1.7757, val loss = 1.2074\n",
      "step 6280: train loss = 1.6410, val loss = 1.4441\n",
      "step 6290: train loss = 1.4634, val loss = 1.8524\n",
      "step 6300: train loss = 1.8037, val loss = 1.7342\n",
      "step 6310: train loss = 1.6806, val loss = 1.6907\n",
      "step 6320: train loss = 1.5171, val loss = 1.4835\n",
      "step 6330: train loss = 1.2863, val loss = 1.4932\n",
      "step 6340: train loss = 1.6314, val loss = 1.6002\n",
      "step 6350: train loss = 1.5435, val loss = 1.7790\n",
      "step 6360: train loss = 1.7608, val loss = 1.6611\n",
      "step 6370: train loss = 1.4910, val loss = 1.8982\n",
      "step 6380: train loss = 1.9823, val loss = 1.6922\n",
      "step 6390: train loss = 1.6384, val loss = 1.6227\n",
      "step 6400: train loss = 1.3788, val loss = 1.5195\n",
      "step 6410: train loss = 1.2851, val loss = 1.7346\n",
      "step 6420: train loss = 1.4903, val loss = 1.6135\n",
      "step 6430: train loss = 1.7192, val loss = 1.1106\n",
      "step 6440: train loss = 1.8453, val loss = 1.8588\n",
      "step 6450: train loss = 1.7563, val loss = 1.5806\n",
      "step 6460: train loss = 1.7253, val loss = 1.5063\n",
      "step 6470: train loss = 1.3972, val loss = 1.7818\n",
      "step 6480: train loss = 1.4357, val loss = 1.5019\n",
      "step 6490: train loss = 1.2886, val loss = 1.7142\n",
      "step 6500: train loss = 1.5514, val loss = 1.4800\n",
      "step 6510: train loss = 1.7817, val loss = 1.8187\n",
      "step 6520: train loss = 1.4367, val loss = 1.6122\n",
      "step 6530: train loss = 1.6512, val loss = 1.4432\n",
      "step 6540: train loss = 1.6357, val loss = 1.3147\n",
      "step 6550: train loss = 1.6210, val loss = 1.5561\n",
      "step 6560: train loss = 1.7506, val loss = 1.6047\n",
      "step 6570: train loss = 1.7475, val loss = 1.4914\n",
      "step 6580: train loss = 1.3877, val loss = 1.4112\n",
      "step 6590: train loss = 1.4487, val loss = 1.3770\n",
      "step 6600: train loss = 1.6823, val loss = 1.6409\n",
      "step 6610: train loss = 2.0309, val loss = 1.7891\n",
      "step 6620: train loss = 1.6971, val loss = 1.5343\n",
      "step 6630: train loss = 1.3636, val loss = 1.7363\n",
      "step 6640: train loss = 1.4235, val loss = 1.6998\n",
      "step 6650: train loss = 1.7662, val loss = 2.1041\n",
      "step 6660: train loss = 2.0808, val loss = 1.8734\n",
      "step 6670: train loss = 1.3821, val loss = 1.8287\n",
      "step 6680: train loss = 1.6814, val loss = 1.3944\n",
      "step 6690: train loss = 1.2519, val loss = 1.6728\n",
      "step 6700: train loss = 1.3676, val loss = 1.5997\n",
      "step 6710: train loss = 1.7805, val loss = 1.6041\n",
      "step 6720: train loss = 1.7545, val loss = 1.7696\n",
      "step 6730: train loss = 1.6514, val loss = 1.6492\n",
      "step 6740: train loss = 1.5395, val loss = 1.5877\n",
      "step 6750: train loss = 1.4970, val loss = 1.6914\n",
      "step 6760: train loss = 1.5939, val loss = 1.6065\n",
      "step 6770: train loss = 1.5693, val loss = 1.8561\n",
      "step 6780: train loss = 1.7958, val loss = 1.5175\n",
      "step 6790: train loss = 1.6409, val loss = 1.6903\n",
      "step 6800: train loss = 1.4679, val loss = 1.7026\n",
      "step 6810: train loss = 1.5676, val loss = 1.5712\n",
      "step 6820: train loss = 1.5670, val loss = 1.4935\n",
      "step 6830: train loss = 1.6998, val loss = 1.5672\n",
      "step 6840: train loss = 1.5033, val loss = 1.5967\n",
      "step 6850: train loss = 1.6233, val loss = 1.5833\n",
      "step 6860: train loss = 1.5123, val loss = 1.7602\n",
      "step 6870: train loss = 1.4940, val loss = 1.6672\n",
      "step 6880: train loss = 1.5039, val loss = 1.6849\n",
      "step 6890: train loss = 1.3654, val loss = 1.9055\n",
      "step 6900: train loss = 1.5393, val loss = 1.5764\n",
      "step 6910: train loss = 1.7297, val loss = 1.4398\n",
      "step 6920: train loss = 1.6486, val loss = 1.7094\n",
      "step 6930: train loss = 1.8116, val loss = 1.5898\n",
      "step 6940: train loss = 1.5487, val loss = 1.8073\n",
      "step 6950: train loss = 1.5593, val loss = 1.5975\n",
      "step 6960: train loss = 1.6950, val loss = 1.6928\n",
      "step 6970: train loss = 1.5683, val loss = 1.4802\n",
      "step 6980: train loss = 1.4167, val loss = 1.3780\n",
      "step 6990: train loss = 1.7071, val loss = 1.2978\n",
      "step 7000: train loss = 1.4221, val loss = 1.5767\n",
      "step 7010: train loss = 1.6207, val loss = 1.9330\n",
      "step 7020: train loss = 1.7744, val loss = 1.5260\n",
      "step 7030: train loss = 1.5616, val loss = 2.3261\n",
      "step 7040: train loss = 1.3134, val loss = 1.4671\n",
      "step 7050: train loss = 1.8401, val loss = 1.7237\n",
      "step 7060: train loss = 1.3461, val loss = 1.5710\n",
      "step 7070: train loss = 1.2374, val loss = 2.1517\n",
      "step 7080: train loss = 1.6518, val loss = 1.7388\n",
      "step 7090: train loss = 1.5759, val loss = 1.8099\n",
      "step 7100: train loss = 1.8391, val loss = 1.6484\n",
      "step 7110: train loss = 1.6342, val loss = 1.6060\n",
      "step 7120: train loss = 1.5227, val loss = 1.5303\n",
      "step 7130: train loss = 1.4408, val loss = 1.5644\n",
      "step 7140: train loss = 1.8578, val loss = 1.8145\n",
      "step 7150: train loss = 1.4292, val loss = 1.7416\n",
      "step 7160: train loss = 1.3872, val loss = 1.5196\n",
      "step 7170: train loss = 1.6911, val loss = 1.8955\n",
      "step 7180: train loss = 1.3801, val loss = 1.6519\n",
      "step 7190: train loss = 1.7111, val loss = 1.5158\n",
      "step 7200: train loss = 1.3376, val loss = 1.5111\n",
      "step 7210: train loss = 1.7227, val loss = 1.4082\n",
      "step 7220: train loss = 1.6332, val loss = 1.8663\n",
      "step 7230: train loss = 1.5039, val loss = 1.4081\n",
      "step 7240: train loss = 1.3925, val loss = 1.6371\n",
      "step 7250: train loss = 1.5601, val loss = 1.4542\n",
      "step 7260: train loss = 1.8337, val loss = 1.5005\n",
      "step 7270: train loss = 1.7135, val loss = 1.5443\n",
      "step 7280: train loss = 1.7001, val loss = 1.5653\n",
      "step 7290: train loss = 1.2914, val loss = 1.8753\n",
      "step 7300: train loss = 1.5366, val loss = 1.7662\n",
      "step 7310: train loss = 1.4504, val loss = 1.6584\n",
      "step 7320: train loss = 1.4048, val loss = 1.6077\n",
      "step 7330: train loss = 1.7048, val loss = 1.6814\n",
      "step 7340: train loss = 1.4623, val loss = 2.0668\n",
      "step 7350: train loss = 1.4611, val loss = 1.3122\n",
      "step 7360: train loss = 1.8299, val loss = 1.4626\n",
      "step 7370: train loss = 1.6324, val loss = 1.6696\n",
      "step 7380: train loss = 1.6945, val loss = 1.6444\n",
      "step 7390: train loss = 1.7613, val loss = 1.8131\n",
      "step 7400: train loss = 1.2465, val loss = 1.4857\n",
      "step 7410: train loss = 1.6733, val loss = 1.4331\n",
      "step 7420: train loss = 1.4586, val loss = 1.4649\n",
      "step 7430: train loss = 1.5702, val loss = 1.3291\n",
      "step 7440: train loss = 1.4454, val loss = 1.5130\n",
      "step 7450: train loss = 1.4498, val loss = 1.7281\n",
      "step 7460: train loss = 1.2171, val loss = 1.5013\n",
      "step 7470: train loss = 1.4461, val loss = 1.5774\n",
      "step 7480: train loss = 1.6488, val loss = 1.6894\n",
      "step 7490: train loss = 1.4021, val loss = 1.7213\n",
      "step 7500: train loss = 1.6162, val loss = 1.5855\n",
      "step 7510: train loss = 1.5098, val loss = 1.7171\n",
      "step 7520: train loss = 1.6453, val loss = 1.4457\n",
      "step 7530: train loss = 1.5830, val loss = 1.5339\n",
      "step 7540: train loss = 1.5883, val loss = 1.5958\n",
      "step 7550: train loss = 1.3953, val loss = 1.6422\n",
      "step 7560: train loss = 1.3666, val loss = 1.6417\n",
      "step 7570: train loss = 1.4388, val loss = 1.6452\n",
      "step 7580: train loss = 1.6077, val loss = 1.6014\n",
      "step 7590: train loss = 1.8143, val loss = 1.8430\n",
      "step 7600: train loss = 1.6783, val loss = 1.5773\n",
      "step 7610: train loss = 1.5382, val loss = 1.9494\n",
      "step 7620: train loss = 1.5095, val loss = 1.5041\n",
      "step 7630: train loss = 1.6023, val loss = 1.8482\n",
      "step 7640: train loss = 1.8289, val loss = 1.5312\n",
      "step 7650: train loss = 1.6238, val loss = 1.1993\n",
      "step 7660: train loss = 1.7355, val loss = 1.2630\n",
      "step 7670: train loss = 1.8212, val loss = 1.5394\n",
      "step 7680: train loss = 1.4302, val loss = 2.1883\n",
      "step 7690: train loss = 1.2909, val loss = 1.6366\n",
      "step 7700: train loss = 1.9200, val loss = 1.4690\n",
      "step 7710: train loss = 1.8433, val loss = 1.6212\n",
      "step 7720: train loss = 1.8707, val loss = 1.5681\n",
      "step 7730: train loss = 1.4563, val loss = 1.7929\n",
      "step 7740: train loss = 1.5947, val loss = 1.3749\n",
      "step 7750: train loss = 1.8798, val loss = 1.5697\n",
      "step 7760: train loss = 1.5878, val loss = 1.8316\n",
      "step 7770: train loss = 1.3792, val loss = 1.3861\n",
      "step 7780: train loss = 1.2431, val loss = 1.4665\n",
      "step 7790: train loss = 1.5776, val loss = 1.3091\n",
      "step 7800: train loss = 1.4113, val loss = 1.3068\n",
      "step 7810: train loss = 1.8059, val loss = 1.7615\n",
      "step 7820: train loss = 1.6215, val loss = 1.4744\n",
      "step 7830: train loss = 1.8072, val loss = 1.3119\n",
      "step 7840: train loss = 1.5219, val loss = 1.7324\n",
      "step 7850: train loss = 1.6150, val loss = 1.6903\n",
      "step 7860: train loss = 1.2159, val loss = 1.9161\n",
      "step 7870: train loss = 1.5959, val loss = 1.2504\n",
      "step 7880: train loss = 1.5587, val loss = 1.6873\n",
      "step 7890: train loss = 1.3777, val loss = 1.6084\n",
      "step 7900: train loss = 1.4763, val loss = 1.5089\n",
      "step 7910: train loss = 1.5175, val loss = 1.3869\n",
      "step 7920: train loss = 1.5579, val loss = 1.6246\n",
      "step 7930: train loss = 1.8327, val loss = 1.5480\n",
      "step 7940: train loss = 1.7123, val loss = 1.7605\n",
      "step 7950: train loss = 1.5924, val loss = 1.5175\n",
      "step 7960: train loss = 1.5136, val loss = 1.7760\n",
      "step 7970: train loss = 1.8217, val loss = 1.6781\n",
      "step 7980: train loss = 1.4832, val loss = 1.3380\n",
      "step 7990: train loss = 1.5482, val loss = 1.8148\n",
      "step 8000: train loss = 1.3356, val loss = 1.3835\n",
      "step 8010: train loss = 1.5168, val loss = 1.6064\n",
      "step 8020: train loss = 1.5214, val loss = 1.7893\n",
      "step 8030: train loss = 1.4532, val loss = 1.9469\n",
      "step 8040: train loss = 1.2551, val loss = 2.0354\n",
      "step 8050: train loss = 1.6588, val loss = 1.6593\n",
      "step 8060: train loss = 1.5282, val loss = 1.5485\n",
      "step 8070: train loss = 1.4240, val loss = 1.9445\n",
      "step 8080: train loss = 1.3525, val loss = 1.5927\n",
      "step 8090: train loss = 1.5056, val loss = 1.7563\n",
      "step 8100: train loss = 1.5121, val loss = 1.4675\n",
      "step 8110: train loss = 1.4825, val loss = 1.5600\n",
      "step 8120: train loss = 1.4561, val loss = 1.6212\n",
      "step 8130: train loss = 1.4380, val loss = 1.4125\n",
      "step 8140: train loss = 1.7504, val loss = 1.8883\n",
      "step 8150: train loss = 1.8056, val loss = 1.8612\n",
      "step 8160: train loss = 1.5953, val loss = 2.0143\n",
      "step 8170: train loss = 1.3361, val loss = 1.7009\n",
      "step 8180: train loss = 1.6683, val loss = 1.2668\n",
      "step 8190: train loss = 1.3736, val loss = 1.8973\n",
      "step 8200: train loss = 1.3383, val loss = 1.5066\n",
      "step 8210: train loss = 1.4986, val loss = 1.8625\n",
      "step 8220: train loss = 1.3256, val loss = 1.6386\n",
      "step 8230: train loss = 1.7700, val loss = 1.8195\n",
      "step 8240: train loss = 1.7591, val loss = 1.5270\n",
      "step 8250: train loss = 1.5107, val loss = 1.7299\n",
      "step 8260: train loss = 1.4782, val loss = 1.7742\n",
      "step 8270: train loss = 1.7828, val loss = 1.8483\n",
      "step 8280: train loss = 1.6114, val loss = 1.7652\n",
      "step 8290: train loss = 1.7266, val loss = 1.3338\n",
      "step 8300: train loss = 1.9014, val loss = 1.7058\n",
      "step 8310: train loss = 1.4796, val loss = 1.7497\n",
      "step 8320: train loss = 1.4508, val loss = 1.3107\n",
      "step 8330: train loss = 1.3339, val loss = 1.5603\n",
      "step 8340: train loss = 1.6361, val loss = 1.6700\n",
      "step 8350: train loss = 1.1759, val loss = 1.4072\n",
      "step 8360: train loss = 1.4873, val loss = 1.6322\n",
      "step 8370: train loss = 1.7342, val loss = 1.4498\n",
      "step 8380: train loss = 1.6118, val loss = 1.4789\n",
      "step 8390: train loss = 1.6980, val loss = 1.6354\n",
      "step 8400: train loss = 1.4035, val loss = 1.6088\n",
      "step 8410: train loss = 1.3580, val loss = 1.6208\n",
      "step 8420: train loss = 1.4832, val loss = 1.6736\n",
      "step 8430: train loss = 1.5976, val loss = 1.7185\n",
      "step 8440: train loss = 1.5703, val loss = 1.2655\n",
      "step 8450: train loss = 1.5718, val loss = 1.3093\n",
      "step 8460: train loss = 1.7647, val loss = 1.3116\n",
      "step 8470: train loss = 1.2165, val loss = 1.4479\n",
      "step 8480: train loss = 1.4456, val loss = 1.4252\n",
      "step 8490: train loss = 1.2673, val loss = 1.5311\n",
      "step 8500: train loss = 1.5402, val loss = 1.9659\n",
      "step 8510: train loss = 1.4131, val loss = 1.5672\n",
      "step 8520: train loss = 1.5989, val loss = 1.2612\n",
      "step 8530: train loss = 1.7163, val loss = 1.7626\n",
      "step 8540: train loss = 1.7277, val loss = 1.5302\n",
      "step 8550: train loss = 1.6558, val loss = 1.4520\n",
      "step 8560: train loss = 1.9056, val loss = 1.5618\n",
      "step 8570: train loss = 1.4165, val loss = 1.2865\n",
      "step 8580: train loss = 1.3870, val loss = 1.9174\n",
      "step 8590: train loss = 1.4427, val loss = 1.5766\n",
      "step 8600: train loss = 1.6102, val loss = 1.4792\n",
      "step 8610: train loss = 1.4237, val loss = 1.6453\n",
      "step 8620: train loss = 1.3758, val loss = 1.6703\n",
      "step 8630: train loss = 1.7869, val loss = 1.8369\n",
      "step 8640: train loss = 1.6371, val loss = 1.5263\n",
      "step 8650: train loss = 1.7275, val loss = 1.6372\n",
      "step 8660: train loss = 1.8549, val loss = 1.5206\n",
      "step 8670: train loss = 1.5865, val loss = 1.4905\n",
      "step 8680: train loss = 1.0742, val loss = 1.5162\n",
      "step 8690: train loss = 1.3265, val loss = 1.4182\n",
      "step 8700: train loss = 1.6358, val loss = 1.4206\n",
      "step 8710: train loss = 1.6891, val loss = 1.2678\n",
      "step 8720: train loss = 1.5282, val loss = 1.9102\n",
      "step 8730: train loss = 1.3460, val loss = 1.4308\n",
      "step 8740: train loss = 1.4482, val loss = 1.5753\n",
      "step 8750: train loss = 1.8402, val loss = 1.5649\n",
      "step 8760: train loss = 1.4261, val loss = 1.4184\n",
      "step 8770: train loss = 1.6391, val loss = 1.7081\n",
      "step 8780: train loss = 1.4880, val loss = 1.4966\n",
      "step 8790: train loss = 1.7258, val loss = 1.5123\n",
      "step 8800: train loss = 1.7749, val loss = 1.7063\n",
      "step 8810: train loss = 1.8001, val loss = 1.5609\n",
      "step 8820: train loss = 1.4779, val loss = 1.6612\n",
      "step 8830: train loss = 1.4849, val loss = 1.5368\n",
      "step 8840: train loss = 1.5695, val loss = 1.2959\n",
      "step 8850: train loss = 1.6225, val loss = 1.5342\n",
      "step 8860: train loss = 1.5349, val loss = 1.8371\n",
      "step 8870: train loss = 1.6053, val loss = 1.3683\n",
      "step 8880: train loss = 1.4628, val loss = 1.3912\n",
      "step 8890: train loss = 1.6258, val loss = 1.5120\n",
      "step 8900: train loss = 1.7247, val loss = 1.3122\n",
      "step 8910: train loss = 1.5381, val loss = 1.5340\n",
      "step 8920: train loss = 1.8271, val loss = 1.5615\n",
      "step 8930: train loss = 1.9557, val loss = 2.0393\n",
      "step 8940: train loss = 1.8125, val loss = 1.8288\n",
      "step 8950: train loss = 1.7223, val loss = 1.6974\n",
      "step 8960: train loss = 1.5603, val loss = 1.5499\n",
      "step 8970: train loss = 1.5061, val loss = 1.4570\n",
      "step 8980: train loss = 1.2844, val loss = 1.7367\n",
      "step 8990: train loss = 1.5173, val loss = 2.0728\n",
      "step 9000: train loss = 1.6005, val loss = 1.4077\n",
      "step 9010: train loss = 1.9451, val loss = 1.6579\n",
      "step 9020: train loss = 1.8086, val loss = 1.5802\n",
      "step 9030: train loss = 1.6540, val loss = 1.3759\n",
      "step 9040: train loss = 1.6105, val loss = 1.5313\n",
      "step 9050: train loss = 1.8917, val loss = 1.7890\n",
      "step 9060: train loss = 1.6865, val loss = 1.3485\n",
      "step 9070: train loss = 1.6539, val loss = 1.3817\n",
      "step 9080: train loss = 1.6338, val loss = 1.5068\n",
      "step 9090: train loss = 1.8011, val loss = 1.5716\n",
      "step 9100: train loss = 1.4148, val loss = 1.7600\n",
      "step 9110: train loss = 1.5592, val loss = 1.3093\n",
      "step 9120: train loss = 1.5698, val loss = 1.7717\n",
      "step 9130: train loss = 1.6153, val loss = 1.6414\n",
      "step 9140: train loss = 1.6938, val loss = 1.5953\n",
      "step 9150: train loss = 1.5269, val loss = 1.7489\n",
      "step 9160: train loss = 1.8424, val loss = 1.5964\n",
      "step 9170: train loss = 1.7607, val loss = 1.5739\n",
      "step 9180: train loss = 1.4476, val loss = 1.4855\n",
      "step 9190: train loss = 1.2435, val loss = 1.7749\n",
      "step 9200: train loss = 1.5314, val loss = 1.7387\n",
      "step 9210: train loss = 1.2586, val loss = 1.5209\n",
      "step 9220: train loss = 1.3415, val loss = 1.7462\n",
      "step 9230: train loss = 1.3861, val loss = 1.4084\n",
      "step 9240: train loss = 1.7377, val loss = 1.2964\n",
      "step 9250: train loss = 1.7208, val loss = 1.9457\n",
      "step 9260: train loss = 1.3454, val loss = 1.3539\n",
      "step 9270: train loss = 1.7399, val loss = 1.7379\n",
      "step 9280: train loss = 1.1412, val loss = 1.6332\n",
      "step 9290: train loss = 1.4303, val loss = 1.7785\n",
      "step 9300: train loss = 1.5339, val loss = 1.8085\n",
      "step 9310: train loss = 1.6278, val loss = 1.5016\n",
      "step 9320: train loss = 1.5728, val loss = 1.5658\n",
      "step 9330: train loss = 1.5216, val loss = 1.4645\n",
      "step 9340: train loss = 1.6162, val loss = 1.6447\n",
      "step 9350: train loss = 1.7113, val loss = 1.8056\n",
      "step 9360: train loss = 1.3233, val loss = 1.6482\n",
      "step 9370: train loss = 1.4169, val loss = 1.5209\n",
      "step 9380: train loss = 1.3821, val loss = 1.5904\n",
      "step 9390: train loss = 1.2359, val loss = 1.3961\n",
      "step 9400: train loss = 1.5220, val loss = 1.5569\n",
      "step 9410: train loss = 1.2773, val loss = 1.8097\n",
      "step 9420: train loss = 1.2689, val loss = 1.5861\n",
      "step 9430: train loss = 1.6519, val loss = 1.7403\n",
      "step 9440: train loss = 1.6500, val loss = 1.7170\n",
      "step 9450: train loss = 1.6375, val loss = 1.5794\n",
      "step 9460: train loss = 1.7727, val loss = 1.3681\n",
      "step 9470: train loss = 1.4021, val loss = 1.4562\n",
      "step 9480: train loss = 1.7348, val loss = 1.5920\n",
      "step 9490: train loss = 1.5266, val loss = 1.7733\n",
      "step 9500: train loss = 1.2551, val loss = 1.6059\n",
      "step 9510: train loss = 1.7636, val loss = 1.7801\n",
      "step 9520: train loss = 1.3276, val loss = 2.1516\n",
      "step 9530: train loss = 1.5401, val loss = 1.7180\n",
      "step 9540: train loss = 1.1897, val loss = 1.4361\n",
      "step 9550: train loss = 1.4922, val loss = 1.4893\n",
      "step 9560: train loss = 1.5827, val loss = 1.5086\n",
      "step 9570: train loss = 1.6328, val loss = 1.8906\n",
      "step 9580: train loss = 1.7723, val loss = 1.7301\n",
      "step 9590: train loss = 1.5514, val loss = 1.7164\n",
      "step 9600: train loss = 1.7561, val loss = 1.4232\n",
      "step 9610: train loss = 1.7360, val loss = 1.4350\n",
      "step 9620: train loss = 1.7124, val loss = 1.7707\n",
      "step 9630: train loss = 1.2983, val loss = 1.7267\n",
      "step 9640: train loss = 1.2953, val loss = 1.7701\n",
      "step 9650: train loss = 1.4260, val loss = 1.5511\n",
      "step 9660: train loss = 1.4117, val loss = 1.4630\n",
      "step 9670: train loss = 1.4022, val loss = 1.6240\n",
      "step 9680: train loss = 1.4940, val loss = 1.5752\n",
      "step 9690: train loss = 1.1886, val loss = 1.4226\n",
      "step 9700: train loss = 1.6136, val loss = 1.7755\n",
      "step 9710: train loss = 1.5930, val loss = 1.6014\n",
      "step 9720: train loss = 1.5282, val loss = 1.3958\n",
      "step 9730: train loss = 1.4576, val loss = 1.5150\n",
      "step 9740: train loss = 1.3740, val loss = 1.3325\n",
      "step 9750: train loss = 1.4025, val loss = 1.7132\n",
      "step 9760: train loss = 1.1934, val loss = 1.8773\n",
      "step 9770: train loss = 1.7704, val loss = 1.3867\n",
      "step 9780: train loss = 1.3888, val loss = 1.9417\n",
      "step 9790: train loss = 1.5762, val loss = 1.7447\n",
      "step 9800: train loss = 1.3482, val loss = 1.5809\n",
      "step 9810: train loss = 1.4322, val loss = 1.7564\n",
      "step 9820: train loss = 1.4668, val loss = 1.7874\n",
      "step 9830: train loss = 1.3710, val loss = 1.4959\n",
      "step 9840: train loss = 1.4424, val loss = 1.2409\n",
      "step 9850: train loss = 1.8940, val loss = 1.7174\n",
      "step 9860: train loss = 1.5050, val loss = 1.6456\n",
      "step 9870: train loss = 1.6965, val loss = 1.8146\n",
      "step 9880: train loss = 1.7811, val loss = 1.6447\n",
      "step 9890: train loss = 1.6909, val loss = 1.5733\n",
      "step 9900: train loss = 1.1689, val loss = 1.3836\n",
      "step 9910: train loss = 1.5282, val loss = 1.5515\n",
      "step 9920: train loss = 1.3870, val loss = 1.6514\n",
      "step 9930: train loss = 1.3565, val loss = 2.0921\n",
      "step 9940: train loss = 1.6788, val loss = 1.8498\n",
      "step 9950: train loss = 1.5003, val loss = 1.4311\n",
      "step 9960: train loss = 1.7078, val loss = 1.6305\n",
      "step 9970: train loss = 1.9085, val loss = 1.7649\n",
      "step 9980: train loss = 1.4705, val loss = 1.3217\n",
      "step 9990: train loss = 1.4208, val loss = 1.8465\n",
      "step 9999: train loss = 1.2452, val loss = 1.3200\n"
     ]
    }
   ],
   "source": [
    "# Write a small test script with a smaller model to train on the cifar10 dataset\n",
    "import torch.nn.init as init\n",
    "\n",
    "# construct model\n",
    "model = ViT(img_size = 32, \n",
    "                    patch_size = 16, \n",
    "                    in_chans=3, \n",
    "                    n_classes = 10, \n",
    "                    embed_dim = 128, \n",
    "                    depth = 8,\n",
    "                    n_heads = 8,\n",
    "                    mlp_ratio = 1.0,\n",
    "                    qkv_bias = True,\n",
    "                    attn_d = 0.2,\n",
    "                    proj_d = 0.2)\n",
    "\n",
    "print(f'Total params: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "# Initialize the weights\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.normal_(m.weight, std = 0.02)\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        init.ones_(m.weight)\n",
    "        init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight, mode = 'fan_in', nonlinearity = 'leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "# load data and convert to torch tensors\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.FloatTensor(y_test).long()\n",
    "\n",
    "print(f'Training set:: input: {x_train.shape}, output: {y_train.shape}')\n",
    "print(f'Test set:: input: {x_test.shape}, output: {y_test.shape}')\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4) # use Adam optimizer with a small learning rate\n",
    "\n",
    "# Send all to device\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# define a function to get mini batches of data\n",
    "def get_batch(mode):\n",
    "    if mode == 'train':\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        xb = x_train[idx]\n",
    "        yb = y_train[idx]\n",
    "    elif mode == 'test':\n",
    "        idx = np.random.randint(0, x_test.shape[0], batch_size)\n",
    "        xb = x_test[idx]\n",
    "        yb = y_test[idx]\n",
    "    return xb,yb\n",
    "\n",
    "# Write a function to evaluate the loss on the training and test set\n",
    "def estimate_loss():\n",
    "    losses = {}\n",
    "    for mode in ['train', 'val']:\n",
    "        if mode == 'train':\n",
    "            xb,yb = get_batch('train')\n",
    "        elif mode == 'val':\n",
    "            xb,yb = get_batch('test')\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        losses[mode] = loss.item()\n",
    "    return losses\n",
    "\n",
    "# train the model\n",
    "max_iters = 10000\n",
    "batch_size = 32\n",
    "eval_interval = 10\n",
    "lossi_train = []\n",
    "lossi_test = []\n",
    "for i in range(max_iters):\n",
    "    # print training and testing loss\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n",
    "    # sample from the batch\n",
    "    xb,yb = get_batch('train')\n",
    "    # forward pass\n",
    "    logits = model(xb)\n",
    "    loss = criterion(logits, yb)\n",
    "    lossi = estimate_loss()\n",
    "    lossi_train.append(lossi['train'])\n",
    "    lossi_test.append(lossi['val'])\n",
    "    # backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "loss_fin = estimate_loss()\n",
    "print(f\"step {i}: train loss = {loss_fin['train']:.4f}, val loss = {loss_fin['val']:.4f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cross entropy loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss function\n",
    "lossi_train_mean = torch.Tensor(lossi_train).view(-1,100).mean(1)\n",
    "lossi_test_mean = torch.Tensor(lossi_test).view(-1,100).mean(1)\n",
    "plt.plot(lossi_train_mean, label = 'train')\n",
    "plt.plot(lossi_test_mean, label = 'test')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cross entropy loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an example of a picture and its prediction on the side, also plot the attention weights\n",
    "import seaborn as sns\n",
    "\n",
    "## Extract the attention weights from your model\n",
    "#layer_number = ... # Choose a specific layer number\n",
    "#head_number = ... # Choose a specific head number\n",
    "#weights = model.transformer.layer[layer_number].attention.self.weights[head_number].detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "def pred2label(pred):\n",
    "    return class_names[pred]\n",
    "\n",
    "\n",
    "def show_example():\n",
    "    xb,yb = get_batch('test')\n",
    "    logits = model(xb)\n",
    "    pred = torch.argmax(logits, dim = 1)\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(xb[0].cpu().numpy().transpose(1,2,0).astype('uint8'))\n",
    "    # Plot the attention weights as a heatmap\n",
    "    #sns.heatmap(weights, cmap=\"YlGnBu\")\n",
    "    plt.title(f'Prediction: {class_names[pred[0].item()]}, Actual: {class_names[yb[0].item()]}')\n",
    "    plt.show()\n",
    "    \n",
    "show_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=20, bias=True)\n",
      "torch.Size([40, 20])\n",
      "torch.Size([40, 30, 20])\n",
      "torch.Size([40, 30, 40, 20])\n"
     ]
    }
   ],
   "source": [
    "# How does the linear layer behave when you have more than 1 dimension\n",
    "module = torch.nn.Linear(10,20)\n",
    "print(module)\n",
    "n_samples = 40\n",
    "input_2d = torch.Tensor(n_samples, 10)\n",
    "print(module(input_2d).shape) # now you have a n_samples X out_features tensor\n",
    "input_3d = torch.Tensor(n_samples,30, 10)\n",
    "print(module(input_3d).shape) # the middle dimension is maintained\n",
    "input_xd = torch.Tensor(n_samples, 30, 40, 10)\n",
    "print(module(input_xd).shape) # the middle dimensions are maintained\n",
    "# So the linear layer only works on the first dimension and the last dimension, the middle dimensions are maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) \n",
      " tensor([[0., 0., 2., 2., 2.],\n",
      "        [0., 2., 0., 2., 0.],\n",
      "        [0., 2., 2., 0., 2.]])\n",
      "tensor(15.) tensor(22.)\n"
     ]
    }
   ],
   "source": [
    "# What is dropout?\n",
    "\n",
    "p = 0.5\n",
    "module = torch.nn.Dropout(p)\n",
    "test_mat = torch.ones(3,5)\n",
    "print(test_mat, '\\n' ,module(test_mat))\n",
    "print(test_mat.sum(),module(test_mat).sum())\n",
    "# We see here that the dropout layer has now been multiplied with\n",
    "# 1/(1-p) = 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98cbb787251fac8b09d58c88d44135973ceca7df75589457d3db0159f5eed1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
